{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:02.951486Z",
     "start_time": "2024-06-27T09:32:02.936678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "23581b0e55d01ac3",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "412905987be26ead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:06.938507Z",
     "start_time": "2024-06-27T09:32:06.920699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hash_feature(value, num_bins=1000):\n",
    "    \"\"\"Helper function to hash a value into a fixed number of bins.\"\"\"\n",
    "    return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % num_bins\n",
    "\n",
    "\n",
    "def extract_features(node):\n",
    "    # Initialize features with default values\n",
    "    type_feature = [0]\n",
    "    name_feature = [0]\n",
    "    value_feature = [0]\n",
    "    src_feature = [0, 0]\n",
    "    type_desc_features = [0, 0]\n",
    "    state_mutability_feature = [0]\n",
    "    visibility_feature = [0]\n",
    "\n",
    "    # Extract basic features\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    type_feature = [hash_feature(node_type)]\n",
    "\n",
    "    # Extract additional features if they exist\n",
    "    if 'name' in node:\n",
    "        name_feature = [hash_feature(node.get('name', ''))]\n",
    "    if 'value' in node:\n",
    "        value_feature = [hash_feature(node.get('value', ''))]\n",
    "\n",
    "    # Extract src features (start, end, and length if available)\n",
    "    if 'src' in node:\n",
    "        start, length, *_ = map(int, node['src'].split(':'))\n",
    "        src_feature = [start, length]\n",
    "\n",
    "    # Extract typeDescriptions features if they exist\n",
    "    if 'typeDescriptions' in node:\n",
    "        type_desc = node['typeDescriptions']\n",
    "        type_desc_features = [\n",
    "            hash_feature(type_desc.get('typeString', '')),\n",
    "            hash_feature(type_desc.get('typeIdentifier', ''))\n",
    "        ]\n",
    "\n",
    "    # Extract stateMutability if it exists\n",
    "    if 'stateMutability' in node:\n",
    "        state_mutability_feature = [hash_feature(node.get('stateMutability', ''))]\n",
    "\n",
    "    # Extract visibility if it exists\n",
    "    if 'visibility' in node:\n",
    "        visibility_feature = [hash_feature(node.get('visibility', ''))]\n",
    "\n",
    "    # Combine all features into a single feature vector\n",
    "    features = (type_feature + name_feature + value_feature +\n",
    "                src_feature + type_desc_features +\n",
    "                state_mutability_feature + visibility_feature)\n",
    "    return features"
   ],
   "id": "43457b36d1175282",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading",
   "id": "a6a935e8ee0ab0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:12.868094Z",
     "start_time": "2024-06-27T09:32:09.952143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_masks_to_data(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    num_nodes = data.x.size(0)\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    train_size = int(num_nodes * train_ratio)\n",
    "    val_size = int(num_nodes * val_ratio)\n",
    "\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def ast_to_graph(ast_json):\n",
    "    graph = nx.DiGraph()\n",
    "    node_id = 0\n",
    "\n",
    "    def add_nodes_edges(node, parent=None):\n",
    "        nonlocal node_id\n",
    "        current_node_id = node_id\n",
    "        graph.add_node(current_node_id, features=extract_features(node))\n",
    "        if parent is not None:\n",
    "            graph.add_edge(parent, current_node_id)\n",
    "        node_id += 1\n",
    "        for key, value in node.items():\n",
    "            if isinstance(value, dict):\n",
    "                add_nodes_edges(value, current_node_id)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        add_nodes_edges(item, current_node_id)\n",
    "\n",
    "    add_nodes_edges(ast_json)\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    x = torch.stack([torch.tensor(graph.nodes[n]['features'], dtype=torch.float) for n in graph.nodes])\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return add_masks_to_data(data)\n",
    "\n",
    "\n",
    "def generate_label_map(ast_directory):\n",
    "    label_map = {}\n",
    "    label_index = 0\n",
    "    for category in os.listdir(ast_directory):\n",
    "        category_path = os.path.join(ast_directory, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            label_map[category] = label_index\n",
    "            label_index += 1\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def load_data(ast_directory, label_map):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for category in os.listdir(ast_directory):\n",
    "        category_path = os.path.join(ast_directory, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for root, _, files in os.walk(category_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.json'):\n",
    "                        filepath = os.path.join(root, file)\n",
    "                        with open(filepath, 'r') as f:\n",
    "                            ast = json.load(f)\n",
    "                        data = ast_to_graph(ast)\n",
    "                        label = label_map[category]\n",
    "                        data.y = torch.tensor([label] * data.x.size(0), dtype=torch.long)  # Assign label to all nodes\n",
    "                        dataset.append(data)\n",
    "                        labels.append(label)\n",
    "    print(f\"Loaded {len(dataset)} samples from {ast_directory}\")\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "ast_directory = '../dataset/aisc/ast'\n",
    "label_map = generate_label_map(ast_directory)\n",
    "dataset, labels = load_data(ast_directory, label_map)\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"No data loaded. Please check the dataset directory and files.\")"
   ],
   "id": "a003fe9d27ab8c41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1020 samples from ../dataset/aisc/ast\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Augmentation",
   "id": "abeead762cbb1622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:14.729233Z",
     "start_time": "2024-06-27T09:32:14.690507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def substitute_nodes(ast, substitutions):\n",
    "    \"\"\"\n",
    "    Substitute certain nodes in the AST with other semantically equivalent nodes.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param substitutions: A dictionary where keys are node types to be replaced, and values are the replacements.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key in substitutions:\n",
    "                ast[key] = substitutions[key]\n",
    "            else:\n",
    "                ast[key] = substitute_nodes(value, substitutions)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = substitute_nodes(ast[i], substitutions)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def insert_nodes(ast, insertions):\n",
    "    \"\"\"\n",
    "    Insert certain nodes into the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param insertions: A dictionary where keys are locations to insert, and values are the nodes to be inserted.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key in insertions:\n",
    "                ast[key] = [value, insertions[key]] if isinstance(value, list) else [value, insertions[key]]\n",
    "            else:\n",
    "                ast[key] = insert_nodes(value, insertions)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = insert_nodes(ast[i], insertions)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def delete_nodes(ast, deletions):\n",
    "    \"\"\"\n",
    "    Delete certain nodes from the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param deletions: A set of node types to be deleted.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        keys_to_delete = [key for key in ast if key in deletions]\n",
    "        for key in keys_to_delete:\n",
    "            del ast[key]\n",
    "        for key, value in ast.items():\n",
    "            ast[key] = delete_nodes(value, deletions)\n",
    "    elif isinstance(ast, list):\n",
    "        ast = [delete_nodes(item, deletions) for item in ast if item not in deletions]\n",
    "    return ast\n",
    "\n",
    "\n",
    "def rename_identifiers(ast, renames):\n",
    "    \"\"\"\n",
    "    Rename variables/functions in the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param renames: A dictionary where keys are the original names and values are the new names.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key == 'name' and value in renames:\n",
    "                ast[key] = renames[value]\n",
    "            else:\n",
    "                ast[key] = rename_identifiers(value, renames)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = rename_identifiers(ast[i], renames)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def reorder_statements(ast):\n",
    "    if isinstance(ast, dict) and 'body' in ast:\n",
    "        if isinstance(ast['body'], list):\n",
    "            random.shuffle(ast['body'])\n",
    "        else:\n",
    "            reorder_statements(ast['body'])\n",
    "    elif isinstance(ast, list):\n",
    "        for item in ast:\n",
    "            reorder_statements(item)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def add_no_op_statements(ast):\n",
    "    no_op_statement = {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}\n",
    "    if isinstance(ast, dict) and 'body' in ast:\n",
    "        if isinstance(ast['body'], list):\n",
    "            ast['body'].append(no_op_statement)\n",
    "        else:\n",
    "            add_no_op_statements(ast['body'])\n",
    "    elif isinstance(ast, list):\n",
    "        for item in ast:\n",
    "            add_no_op_statements(item)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def apply_augmentation(ast):\n",
    "    # Define your augmentation strategies\n",
    "    substitutions = {'FunctionDefinition': 'ModifierDefinition'}\n",
    "    insertions = {'body': {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}}\n",
    "    deletions = {'ModifierDefinition'}\n",
    "    renames = {'oldVarName': 'newVarName', 'oldFuncName': 'newFuncName'}\n",
    "\n",
    "    # Apply augmentations randomly\n",
    "    if random.random() > 0.5:\n",
    "        ast = substitute_nodes(ast, substitutions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = insert_nodes(ast, insertions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = delete_nodes(ast, deletions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = rename_identifiers(ast, renames)\n",
    "    if random.random() > 0.5:\n",
    "        ast = reorder_statements(ast)\n",
    "    if random.random() > 0.5:\n",
    "        ast = add_no_op_statements(ast)\n",
    "\n",
    "    return ast\n",
    "\n",
    "\n",
    "def generate_augmented_asts(dataset, num_augmentations=5):\n",
    "    augmented_dataset = []\n",
    "    for ast in dataset:\n",
    "        augmented_dataset.append(ast)\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_ast = apply_augmentation(copy.deepcopy(ast))\n",
    "            augmented_dataset.append(augmented_ast)\n",
    "    return augmented_dataset\n",
    "\n"
   ],
   "id": "919e05313241418a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph Neural Network",
   "id": "f6c4e9bc867252e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:17.508561Z",
     "start_time": "2024-06-27T09:32:17.471961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class AugmentedASTDataset(Dataset):\n",
    "    def __init__(self, dataset, apply_augmentation_func, num_augmentations=5, train=True):\n",
    "        self.dataset = dataset\n",
    "        self.apply_augmentation_func = apply_augmentation_func\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * (self.num_augmentations if self.train else 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.dataset)\n",
    "        ast = self.dataset[original_idx]\n",
    "\n",
    "        if self.train:\n",
    "            augmented_ast = self.apply_augmentation_func(copy.deepcopy(ast))\n",
    "            return augmented_ast\n",
    "        else:\n",
    "            return ast\n",
    "\n",
    "\n",
    "def prepare_augmented_dataloader(dataset, apply_augmentation_func, batch_size=32, num_augmentations=5, shuffle=True):\n",
    "    augmented_dataset = AugmentedASTDataset(dataset, apply_augmentation_func, num_augmentations=num_augmentations,\n",
    "                                            train=True)\n",
    "    return DataLoader(augmented_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def prepare_dataloader(dataset, batch_size=32, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def stratified_split(dataset, labels):\n",
    "    # Split data into training + validation and test data\n",
    "    train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "        dataset, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "\n",
    "    # Split training + validation into actual training and validation data\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_val_data, train_val_labels, test_size=0.11, stratify=train_val_labels,\n",
    "        random_state=42)  # 0.11 * 0.9 ≈ 0.1\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = out[data.train_mask].argmax(dim=1)\n",
    "        y_true.extend(data.y[data.train_mask].cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "        y_score.extend(F.softmax(out[data.train_mask], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_score, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return total_loss / len(loader), accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, mask_type):\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            mask = getattr(data, mask_type)\n",
    "            pred = out[mask].argmax(dim=1)\n",
    "            y_true.extend(data.y[mask].cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "            y_score.extend(F.softmax(out[mask], dim=1).cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, y_score, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc\n"
   ],
   "id": "a47b081396d59fbe",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:31.578298Z",
     "start_time": "2024-06-27T09:32:21.202467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, val_data, test_data = stratified_split(dataset, labels)\n",
    "\n",
    "print(f\"Train data size ........ : {len(train_data)}\")\n",
    "print(f\"Validation data size ... : {len(val_data)}\")\n",
    "print(f\"Test data size ......... : {len(test_data)}\")\n",
    "\n",
    "train_loader = prepare_augmented_dataloader(train_data, apply_augmentation, batch_size=32, num_augmentations=5)\n",
    "val_loader = prepare_dataloader(val_data, batch_size=32)\n",
    "test_loader = prepare_dataloader(test_data, batch_size=32)\n",
    "\n",
    "num_features = train_data[0].x.shape[1]\n",
    "num_classes = len(label_map)\n",
    "\n",
    "model = GCN(num_features=num_features, hidden_channels=64, num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1500):\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc = train(model, train_loader, optimizer,\n",
    "                                                                                  criterion)\n",
    "    val_acc, val_prec, val_rec, val_f1, val_roc_auc = evaluate(model, val_loader, 'val_mask')\n",
    "\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    print(f'EPOCH: {epoch + 1} -> Loss: {train_loss:.4f}')\n",
    "    print(\n",
    "        f'(Train) Accuracy: {train_acc:.4f}, Precision: {train_prec:.4f}, Recall: {train_rec:.4f}, F1: {train_f1:.4f}, AUC: {train_roc_auc:.4f}')\n",
    "    print(\n",
    "        f'(Valid) Accuracy: {val_acc:.4f}, Precision: {val_prec:.4f}, Recall: {val_rec:.4f}, F1: {val_f1:.4f}, AUC: {val_roc_auc:.4f}')\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "test_acc, test_prec, test_rec, test_f1, test_roc_auc = evaluate(model, test_loader, 'test_mask')\n",
    "print(\n",
    "    f'(Test) Accuracy: {test_acc:.4f}, Precision: {test_prec:.4f}, Recall: {test_rec:.4f}, F1: {test_f1:.4f}, AUC: {test_roc_auc:.4f}')\n"
   ],
   "id": "6d732648de0dc590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size ........ : 817\n",
      "Validation data size ... : 101\n",
      "Test data size ......... : 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteorizzo/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 1 -> Loss: 22.8029\n",
      "(Train) Accuracy: 0.1410, Precision: 0.1227, Recall: 0.1410, F1: 0.0975, AUC: 0.5065\n",
      "(Valid) Accuracy: 0.1907, Precision: 0.1220, Recall: 0.1907, F1: 0.0649, AUC: 0.5105\n",
      "-----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1500\u001B[39m):\n\u001B[0;32m---> 20\u001B[0m     train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m                                                                                  \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     val_acc, val_prec, val_rec, val_f1, val_roc_auc \u001B[38;5;241m=\u001B[39m evaluate(model, val_loader, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_mask\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-----------------------------------------------------------------------------------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[11], line 67\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, criterion)\u001B[0m\n\u001B[1;32m     64\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     65\u001B[0m y_score \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[1;32m     68\u001B[0m     data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     69\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[11], line 31\u001B[0m, in \u001B[0;36mAugmentedASTDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     28\u001B[0m ast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[original_idx]\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain:\n\u001B[0;32m---> 31\u001B[0m     augmented_ast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_augmentation_func(\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m augmented_ast\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:153\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    151\u001B[0m copier \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__deepcopy__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcopier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     reductor \u001B[38;5;241m=\u001B[39m dispatch_table\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch_geometric/data/data.py:595\u001B[0m, in \u001B[0;36mData.__deepcopy__\u001B[0;34m(self, memo)\u001B[0m\n\u001B[1;32m    593\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m    594\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 595\u001B[0m     out\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[key] \u001B[38;5;241m=\u001B[39m \u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    596\u001B[0m out\u001B[38;5;241m.\u001B[39m_store\u001B[38;5;241m.\u001B[39m_parent \u001B[38;5;241m=\u001B[39m out\n\u001B[1;32m    597\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:153\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    151\u001B[0m copier \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__deepcopy__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcopier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     reductor \u001B[38;5;241m=\u001B[39m dispatch_table\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch_geometric/data/storage.py:147\u001B[0m, in \u001B[0;36mBaseStorage.__deepcopy__\u001B[0;34m(self, memo)\u001B[0m\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    146\u001B[0m     out\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m[key] \u001B[38;5;241m=\u001B[39m value\n\u001B[0;32m--> 147\u001B[0m out\u001B[38;5;241m.\u001B[39m_mapping \u001B[38;5;241m=\u001B[39m \u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mapping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:146\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    144\u001B[0m copier \u001B[38;5;241m=\u001B[39m _deepcopy_dispatch\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcopier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28missubclass\u001B[39m(\u001B[38;5;28mcls\u001B[39m, \u001B[38;5;28mtype\u001B[39m):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:231\u001B[0m, in \u001B[0;36m_deepcopy_dict\u001B[0;34m(x, memo, deepcopy)\u001B[0m\n\u001B[1;32m    229\u001B[0m memo[\u001B[38;5;28mid\u001B[39m(x)] \u001B[38;5;241m=\u001B[39m y\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 231\u001B[0m     y[deepcopy(key, memo)] \u001B[38;5;241m=\u001B[39m \u001B[43mdeepcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py:153\u001B[0m, in \u001B[0;36mdeepcopy\u001B[0;34m(x, memo, _nil)\u001B[0m\n\u001B[1;32m    151\u001B[0m copier \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(x, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__deepcopy__\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m copier \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[43mcopier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmemo\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     reductor \u001B[38;5;241m=\u001B[39m dispatch_table\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28mcls\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:122\u001B[0m, in \u001B[0;36mTensor.__deepcopy__\u001B[0;34m(self, memo)\u001B[0m\n\u001B[1;32m    113\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    114\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe default implementation of __deepcopy__() for wrapper subclasses \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    115\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124monly works for subclass types that implement clone() and for which \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdifferent type.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m         )\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 122\u001B[0m     new_storage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_typed_storage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39m_deepcopy(memo)\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_quantized:\n\u001B[1;32m    124\u001B[0m         \u001B[38;5;66;03m# quantizer_params can be different type based on torch attribute\u001B[39;00m\n\u001B[1;32m    125\u001B[0m         quantizer_params: Union[\n\u001B[1;32m    126\u001B[0m             Tuple[torch\u001B[38;5;241m.\u001B[39mqscheme, \u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m    127\u001B[0m             Tuple[torch\u001B[38;5;241m.\u001B[39mqscheme, Tensor, Tensor, \u001B[38;5;28mint\u001B[39m],\n\u001B[1;32m    128\u001B[0m         ]\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:243\u001B[0m, in \u001B[0;36mTensor._typed_storage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_typed_storage\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    242\u001B[0m     untyped_storage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muntyped_storage()\n\u001B[0;32m--> 243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTypedStorage\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwrap_storage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muntyped_storage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_internal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[1;32m    245\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/storage.py:605\u001B[0m, in \u001B[0;36mTypedStorage.__init__\u001B[0;34m(self, device, dtype, wrap_storage, _internal, *args)\u001B[0m\n\u001B[1;32m    599\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    600\u001B[0m         arg_error_msg \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    601\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mArgument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should not be specified when \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwrap_storage\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is given\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m=\u001B[39m dtype\n\u001B[0;32m--> 605\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mwrap_storage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mUntypedStorage\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    606\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    607\u001B[0m         arg_error_msg \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m    608\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mArgument \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwrap_storage\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be UntypedStorage, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(wrap_storage)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    610\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_untyped_storage \u001B[38;5;241m=\u001B[39m wrap_storage\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Traditional Models",
   "id": "cde6ff44ab2a22ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:32:42.714777Z",
     "start_time": "2024-06-27T09:32:42.706173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_features_to_fixed_length(features, fixed_length=4716):\n",
    "    \"\"\"Pad feature vectors to a fixed length.\"\"\"\n",
    "    padding_length = fixed_length - len(features)\n",
    "    if padding_length > 0:\n",
    "        return np.concatenate([features, np.zeros(padding_length)])\n",
    "    else:\n",
    "        return features[:fixed_length]\n",
    "\n",
    "\n",
    "def extract_graph_features(graph, fixed_length=4716):\n",
    "    node_features = [graph.nodes[n]['features'] for n in graph.nodes]\n",
    "    max_length = max(len(f) for f in node_features)\n",
    "    padded_features = [pad_features_to_fixed_length(f, max_length) for f in node_features]\n",
    "    flat_features = np.array(padded_features).flatten()\n",
    "    flat_features = pad_features_to_fixed_length(flat_features, fixed_length)\n",
    "    feature_to_node_map = []\n",
    "    for node_id in range(len(graph.nodes)):\n",
    "        node_start_idx = node_id * max_length\n",
    "        node_end_idx = node_start_idx + max_length\n",
    "        feature_to_node_map.append((node_start_idx, node_end_idx, node_id))\n",
    "    return flat_features, feature_to_node_map\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, fixed_length=4716):\n",
    "    features = []\n",
    "    labels = []\n",
    "    feature_node_mappings = []\n",
    "    for data in dataset:\n",
    "        graph = nx.DiGraph()\n",
    "        for node_id in range(data.x.size(0)):\n",
    "            graph.add_node(node_id, features=data.x[node_id].numpy())\n",
    "        graph_features, feature_to_node_map = extract_graph_features(graph, fixed_length)\n",
    "        features.append(graph_features)\n",
    "        feature_node_mappings.append(feature_to_node_map)\n",
    "        if data.y.numel() == 1:\n",
    "            labels.append(data.y.item())\n",
    "        else:\n",
    "            labels.append(data.y[0].item())\n",
    "    return np.array(features), np.array(labels), feature_node_mappings\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_score = clf.predict_proba(X_test) if hasattr(clf, \"predict_proba\") else clf.decision_function(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo') if hasattr(clf,\n",
    "                                                                                               \"predict_proba\") else None\n",
    "    return accuracy, precision, recall, f1, roc_auc"
   ],
   "id": "78484160a0bf493c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T09:33:46.750260Z",
     "start_time": "2024-06-27T09:33:17.328338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare dataset with a fixed length of 4716 features\n",
    "fixed_length = 4716\n",
    "features, labels, _ = prepare_dataset(dataset, fixed_length)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "metrics = {\n",
    "    'RandomForest': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'SVM': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'DecisionTree': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'GaussianNB': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'GradientBoosting': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "\n",
    "    print(f\"*** {name} ***\\n\")\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(features, labels)):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Generate augmented training data\n",
    "        train_dataset = [dataset[i] for i in train_index]\n",
    "        augmented_train_dataset = generate_augmented_asts(train_dataset, num_augmentations=5)\n",
    "        X_train_augmented, y_train_augmented, _ = prepare_dataset(augmented_train_dataset, fixed_length)\n",
    "\n",
    "        acc, prec, rec, f1, roc_auc = evaluate_classifier(clf, X_train_augmented, y_train_augmented, X_test, y_test)\n",
    "        metrics[name]['accuracy'].append(acc)\n",
    "        metrics[name]['precision'].append(prec)\n",
    "        metrics[name]['recall'].append(rec)\n",
    "        metrics[name]['f1'].append(f1)\n",
    "        metrics[name]['roc_auc'].append(roc_auc)\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold + 1} - Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    print(\"........................................................\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "for name in classifiers.keys():\n",
    "    avg_accuracy = np.mean(metrics[name]['accuracy'])\n",
    "    avg_precision = np.mean(metrics[name]['precision'])\n",
    "    avg_recall = np.mean(metrics[name]['recall'])\n",
    "    avg_f1 = np.mean(metrics[name]['f1'])\n",
    "    avg_roc_auc = np.mean([x for x in metrics[name]['roc_auc'] if x is not None])\n",
    "\n",
    "    print(f\"\\n{name} Average Metrics over 10 folds:\")\n",
    "    print(f\"Accuracy .... : {avg_accuracy:.4f}\")\n",
    "    print(f\"Precision ... : {avg_precision:.4f}\")\n",
    "    print(f\"Recall ...... : {avg_recall:.4f}\")\n",
    "    print(f\"F1 Score .... : {avg_f1:.4f}\")\n",
    "    print(f\"ROC-AUC ..... : {avg_roc_auc:.4f}\")\n"
   ],
   "id": "5427d6c8076a81b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RandomForest ***\n",
      "\n",
      "Fold 1 - Accuracy: 0.9216, Precision: 0.9383, Recall: 0.9216, F1: 0.9227, AUC: 0.9959\n",
      "Fold 2 - Accuracy: 0.9216, Precision: 0.9264, Recall: 0.9216, F1: 0.9213, AUC: 0.9973\n",
      "Fold 3 - Accuracy: 0.9118, Precision: 0.9193, Recall: 0.9118, F1: 0.9113, AUC: 0.9955\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 32\u001B[0m\n\u001B[1;32m     30\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m [dataset[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m train_index]\n\u001B[1;32m     31\u001B[0m augmented_train_dataset \u001B[38;5;241m=\u001B[39m generate_augmented_asts(train_dataset, num_augmentations\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n\u001B[0;32m---> 32\u001B[0m X_train_augmented, y_train_augmented, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugmented_train_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m acc, prec, rec, f1, roc_auc \u001B[38;5;241m=\u001B[39m evaluate_classifier(clf, X_train_augmented, y_train_augmented, X_test, y_test)\n\u001B[1;32m     35\u001B[0m metrics[name][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(acc)\n",
      "Cell \u001B[0;32mIn[13], line 31\u001B[0m, in \u001B[0;36mprepare_dataset\u001B[0;34m(dataset, fixed_length)\u001B[0m\n\u001B[1;32m     29\u001B[0m graph \u001B[38;5;241m=\u001B[39m nx\u001B[38;5;241m.\u001B[39mDiGraph()\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node_id \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(data\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)):\n\u001B[0;32m---> 31\u001B[0m     graph\u001B[38;5;241m.\u001B[39madd_node(node_id, features\u001B[38;5;241m=\u001B[39m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m[\u001B[49m\u001B[43mnode_id\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     32\u001B[0m graph_features, feature_to_node_map \u001B[38;5;241m=\u001B[39m extract_graph_features(graph, fixed_length)\n\u001B[1;32m     33\u001B[0m features\u001B[38;5;241m.\u001B[39mappend(graph_features)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explainability",
   "id": "5bc99c15c92610bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T10:17:47.837671Z",
     "start_time": "2024-06-27T10:17:47.784581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import hashlib\n",
    "\n",
    "global_node_id = 0\n",
    "\n",
    "\n",
    "def reset_global_node_id():\n",
    "    global global_node_id\n",
    "    global_node_id = 0\n",
    "\n",
    "\n",
    "def assign_unique_id(node):\n",
    "    global global_node_id\n",
    "    node['_id'] = global_node_id\n",
    "    global_node_id += 1\n",
    "\n",
    "\n",
    "def traverse_and_assign_ids(node):\n",
    "    if isinstance(node, dict):\n",
    "        assign_unique_id(node)\n",
    "        for key, value in node.items():\n",
    "            traverse_and_assign_ids(value)\n",
    "    elif isinstance(node, list):\n",
    "        for item in node:\n",
    "            traverse_and_assign_ids(item)\n",
    "\n",
    "\n",
    "def load_data(ast_directory, label_map):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(ast_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    ast = json.load(f)\n",
    "                reset_global_node_id()\n",
    "                traverse_and_assign_ids(ast)\n",
    "                dataset.append(ast)\n",
    "                label_folder = root.split(os.sep)[-1]\n",
    "                label = label_map[label_folder]\n",
    "                labels.append(label)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def hash_feature(value, num_bins=1000):\n",
    "    return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % num_bins\n",
    "\n",
    "\n",
    "def extract_features(node):\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    type_feature = [hash_feature(node_type)]\n",
    "    name_feature = [hash_feature(node.get('name', ''))] if 'name' in node else [0]\n",
    "    value_feature = [hash_feature(node.get('value', ''))] if 'value' in node else [0]\n",
    "    src_feature = [0, 0]\n",
    "    if 'src' in node:\n",
    "        start, length, *_ = map(int, node['src'].split(':'))\n",
    "        src_feature = [start, length]\n",
    "    type_desc_features = []\n",
    "    if 'typeDescriptions' in node:\n",
    "        type_desc = node['typeDescriptions']\n",
    "        type_desc_features.append(hash_feature(type_desc.get('typeString', '')))\n",
    "        type_desc_features.append(hash_feature(type_desc.get('typeIdentifier', '')))\n",
    "    state_mutability_feature = [hash_feature(node.get('stateMutability', ''))] if 'stateMutability' in node else [0]\n",
    "    visibility_feature = [hash_feature(node.get('visibility', ''))] if 'visibility' in node else [0]\n",
    "    features = type_feature + name_feature + value_feature + src_feature + type_desc_features + state_mutability_feature + visibility_feature\n",
    "    return features\n",
    "\n",
    "\n",
    "def pad_features_to_fixed_length(features, fixed_length=4716):\n",
    "    padding_length = fixed_length - len(features)\n",
    "    if padding_length > 0:\n",
    "        return np.concatenate([features, np.zeros(padding_length)])\n",
    "    else:\n",
    "        return features[:fixed_length]\n",
    "\n",
    "\n",
    "def extract_graph_features_with_mapping(ast, fixed_length=4716):\n",
    "    graph = nx.DiGraph()\n",
    "    node_features = []\n",
    "    feature_to_node_map = []\n",
    "\n",
    "    for node in ast:\n",
    "        if isinstance(node, dict) and '_id' in node:\n",
    "            node_id = node['_id']\n",
    "            features = extract_features(node)\n",
    "            graph.add_node(node_id, features=features)\n",
    "            node_features.append(features)\n",
    "            feature_to_node_map.append((len(node_features) - 1, len(node_features), node_id))\n",
    "\n",
    "    max_length = max(len(f) for f in node_features)\n",
    "    padded_features = [pad_features_to_fixed_length(f, max_length) for f in node_features]\n",
    "    flat_features = np.array(padded_features).flatten()\n",
    "    flat_features = pad_features_to_fixed_length(flat_features, fixed_length)\n",
    "\n",
    "    return flat_features, feature_to_node_map\n",
    "\n",
    "\n",
    "def prepare_dataset_with_mapping(dataset, fixed_length=4716):\n",
    "    features = []\n",
    "    labels = []\n",
    "    feature_node_mappings = []\n",
    "\n",
    "    for ast in dataset:\n",
    "        graph_features, feature_to_node_map = extract_graph_features_with_mapping(ast, fixed_length)\n",
    "        features.append(graph_features)\n",
    "        feature_node_mappings.append(feature_to_node_map)\n",
    "        labels.append(ast[0]['y'])\n",
    "\n",
    "    return np.array(features), np.array(labels), feature_node_mappings\n",
    "\n",
    "\n",
    "def get_important_nodes(feature_importances, feature_to_node_map, num_top_features=10):\n",
    "    important_features_indices = np.argsort(feature_importances)[-num_top_features:]\n",
    "    important_nodes = set()\n",
    "    for feature_idx in important_features_indices:\n",
    "        for start_idx, end_idx, node_id in feature_to_node_map:\n",
    "            if start_idx <= feature_idx < end_idx:\n",
    "                important_nodes.add(node_id)\n",
    "                break\n",
    "    return important_nodes\n",
    "\n",
    "\n",
    "def extract_lines_of_code_from_nodes(ast, important_nodes):\n",
    "    lines_of_code = set()\n",
    "\n",
    "    def traverse_ast(node):\n",
    "        if isinstance(node, dict):\n",
    "            node_id = node['_id']\n",
    "            if node_id in important_nodes:\n",
    "                if 'src' in node:\n",
    "                    start, length, *_ = map(int, node['src'].split(':'))\n",
    "                    lines_of_code.add((start, length))\n",
    "            for key, value in node.items():\n",
    "                traverse_ast(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                traverse_ast(item)\n",
    "\n",
    "    traverse_ast(ast)\n",
    "    return lines_of_code\n",
    "\n",
    "\n",
    "def highlight_important_lines(ast, feature_importances, feature_to_node_map, num_top_features=10):\n",
    "    important_nodes = get_important_nodes(feature_importances, feature_to_node_map, num_top_features)\n",
    "    important_lines = extract_lines_of_code_from_nodes(ast, important_nodes)\n",
    "    return important_lines\n",
    "\n",
    "\n",
    "def get_feature_importance(clf):\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        return clf.feature_importances_\n",
    "    else:\n",
    "        raise ValueError(f\"Model of type {type(clf)} does not support feature importances.\")\n"
   ],
   "id": "d1ec035bdae8c8b0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T10:17:50.956511Z",
     "start_time": "2024-06-27T10:17:48.895031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    ast_directory = '../dataset/aisc/ast'\n",
    "    label_map = generate_label_map(ast_directory)\n",
    "    dataset, labels = load_data(ast_directory, label_map)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data loaded. Please check the dataset directory and files.\")\n",
    "        return\n",
    "\n",
    "    # Prepare dataset with a fixed length of 4716 features and get feature-to-node mappings\n",
    "    fixed_length = 4716\n",
    "    features, labels, feature_node_mappings = prepare_dataset_with_mapping(dataset, fixed_length)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(features, labels)\n",
    "\n",
    "    feature_importances = get_feature_importance(clf)\n",
    "\n",
    "    important_lines_per_ast = []\n",
    "    for i, ast in enumerate(dataset):\n",
    "        important_lines = highlight_important_lines(ast, feature_importances, feature_node_mappings[i])\n",
    "        important_lines_per_ast.append(important_lines)\n",
    "\n",
    "    for i, important_lines in enumerate(important_lines_per_ast):\n",
    "        print(f\"AST {i} Important lines: {important_lines}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "1c70618f414538e5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 29\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAST \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Important lines: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimportant_lines\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 29\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 12\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Prepare dataset with a fixed length of 4716 features and get feature-to-node mappings\u001B[39;00m\n\u001B[1;32m     11\u001B[0m fixed_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4716\u001B[39m\n\u001B[0;32m---> 12\u001B[0m features, labels, feature_node_mappings \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_dataset_with_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m clf \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     15\u001B[0m clf\u001B[38;5;241m.\u001B[39mfit(features, labels)\n",
      "Cell \u001B[0;32mIn[21], line 108\u001B[0m, in \u001B[0;36mprepare_dataset_with_mapping\u001B[0;34m(dataset, fixed_length)\u001B[0m\n\u001B[1;32m    105\u001B[0m feature_node_mappings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ast \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[0;32m--> 108\u001B[0m     graph_features, feature_to_node_map \u001B[38;5;241m=\u001B[39m \u001B[43mextract_graph_features_with_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m     features\u001B[38;5;241m.\u001B[39mappend(graph_features)\n\u001B[1;32m    110\u001B[0m     feature_node_mappings\u001B[38;5;241m.\u001B[39mappend(feature_to_node_map)\n",
      "Cell \u001B[0;32mIn[21], line 94\u001B[0m, in \u001B[0;36mextract_graph_features_with_mapping\u001B[0;34m(ast, fixed_length)\u001B[0m\n\u001B[1;32m     91\u001B[0m         node_features\u001B[38;5;241m.\u001B[39mappend(features)\n\u001B[1;32m     92\u001B[0m         feature_to_node_map\u001B[38;5;241m.\u001B[39mappend((\u001B[38;5;28mlen\u001B[39m(node_features) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(node_features), node_id))\n\u001B[0;32m---> 94\u001B[0m max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m padded_features \u001B[38;5;241m=\u001B[39m [pad_features_to_fixed_length(f, max_length) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m node_features]\n\u001B[1;32m     96\u001B[0m flat_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(padded_features)\u001B[38;5;241m.\u001B[39mflatten()\n",
      "\u001B[0;31mValueError\u001B[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdef5539d0664143"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
