{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.713122Z",
     "start_time": "2024-07-24T09:27:12.706206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Set\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "from tqdm import tqdm\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn\n"
   ],
   "id": "23581b0e55d01ac3",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config",
   "id": "a60e58b20901dd49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.752575Z",
     "start_time": "2024-07-24T09:27:12.744422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_FOLDS = 5\n",
    "print(f\"NUM_FOLDS set to {NUM_FOLDS}\")\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "print(f\"NUM_EPOCHS set to {NUM_EPOCHS}\")\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "print(f\"HIDDEN_SIZE set to {HIDDEN_SIZE}\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "print(f\"BATCH_SIZE set to {BATCH_SIZE}\")\n",
    "\n",
    "LR = 0.01\n",
    "print(f\"LR set to {LR}\")\n",
    "\n",
    "NUM_AUGMENTATIONS = 5\n",
    "print(f\"NUM_AUGMENTATIONS set to {NUM_AUGMENTATIONS}\")\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "print(f\"TEST_SIZE set to {TEST_SIZE}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"DEVICE set to {DEVICE}\")\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(f\"Random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# Creating the log directory if it doesn't exist\n",
    "LOG_DIR = os.path.join(\"log\")\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "    print(f\"Log directory created at {LOG_DIR}\")\n",
    "else:\n",
    "    print(f\"Log directory already exists at {LOG_DIR}\")"
   ],
   "id": "3fed77b73fde9b64",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_FOLDS set to 5\n",
      "NUM_EPOCHS set to 15\n",
      "HIDDEN_SIZE set to 128\n",
      "BATCH_SIZE set to 32\n",
      "LR set to 0.01\n",
      "NUM_AUGMENTATIONS set to 5\n",
      "TEST_SIZE set to 0.1\n",
      "DEVICE set to cpu\n",
      "Random seeds set to 0\n",
      "Log directory already exists at log\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset and Augmentation",
   "id": "abeead762cbb1622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.758731Z",
     "start_time": "2024-07-24T09:27:12.754990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs: List[Data]):\n",
    "        self.graphs = graphs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]"
   ],
   "id": "911d1e680a10a170",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.772087Z",
     "start_time": "2024-07-24T09:27:12.760762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ASTAugmentor:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the ASTAugmentor with predefined strategies.\"\"\"\n",
    "        self.substitutions = {'FunctionDefinition': 'ModifierDefinition'}\n",
    "        self.insertions = {\n",
    "            'body': {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}}\n",
    "        self.deletions = {'ModifierDefinition'}\n",
    "        self.renames = {'oldVarName': 'newVarName', 'oldFuncName': 'newFuncName'}\n",
    "\n",
    "    def substitute_nodes(self, ast: Any, substitutions: Dict[str, str]) -> Any:\n",
    "        \"\"\"Substitute certain nodes in the AST with other semantically equivalent nodes.\"\"\"\n",
    "        if isinstance(ast, dict):\n",
    "            for key, value in ast.items():\n",
    "                if key in substitutions:\n",
    "                    ast[key] = substitutions[key]\n",
    "                else:\n",
    "                    ast[key] = self.substitute_nodes(value, substitutions)\n",
    "        elif isinstance(ast, list):\n",
    "            for i in range(len(ast)):\n",
    "                ast[i] = self.substitute_nodes(ast[i], substitutions)\n",
    "        return ast\n",
    "\n",
    "    def insert_nodes(self, ast: Any, insertions: Dict[str, Any]) -> Any:\n",
    "        \"\"\"Insert certain nodes into the AST.\"\"\"\n",
    "        if isinstance(ast, dict):\n",
    "            for key, value in ast.items():\n",
    "                if key in insertions:\n",
    "                    ast[key] = [value, insertions[key]] if isinstance(value, list) else [value, insertions[key]]\n",
    "                else:\n",
    "                    ast[key] = self.insert_nodes(value, insertions)\n",
    "        elif isinstance(ast, list):\n",
    "            for i in range(len(ast)):\n",
    "                ast[i] = self.insert_nodes(ast[i], insertions)\n",
    "        return ast\n",
    "\n",
    "    def delete_nodes(self, ast: Any, deletions: Set[str]) -> Any:\n",
    "        \"\"\"Delete certain nodes from the AST.\"\"\"\n",
    "        if isinstance(ast, dict):\n",
    "            keys_to_delete = [key for key in ast if key in deletions]\n",
    "            for key in keys_to_delete:\n",
    "                del ast[key]\n",
    "            for key, value in ast.items():\n",
    "                ast[key] = self.delete_nodes(value, deletions)\n",
    "        elif isinstance(ast, list):\n",
    "            ast = [self.delete_nodes(item, deletions) for item in ast if item not in deletions]\n",
    "        return ast\n",
    "\n",
    "    def rename_identifiers(self, ast: Any, renames: Dict[str, str]) -> Any:\n",
    "        \"\"\"Rename variables/functions in the AST.\"\"\"\n",
    "        if isinstance(ast, dict):\n",
    "            for key, value in ast.items():\n",
    "                if key == 'name' and value in renames:\n",
    "                    ast[key] = renames[value]\n",
    "                else:\n",
    "                    ast[key] = self.rename_identifiers(value, renames)\n",
    "        elif isinstance(ast, list):\n",
    "            for i in range(len(ast)):\n",
    "                ast[i] = self.rename_identifiers(ast[i], renames)\n",
    "        return ast\n",
    "\n",
    "    def reorder_statements(self, ast: Any) -> Any:\n",
    "        \"\"\"Randomly reorder statements in the AST.\"\"\"\n",
    "        if isinstance(ast, dict) and 'body' in ast:\n",
    "            if isinstance(ast['body'], list):\n",
    "                random.shuffle(ast['body'])\n",
    "            else:\n",
    "                self.reorder_statements(ast['body'])\n",
    "        elif isinstance(ast, list):\n",
    "            for item in ast:\n",
    "                self.reorder_statements(item)\n",
    "        return ast\n",
    "\n",
    "    def add_no_op_statements(self, ast: Any) -> Any:\n",
    "        \"\"\"Add no-op statements to the AST.\"\"\"\n",
    "        no_op_statement = {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}\n",
    "        if isinstance(ast, dict) and 'body' in ast:\n",
    "            if isinstance(ast['body'], list):\n",
    "                ast['body'].append(no_op_statement)\n",
    "            else:\n",
    "                self.add_no_op_statements(ast['body'])\n",
    "        elif isinstance(ast, list):\n",
    "            for item in ast:\n",
    "                self.add_no_op_statements(item)\n",
    "        return ast\n",
    "\n",
    "    def apply_augmentation(self, ast: Any) -> Any:\n",
    "        \"\"\"Apply random augmentations to the AST.\"\"\"\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.substitute_nodes(ast, self.substitutions)\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.insert_nodes(ast, self.insertions)\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.delete_nodes(ast, self.deletions)\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.rename_identifiers(ast, self.renames)\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.reorder_statements(ast)\n",
    "        if random.random() > 0.5:\n",
    "            ast = self.add_no_op_statements(ast)\n",
    "        return ast\n",
    "\n",
    "    def generate_augmented_asts(self, dataset: List[Any], num_augmentations: int = 5) -> List[Any]:\n",
    "        \"\"\"Generate augmented ASTs for each AST in the dataset.\"\"\"\n",
    "        augmented_dataset = []\n",
    "        for ast in tqdm(dataset, desc=\"Generating augmented ASTs\"):\n",
    "            augmented_dataset.append(ast)\n",
    "            for _ in range(num_augmentations):\n",
    "                augmented_ast = self.apply_augmentation(copy.deepcopy(ast))\n",
    "                augmented_dataset.append(augmented_ast)\n",
    "        return augmented_dataset"
   ],
   "id": "919e05313241418a",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.776017Z",
     "start_time": "2024-07-24T09:27:12.773404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AugmentedGraphDataset(GraphDataset):\n",
    "    def __init__(self, graphs: List[Any], num_augmentations: int = 5):\n",
    "        super().__init__(graphs)\n",
    "        self.augmentor = ASTAugmentor()\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.augmented_graphs = self._augment_graphs()\n",
    "\n",
    "    def _augment_graphs(self) -> List[Any]:\n",
    "        return self.augmentor.generate_augmented_asts(self.graphs, self.num_augmentations)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.augmented_graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.augmented_graphs[idx]"
   ],
   "id": "bc6339b884223ef0",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "412905987be26ead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.792087Z",
     "start_time": "2024-07-24T09:27:12.787463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureExtractor:\n",
    "    @staticmethod\n",
    "    def hash_feature(value: Any, num_bins: int = 1000) -> int:\n",
    "        \"\"\"Helper function to hash a value into a fixed number of bins.\"\"\"\n",
    "        return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % num_bins\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_features(node: Dict) -> List[int]:\n",
    "        \"\"\"Extract features from a given AST node.\"\"\"\n",
    "        # Initialize features with default values\n",
    "        name_feature, value_feature = [0], [0]\n",
    "        src_feature, type_desc_features = [0, 0], [0, 0]\n",
    "        state_mutability_feature, visibility_feature = [0], [0]\n",
    "\n",
    "        # Extract basic features\n",
    "        node_type = node.get('nodeType', 'Unknown')\n",
    "        type_feature = [FeatureExtractor.hash_feature(node_type)]\n",
    "\n",
    "        # Extract additional features if they exist\n",
    "        if 'name' in node:\n",
    "            name_feature = [FeatureExtractor.hash_feature(node.get('name', ''))]\n",
    "        if 'value' in node:\n",
    "            value_feature = [FeatureExtractor.hash_feature(node.get('value', ''))]\n",
    "\n",
    "        # Extract src features (start, end, and length if available)\n",
    "        if 'src' in node:\n",
    "            start, length, *_ = map(int, node['src'].split(':'))\n",
    "            src_feature = [start, length]\n",
    "\n",
    "        # Extract typeDescriptions features if they exist\n",
    "        if 'typeDescriptions' in node:\n",
    "            type_desc = node['typeDescriptions']\n",
    "            type_desc_features = [FeatureExtractor.hash_feature(type_desc.get('typeString', '')),\n",
    "                                  FeatureExtractor.hash_feature(type_desc.get('typeIdentifier', ''))]\n",
    "\n",
    "        # Extract stateMutability if it exists\n",
    "        if 'stateMutability' in node:\n",
    "            state_mutability_feature = [FeatureExtractor.hash_feature(node.get('stateMutability', ''))]\n",
    "\n",
    "        # Extract visibility if it exists\n",
    "        if 'visibility' in node:\n",
    "            visibility_feature = [FeatureExtractor.hash_feature(node.get('visibility', ''))]\n",
    "\n",
    "        # Combine all features into a single feature vector\n",
    "        return (type_feature + name_feature + value_feature + src_feature +\n",
    "                type_desc_features + state_mutability_feature + visibility_feature)\n"
   ],
   "id": "ab53f6dd1c75970f",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading",
   "id": "a6a935e8ee0ab0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:12.807366Z",
     "start_time": "2024-07-24T09:27:12.796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ASTGraphConverter:\n",
    "    @staticmethod\n",
    "    def add_masks_to_data(data: Data) -> Data:\n",
    "        \"\"\"Add train, validation, and test masks to the data.\"\"\"\n",
    "        num_nodes = data.x.size(0)\n",
    "        indices = torch.randperm(num_nodes)\n",
    "\n",
    "        train_size = int(num_nodes * (1 - TEST_SIZE))\n",
    "        val_size = int(num_nodes * TEST_SIZE)\n",
    "\n",
    "        train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "        train_mask[indices[:train_size]] = True\n",
    "        val_mask[indices[train_size:train_size + val_size]] = True\n",
    "        test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "        data.train_mask = train_mask\n",
    "        data.val_mask = val_mask\n",
    "        data.test_mask = test_mask\n",
    "\n",
    "        return data\n",
    "\n",
    "    def ast_to_graph(self, ast_json: Dict) -> Data:\n",
    "        \"\"\"Convert an AST JSON object to a PyTorch Geometric Data object.\"\"\"\n",
    "        graph = nx.DiGraph()\n",
    "        node_id = 0\n",
    "\n",
    "        def add_nodes_edges(node: Dict, parent: int = None):\n",
    "            nonlocal node_id\n",
    "            current_node_id = node_id\n",
    "            graph.add_node(current_node_id, features=FeatureExtractor.extract_features(node))\n",
    "            if parent is not None:\n",
    "                graph.add_edge(parent, current_node_id)\n",
    "            node_id += 1\n",
    "            for key, value in node.items():\n",
    "                if isinstance(value, dict):\n",
    "                    add_nodes_edges(value, current_node_id)\n",
    "                elif isinstance(value, list):\n",
    "                    for item in value:\n",
    "                        if isinstance(item, dict):\n",
    "                            add_nodes_edges(item, current_node_id)\n",
    "\n",
    "        add_nodes_edges(ast_json)\n",
    "        edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "        x = torch.stack([torch.tensor(graph.nodes[n]['features'], dtype=torch.float) for n in graph.nodes])\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data = self.add_masks_to_data(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "class DataFetcher:\n",
    "    def __init__(self, data_dir: str):\n",
    "        \"\"\"Initialize the DataLoader with the AST directory, label map, and graph converter.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.label_map = self.generate_label_map()\n",
    "        self.graph_converter = ASTGraphConverter()\n",
    "\n",
    "    def generate_label_map(self) -> Dict[str, int]:\n",
    "        \"\"\"Generate a label map from the directory structure.\"\"\"\n",
    "        print(f\"Generating label map from directory: {self.data_dir}\")\n",
    "        label_map = {}\n",
    "        label_index = 0\n",
    "        for category in os.listdir(self.data_dir):\n",
    "            category_path = os.path.join(self.data_dir, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                label_map[category] = label_index\n",
    "                label_index += 1\n",
    "        print(f\"Label map generated with {len(label_map)} labels\")\n",
    "        return label_map\n",
    "\n",
    "    def load_data(self) -> Tuple[List[Data], List[int]]:\n",
    "        \"\"\"Load data from the AST directory and return a dataset and labels.\"\"\"\n",
    "        print(f\"Loading data from directory: {ast_directory}\")\n",
    "        dataset = []\n",
    "        for category in tqdm(os.listdir(self.data_dir), desc=\"Processing categories\"):\n",
    "            category_path = os.path.join(self.data_dir, category)\n",
    "            if os.path.isdir(category_path):\n",
    "                for root, _, files in os.walk(category_path):\n",
    "                    for file in files:\n",
    "                        if file.endswith('.json'):\n",
    "                            filepath = os.path.join(root, file)\n",
    "                            with open(filepath, 'r') as f:\n",
    "                                ast = json.load(f)\n",
    "                            data = self.graph_converter.ast_to_graph(ast)\n",
    "                            label = self.label_map[category]\n",
    "                            data.y = torch.tensor([label] * data.x.size(0),\n",
    "                                                  dtype=torch.long)  # Assign label to all nodes\n",
    "                            dataset.append(data)\n",
    "        print(f\"Loaded {len(dataset)} samples from {self.data_dir}\")\n",
    "        return dataset"
   ],
   "id": "a003fe9d27ab8c41",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:17.738836Z",
     "start_time": "2024-07-24T09:27:12.809788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the directory for the AST data\n",
    "ast_directory = os.path.join(\"..\", \"dataset\", \"aisc\", \"ast\")\n",
    "\n",
    "# Load the data using the data loader\n",
    "graphs = DataFetcher(ast_directory).load_data()\n",
    "\n",
    "# Check if any data was loaded\n",
    "if len(graphs) == 0:\n",
    "    print(\"No data loaded. Please check the dataset directory and files.\")\n",
    "else:\n",
    "    print(f\"Data loaded successfully with {len(graphs)} samples.\")\n",
    "\n",
    "    # Create a custom dataset\n",
    "    DATASET = GraphDataset(graphs)\n",
    "    AUGMENTED_DATASET = AugmentedGraphDataset(graphs)\n",
    "\n",
    "    # Initialize NUM_LABELS and NUM_FEATURES\n",
    "    all_labels = torch.cat([data.y for data in graphs], dim=0)\n",
    "    NUM_LABELS = len(torch.unique(all_labels))\n",
    "    NUM_FEATURES = graphs[0].x.shape[1]  # Assuming all graphs have the same number of features\n",
    "    print(f\"Number of labels: {NUM_LABELS}\")\n",
    "    print(f\"Number of features: {NUM_FEATURES}\")\n"
   ],
   "id": "f29e7e0459d48563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating label map from directory: ../dataset/aisc/ast\n",
      "Label map generated with 11 labels\n",
      "Loading data from directory: ../dataset/aisc/ast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 100%|██████████| 11/11 [00:03<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2040 samples from ../dataset/aisc/ast\n",
      "Data loaded successfully with 2040 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating augmented ASTs: 100%|██████████| 2040/2040 [00:01<00:00, 1978.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 11\n",
      "Number of features: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training and Cross-validation",
   "id": "c359705e9c0f38a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:17.745893Z",
     "start_time": "2024-07-24T09:27:17.740341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels: List[Any], pred_labels: List[Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the given true and predicted labels.\n",
    "\n",
    "    :param true_labels: The ground truth labels.\n",
    "    :param pred_labels: The predicted labels.\n",
    "    :return: A dictionary containing precision, recall, F1 score, and accuracy.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='weighted', zero_division=0),\n",
    "        \"accuracy\": accuracy_score(true_labels, pred_labels)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the results to a CSV file.\n",
    "\n",
    "    :param results: The results to save, typically a list of dictionaries.\n",
    "    :param filename: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(LOG_DIR, filename), index=False)\n",
    "    print(f\"All fold results saved to '{os.path.join(LOG_DIR, filename)}'\")"
   ],
   "id": "1a3f037a03b07325",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:17.756381Z",
     "start_time": "2024-07-24T09:27:17.747711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for handling the training and evaluation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model, loss criterion, and optimizer.\n",
    "\n",
    "        :param model: The neural network model to be trained.\n",
    "        \"\"\"\n",
    "        self.__untrained_model = model\n",
    "        self._model = model.to(DEVICE)\n",
    "        self._optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "        self._loss_fn = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "    def reset_model(self):\n",
    "        \"\"\"\n",
    "        Reset the model to its initial untrained state.\n",
    "        \"\"\"\n",
    "        self._model = self.__untrained_model\n",
    "        self._optimizer = torch.optim.Adam(self.__untrained_model.parameters(), lr=LR)\n",
    "\n",
    "    def _evaluate_batch(self, batch: Data) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A Data object containing the batch of graphs.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch to the appropriate device (CPU/GPU)\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # Prepare the inputs for the model\n",
    "        inputs, labels = batch.x, batch.y\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(batch)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            batch_metrics = compute_metrics(labels.cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Data) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A Data object containing the batch of graphs.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch to the appropriate device (CPU/GPU)\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        # Prepare inputs for the model\n",
    "        inputs, labels = batch.x, batch.y\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(batch)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def run_epoch(self, dataloader: DataLoader, train_mode: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single epoch of training or evaluation.\n",
    "\n",
    "        :param dataloader: DataLoader providing the data for the epoch.\n",
    "        :param train_mode: Boolean flag indicating whether to train or evaluate.\n",
    "        :return: A tuple containing the average loss and a dictionary of average metrics.\n",
    "        \"\"\"\n",
    "        # Set the mode for the epoch (Training or Testing)\n",
    "        phase = 'Training' if train_mode else 'Testing'\n",
    "        self._model.train() if train_mode else self._model.eval()\n",
    "\n",
    "        losses, metrics_list = [], []\n",
    "\n",
    "        # Iterate over the data loader\n",
    "        for batch in tqdm(dataloader, desc=phase):\n",
    "            loss, batch_metrics = self._train_batch(batch) if train_mode else self._evaluate_batch(batch)\n",
    "\n",
    "            # Accumulate the loss and metrics\n",
    "            losses.append(loss)\n",
    "            metrics_list.append(batch_metrics)\n",
    "\n",
    "        # Compute average loss and metrics for the epoch\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "\n",
    "        return avg_loss, avg_metrics\n"
   ],
   "id": "3cd4528abe44888c",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:17.765269Z",
     "start_time": "2024-07-24T09:27:17.757903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    CrossValidator class for handling stratified k-fold cross-validation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer: Trainer, train_data: Dataset, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Initialize the CrossValidator with trainer, training data, and test data.\n",
    "\n",
    "        :param trainer: An instance of the Trainer class.\n",
    "        :param train_data: The training dataset.\n",
    "        :param test_data: The test dataset.\n",
    "        \"\"\"\n",
    "        self.__trainer = trainer\n",
    "        self.__train_data = train_data\n",
    "        self.__test_data = test_data\n",
    "\n",
    "    def __train_and_evaluate(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a specified number of epochs.\n",
    "\n",
    "        :param train_dataloader: DataLoader for the training data.\n",
    "        :param test_dataloader: DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n --- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "            # Train the model and print training metrics\n",
    "            avg_train_loss, avg_train_metrics = self.__trainer.run_epoch(train_dataloader, train_mode=True)\n",
    "            print(f\"\\n TRAIN | Loss: {avg_train_loss:.4f} |\"\n",
    "                  f\" Accuracy: {avg_train_metrics['accuracy']:.4f},\"\n",
    "                  f\" Precision: {avg_train_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_train_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_train_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "            # Evaluate the model on the validation set and print validation metrics\n",
    "            avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "            print(f\" VALID | Loss: {avg_test_loss:.4f} |\"\n",
    "                  f\" Accuracy: {avg_test_metrics['accuracy']:.4f},\"\n",
    "                  f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "    def __evaluate_on_test_set(self, test_dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "\n",
    "        :param test_dataloader: DataLoader for the test data.\n",
    "        :return: A dictionary of test set metrics.\n",
    "        \"\"\"\n",
    "        avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "\n",
    "        # Print test set metrics\n",
    "        print(f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} |\"\n",
    "              f\" Accuracy: {avg_test_metrics['accuracy']:.4f},\"\n",
    "              f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "              f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "              f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "        return avg_test_metrics\n",
    "\n",
    "    def k_fold_cv(self, log_id: str = \"gcn\") -> None:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        :param log_id: Identifier for logging purposes, typically the model name.\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Extract labels for stratification\n",
    "        labels = [data.y[0].item() for data in self.__train_data]\n",
    "\n",
    "        # Iterate over each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "            # Create data loaders for training and validation sets\n",
    "            train_subset = Subset(self.__train_data, train_idx)\n",
    "            val_subset = Subset(self.__train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            print(f\"Starting Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "            # Train and evaluate the model for the current fold\n",
    "            self.__train_and_evaluate(train_loader, val_loader)\n",
    "\n",
    "            # Evaluate on the test set after each fold\n",
    "            test_loader = DataLoader(self.__test_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            metrics = self.__evaluate_on_test_set(test_loader)\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "            # Reset the model to untrained\n",
    "            self.__trainer.reset_model()\n",
    "\n",
    "        # Calculate average and standard deviation of each metric across all folds\n",
    "        metric_keys = fold_metrics[0].keys()  # Assuming all metrics dictionaries have the same structure\n",
    "        average_metrics = {key: np.mean([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "        std_dev_metrics = {key: np.std([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "\n",
    "        # Print average metrics and their standard deviations\n",
    "        print(\"Average Metrics Over All Folds:\")\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f\"{key}: {value:.4f} (±{std_dev_metrics[key]:.4f})\")\n",
    "\n",
    "        # Save metrics to CSV file\n",
    "        save_results(fold_metrics, filename=f\"{log_id}.csv\")"
   ],
   "id": "ea274ef13efa8ac6",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph Neural Network",
   "id": "f6c4e9bc867252e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:27:17.769438Z",
     "start_time": "2024-07-24T09:27:17.766275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, num_labels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_size)\n",
    "        self.bn1 = BatchNorm(hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, hidden_size)\n",
    "        self.bn2 = BatchNorm(hidden_size)\n",
    "        self.conv3 = GCNConv(hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "\n",
    "        # Third layer (output layer)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "id": "9dfeca41b673a73d",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:36:22.039706Z",
     "start_time": "2024-07-24T09:27:17.770326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GCN(NUM_FEATURES, HIDDEN_SIZE, NUM_LABELS)\n",
    "TRAIN_DATASET, TEST_DATASET = torch.utils.data.random_split(AUGMENTED_DATASET, [1 - TEST_SIZE, TEST_SIZE],\n",
    "                                                            generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "\n",
    "CrossValidator(Trainer(model), TRAIN_DATASET, TEST_DATASET).k_fold_cv(log_id=\"gcn\")"
   ],
   "id": "6d732648de0dc590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/5\n",
      "\n",
      " --- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.8350 | Accuracy: 0.4891, Precision: 0.3262, Recall: 0.4891, F1: 0.3394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 66.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.7140 | Accuracy: 0.4975, Precision: 0.2972, Recall: 0.4975, F1: 0.3501\n",
      "\n",
      "\n",
      " --- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 30.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.7331 | Accuracy: 0.4985, Precision: 0.3641, Recall: 0.4985, F1: 0.3455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 58.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.6959 | Accuracy: 0.4965, Precision: 0.3606, Recall: 0.4965, F1: 0.3522\n",
      "\n",
      "\n",
      " --- Epoch 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.7033 | Accuracy: 0.4976, Precision: 0.3820, Recall: 0.4976, F1: 0.3474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 56.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.6499 | Accuracy: 0.4982, Precision: 0.3604, Recall: 0.4982, F1: 0.3506\n",
      "\n",
      "\n",
      " --- Epoch 4/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6849 | Accuracy: 0.4977, Precision: 0.4074, Recall: 0.4977, F1: 0.3513\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 53.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.6287 | Accuracy: 0.4948, Precision: 0.3882, Recall: 0.4948, F1: 0.3576\n",
      "\n",
      "\n",
      " --- Epoch 5/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 27.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6695 | Accuracy: 0.4972, Precision: 0.4125, Recall: 0.4972, F1: 0.3508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 68.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.6051 | Accuracy: 0.4970, Precision: 0.4177, Recall: 0.4970, F1: 0.3585\n",
      "\n",
      "\n",
      " --- Epoch 6/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6589 | Accuracy: 0.4968, Precision: 0.4309, Recall: 0.4968, F1: 0.3544\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 66.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5991 | Accuracy: 0.4974, Precision: 0.4206, Recall: 0.4974, F1: 0.3569\n",
      "\n",
      "\n",
      " --- Epoch 7/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 29.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6506 | Accuracy: 0.4968, Precision: 0.4307, Recall: 0.4968, F1: 0.3555\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 59.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.6158 | Accuracy: 0.4988, Precision: 0.3496, Recall: 0.4988, F1: 0.3404\n",
      "\n",
      "\n",
      " --- Epoch 8/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6423 | Accuracy: 0.4964, Precision: 0.4515, Recall: 0.4964, F1: 0.3550\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 66.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5865 | Accuracy: 0.4980, Precision: 0.3813, Recall: 0.4980, F1: 0.3498\n",
      "\n",
      "\n",
      " --- Epoch 9/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6350 | Accuracy: 0.4958, Precision: 0.4476, Recall: 0.4958, F1: 0.3557\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 65.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5747 | Accuracy: 0.4959, Precision: 0.4267, Recall: 0.4959, F1: 0.3594\n",
      "\n",
      "\n",
      " --- Epoch 10/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6316 | Accuracy: 0.4963, Precision: 0.4544, Recall: 0.4963, F1: 0.3575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 69.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5796 | Accuracy: 0.4981, Precision: 0.3542, Recall: 0.4981, F1: 0.3458\n",
      "\n",
      "\n",
      " --- Epoch 11/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6263 | Accuracy: 0.4965, Precision: 0.4618, Recall: 0.4965, F1: 0.3575\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 65.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5710 | Accuracy: 0.4975, Precision: 0.3336, Recall: 0.4975, F1: 0.3492\n",
      "\n",
      "\n",
      " --- Epoch 12/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6197 | Accuracy: 0.4980, Precision: 0.4629, Recall: 0.4980, F1: 0.3597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 63.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5658 | Accuracy: 0.4967, Precision: 0.3711, Recall: 0.4967, F1: 0.3528\n",
      "\n",
      "\n",
      " --- Epoch 13/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6209 | Accuracy: 0.4973, Precision: 0.4661, Recall: 0.4973, F1: 0.3574\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 52.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5679 | Accuracy: 0.4962, Precision: 0.4225, Recall: 0.4962, F1: 0.3617\n",
      "\n",
      "\n",
      " --- Epoch 14/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 29.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6175 | Accuracy: 0.4963, Precision: 0.4670, Recall: 0.4963, F1: 0.3598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 70.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5678 | Accuracy: 0.4985, Precision: 0.3886, Recall: 0.4985, F1: 0.3489\n",
      "\n",
      "\n",
      " --- Epoch 15/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6135 | Accuracy: 0.4958, Precision: 0.4704, Recall: 0.4958, F1: 0.3605\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 70.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5614 | Accuracy: 0.4985, Precision: 0.3540, Recall: 0.4985, F1: 0.3457\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 39/39 [00:00<00:00, 73.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 1.5559 | Accuracy: 0.5056, Precision: 0.3684, Recall: 0.5056, F1: 0.3551\n",
      "\n",
      "Starting Fold 2/5\n",
      "\n",
      " --- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6136 | Accuracy: 0.4963, Precision: 0.4697, Recall: 0.4963, F1: 0.3599\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 71.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5412 | Accuracy: 0.4995, Precision: 0.3946, Recall: 0.4995, F1: 0.3571\n",
      "\n",
      "\n",
      " --- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 29.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6101 | Accuracy: 0.4956, Precision: 0.4718, Recall: 0.4956, F1: 0.3605\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 62.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5403 | Accuracy: 0.5015, Precision: 0.4093, Recall: 0.5015, F1: 0.3472\n",
      "\n",
      "\n",
      " --- Epoch 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6072 | Accuracy: 0.4957, Precision: 0.4738, Recall: 0.4957, F1: 0.3593\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 61.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5442 | Accuracy: 0.5020, Precision: 0.3644, Recall: 0.5020, F1: 0.3438\n",
      "\n",
      "\n",
      " --- Epoch 4/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6049 | Accuracy: 0.4963, Precision: 0.4757, Recall: 0.4963, F1: 0.3588\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 80.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5580 | Accuracy: 0.4992, Precision: 0.3884, Recall: 0.4992, F1: 0.3622\n",
      "\n",
      "\n",
      " --- Epoch 5/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6005 | Accuracy: 0.4980, Precision: 0.4852, Recall: 0.4980, F1: 0.3640\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 55.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5392 | Accuracy: 0.4991, Precision: 0.4314, Recall: 0.4991, F1: 0.3650\n",
      "\n",
      "\n",
      " --- Epoch 6/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6029 | Accuracy: 0.4952, Precision: 0.4827, Recall: 0.4952, F1: 0.3614\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 63.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5352 | Accuracy: 0.5008, Precision: 0.3922, Recall: 0.5008, F1: 0.3494\n",
      "\n",
      "\n",
      " --- Epoch 7/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.6000 | Accuracy: 0.4960, Precision: 0.4820, Recall: 0.4960, F1: 0.3626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 67.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5287 | Accuracy: 0.5013, Precision: 0.3674, Recall: 0.5013, F1: 0.3468\n",
      "\n",
      "\n",
      " --- Epoch 8/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 27.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5977 | Accuracy: 0.4969, Precision: 0.4869, Recall: 0.4969, F1: 0.3629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 71.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5180 | Accuracy: 0.5004, Precision: 0.4057, Recall: 0.5004, F1: 0.3548\n",
      "\n",
      "\n",
      " --- Epoch 9/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5978 | Accuracy: 0.4960, Precision: 0.4867, Recall: 0.4960, F1: 0.3626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 62.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5348 | Accuracy: 0.5018, Precision: 0.3683, Recall: 0.5018, F1: 0.3462\n",
      "\n",
      "\n",
      " --- Epoch 10/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5982 | Accuracy: 0.4967, Precision: 0.4918, Recall: 0.4967, F1: 0.3629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 61.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5330 | Accuracy: 0.5005, Precision: 0.4371, Recall: 0.5005, F1: 0.3578\n",
      "\n",
      "\n",
      " --- Epoch 11/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5988 | Accuracy: 0.4955, Precision: 0.4884, Recall: 0.4955, F1: 0.3639\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:00<00:00, 71.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5371 | Accuracy: 0.5013, Precision: 0.3645, Recall: 0.5013, F1: 0.3509\n",
      "\n",
      "\n",
      " --- Epoch 12/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 27.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5939 | Accuracy: 0.4969, Precision: 0.4856, Recall: 0.4969, F1: 0.3629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 64.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5375 | Accuracy: 0.5001, Precision: 0.4562, Recall: 0.5001, F1: 0.3647\n",
      "\n",
      "\n",
      " --- Epoch 13/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5958 | Accuracy: 0.4958, Precision: 0.4916, Recall: 0.4958, F1: 0.3617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 66.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5187 | Accuracy: 0.5012, Precision: 0.3744, Recall: 0.5012, F1: 0.3524\n",
      "\n",
      "\n",
      " --- Epoch 14/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5974 | Accuracy: 0.4954, Precision: 0.4830, Recall: 0.4954, F1: 0.3620\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 65.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5168 | Accuracy: 0.4995, Precision: 0.4023, Recall: 0.4995, F1: 0.3609\n",
      "\n",
      "\n",
      " --- Epoch 15/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 29.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5918 | Accuracy: 0.4957, Precision: 0.4926, Recall: 0.4957, F1: 0.3639\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 68.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5430 | Accuracy: 0.5010, Precision: 0.4045, Recall: 0.5010, F1: 0.3498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 39/39 [00:00<00:00, 58.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 1.5450 | Accuracy: 0.5051, Precision: 0.3998, Recall: 0.5051, F1: 0.3543\n",
      "\n",
      "Starting Fold 3/5\n",
      "\n",
      " --- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:09<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5935 | Accuracy: 0.4950, Precision: 0.4837, Recall: 0.4950, F1: 0.3597\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 56.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5234 | Accuracy: 0.5020, Precision: 0.4375, Recall: 0.5020, F1: 0.3491\n",
      "\n",
      "\n",
      " --- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5926 | Accuracy: 0.4969, Precision: 0.4840, Recall: 0.4969, F1: 0.3628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 63.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5171 | Accuracy: 0.5010, Precision: 0.4012, Recall: 0.5010, F1: 0.3554\n",
      "\n",
      "\n",
      " --- Epoch 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5902 | Accuracy: 0.4958, Precision: 0.4821, Recall: 0.4958, F1: 0.3633\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 54.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5200 | Accuracy: 0.5021, Precision: 0.4714, Recall: 0.5021, F1: 0.3564\n",
      "\n",
      "\n",
      " --- Epoch 4/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5906 | Accuracy: 0.4966, Precision: 0.4889, Recall: 0.4966, F1: 0.3642\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 60.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5131 | Accuracy: 0.5018, Precision: 0.4262, Recall: 0.5018, F1: 0.3545\n",
      "\n",
      "\n",
      " --- Epoch 5/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 27.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5860 | Accuracy: 0.4959, Precision: 0.4888, Recall: 0.4959, F1: 0.3632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 60.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5153 | Accuracy: 0.5017, Precision: 0.4027, Recall: 0.5017, F1: 0.3477\n",
      "\n",
      "\n",
      " --- Epoch 6/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5881 | Accuracy: 0.4952, Precision: 0.4906, Recall: 0.4952, F1: 0.3609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 67.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5246 | Accuracy: 0.5010, Precision: 0.4247, Recall: 0.5010, F1: 0.3597\n",
      "\n",
      "\n",
      " --- Epoch 7/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 27.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5902 | Accuracy: 0.4958, Precision: 0.4824, Recall: 0.4958, F1: 0.3609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 61.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5298 | Accuracy: 0.5013, Precision: 0.4006, Recall: 0.5013, F1: 0.3527\n",
      "\n",
      "\n",
      " --- Epoch 8/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5856 | Accuracy: 0.4947, Precision: 0.4889, Recall: 0.4947, F1: 0.3624\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 62.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5298 | Accuracy: 0.5006, Precision: 0.4182, Recall: 0.5006, F1: 0.3596\n",
      "\n",
      "\n",
      " --- Epoch 9/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5870 | Accuracy: 0.4960, Precision: 0.4901, Recall: 0.4960, F1: 0.3638\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 57.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5309 | Accuracy: 0.5013, Precision: 0.4417, Recall: 0.5013, F1: 0.3547\n",
      "\n",
      "\n",
      " --- Epoch 10/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5869 | Accuracy: 0.4958, Precision: 0.4863, Recall: 0.4958, F1: 0.3620\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 54.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5246 | Accuracy: 0.5010, Precision: 0.4478, Recall: 0.5010, F1: 0.3513\n",
      "\n",
      "\n",
      " --- Epoch 11/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5864 | Accuracy: 0.4958, Precision: 0.4965, Recall: 0.4958, F1: 0.3646\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 42.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5093 | Accuracy: 0.5013, Precision: 0.4196, Recall: 0.5013, F1: 0.3595\n",
      "\n",
      "\n",
      " --- Epoch 12/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5858 | Accuracy: 0.4952, Precision: 0.4898, Recall: 0.4952, F1: 0.3626\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 54.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5353 | Accuracy: 0.5019, Precision: 0.4260, Recall: 0.5019, F1: 0.3560\n",
      "\n",
      "\n",
      " --- Epoch 13/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5823 | Accuracy: 0.4959, Precision: 0.4895, Recall: 0.4959, F1: 0.3656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 57.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5149 | Accuracy: 0.5010, Precision: 0.4121, Recall: 0.5010, F1: 0.3525\n",
      "\n",
      "\n",
      " --- Epoch 14/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 23.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5844 | Accuracy: 0.4962, Precision: 0.4945, Recall: 0.4962, F1: 0.3654\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 54.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5187 | Accuracy: 0.5018, Precision: 0.3520, Recall: 0.5018, F1: 0.3431\n",
      "\n",
      "\n",
      " --- Epoch 15/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:11<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5801 | Accuracy: 0.4964, Precision: 0.4919, Recall: 0.4964, F1: 0.3648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 52.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5073 | Accuracy: 0.5009, Precision: 0.4364, Recall: 0.5009, F1: 0.3552\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 39/39 [00:00<00:00, 54.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 1.5108 | Accuracy: 0.5042, Precision: 0.4316, Recall: 0.5042, F1: 0.3607\n",
      "\n",
      "Starting Fold 4/5\n",
      "\n",
      " --- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5811 | Accuracy: 0.4963, Precision: 0.4876, Recall: 0.4963, F1: 0.3656\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 53.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5132 | Accuracy: 0.4979, Precision: 0.3862, Recall: 0.4979, F1: 0.3462\n",
      "\n",
      "\n",
      " --- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 276/276 [00:10<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 1.5803 | Accuracy: 0.4960, Precision: 0.4980, Recall: 0.4960, F1: 0.3634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 69/69 [00:01<00:00, 64.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 1.5284 | Accuracy: 0.4962, Precision: 0.3947, Recall: 0.4962, F1: 0.3536\n",
      "\n",
      "\n",
      " --- Epoch 3/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|██████▍   | 177/276 [00:06<00:03, 28.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 5\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m GCN(NUM_FEATURES, HIDDEN_SIZE, NUM_LABELS)\n\u001B[1;32m      2\u001B[0m TRAIN_DATASET, TEST_DATASET \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mrandom_split(AUGMENTED_DATASET, [\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m TEST_SIZE, TEST_SIZE],\n\u001B[1;32m      3\u001B[0m                                                             generator\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mGenerator()\u001B[38;5;241m.\u001B[39mmanual_seed(RANDOM_SEED))\n\u001B[0;32m----> 5\u001B[0m \u001B[43mCrossValidator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTRAIN_DATASET\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTEST_DATASET\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk_fold_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgcn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[53], line 86\u001B[0m, in \u001B[0;36mCrossValidator.k_fold_cv\u001B[0;34m(self, log_id)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_FOLDS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     85\u001B[0m \u001B[38;5;66;03m# Train and evaluate the model for the current fold\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__train_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set after each fold\u001B[39;00m\n\u001B[1;32m     89\u001B[0m test_loader \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__test_data, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[53], line 29\u001B[0m, in \u001B[0;36mCrossValidator.__train_and_evaluate\u001B[0;34m(self, train_dataloader, test_dataloader)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m --- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model and print training metrics\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m avg_train_loss, avg_train_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m TRAIN | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     32\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     34\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# Evaluate the model on the validation set and print validation metrics\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[52], line 99\u001B[0m, in \u001B[0;36mTrainer.run_epoch\u001B[0;34m(self, dataloader, train_mode)\u001B[0m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;66;03m# Iterate over the data loader\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39mphase):\n\u001B[0;32m---> 99\u001B[0m     loss, batch_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m train_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate_batch(batch)\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# Accumulate the loss and metrics\u001B[39;00m\n\u001B[1;32m    102\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "Cell \u001B[0;32mIn[52], line 68\u001B[0m, in \u001B[0;36mTrainer._train_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Compute the loss\u001B[39;00m\n\u001B[1;32m     71\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_fn(outputs, labels)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[54], line 15\u001B[0m, in \u001B[0;36mGCN.forward\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# First layer\u001B[39;00m\n\u001B[1;32m     14\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv1(x, edge_index)\n\u001B[0;32m---> 15\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbn1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     17\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mdropout(x, training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch_geometric/nn/norm/batch_norm.py:88\u001B[0m, in \u001B[0;36mBatchNorm.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mallow_single_element \u001B[38;5;129;01mand\u001B[39;00m x\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mbatch_norm(\n\u001B[1;32m     79\u001B[0m         x,\n\u001B[1;32m     80\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39mrunning_mean,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule\u001B[38;5;241m.\u001B[39meps,\n\u001B[1;32m     87\u001B[0m     )\n\u001B[0;32m---> 88\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:175\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    168\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    170\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/functional.py:2509\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2506\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m   2507\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m-> 2509\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2510\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2511\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Traditional Models",
   "id": "cde6ff44ab2a22ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:36:22.041491Z",
     "start_time": "2024-07-24T09:36:22.041431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassifiersPoolEvaluator:\n",
    "    \"\"\"\n",
    "    ClassifiersPoolEvaluator class for evaluating a pool of classifiers using graph features and stratified k-fold cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, graph_dataset):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifiersPoolEvaluator with a dictionary of classifiers.\n",
    "\n",
    "        :param graph_dataset: The graph dataset to be used for evaluation.\n",
    "        \"\"\"\n",
    "        # Define a dictionary of classifiers to evaluate\n",
    "        self.__classifiers = {\n",
    "            \"svm\": OneVsRestClassifier(SVC(kernel='linear', probability=True)),\n",
    "            \"random_forest\": OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)),\n",
    "            \"gradient_boosting\": OneVsRestClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=100, learning_rate=LR, max_depth=3)),\n",
    "            \"logistic_regression\": OneVsRestClassifier(LogisticRegression(random_state=RANDOM_SEED)),\n",
    "            \"knn\": OneVsRestClassifier(KNeighborsClassifier(n_neighbors=5)),\n",
    "            \"xgboost\": OneVsRestClassifier(\n",
    "                XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_SEED))\n",
    "        }\n",
    "\n",
    "        # Extract features and labels from the graph dataset\n",
    "        self.X, self.y = self._extract_features_and_labels(graph_dataset)\n",
    "\n",
    "    def _extract_features_and_labels(self, graph_dataset) -> (np.ndarray, np.ndarray):\n",
    "        \"\"\"\n",
    "        Extract features and labels from the graph dataset.\n",
    "\n",
    "        :param graph_dataset: The graph dataset.\n",
    "        :return: A tuple of features and labels as numpy arrays.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "\n",
    "        for data in graph_dataset:\n",
    "            features.append(data.x.numpy())  # Assuming `data.x` contains the features as a tensor\n",
    "            labels.append(data.y[0].numpy())  # Assuming `data.y` contains the labels as a tensor\n",
    "\n",
    "        # Determine the maximum shape for padding over the second dimension\n",
    "        max_length = max(f.shape[0] for f in features)\n",
    "\n",
    "        # Pad features to ensure they all have the same shape over the second dimension\n",
    "        padded_features = np.array([np.pad(f, ((0, max_length - f.shape[0]), (0, 0)), 'constant') for f in features])\n",
    "\n",
    "        # Convert labels to a numpy array and flatten if necessary\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        return padded_features, labels\n",
    "\n",
    "    def __evaluate_fold(self, classifier: OneVsRestClassifier, train_index: List[int], test_index: List[int],\n",
    "                        fold_num: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a classifier on a single fold of cross-validation.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :param train_index: Indices for the training data.\n",
    "        :param test_index: Indices for the test data.\n",
    "        :param fold_num: The fold number.\n",
    "        :return: A dictionary of computed metrics.\n",
    "        \"\"\"\n",
    "        X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "        y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "\n",
    "        # Flatten the features to be compatible with classifiers\n",
    "        X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Make predictions on the test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "\n",
    "        # Compute metrics using the provided utility function\n",
    "        metrics = compute_metrics(y_test, predictions)\n",
    "        print(f\"Results for fold {fold_num} | \"\n",
    "              f\"Precision: {metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['recall']:.4f}, \"\n",
    "              f\"F1: {metrics['f1']:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "    def __stratified_k_fold_cv(self, classifier: OneVsRestClassifier) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform stratified k-fold cross-validation on a given classifier.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :return: A DataFrame containing the results of each fold.\n",
    "        \"\"\"\n",
    "        skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "        # Evaluate the classifier on each fold and collect the results\n",
    "        results = []\n",
    "        for fold_num, (train_index, test_index) in enumerate(skf.split(self.X, self.y), 1):\n",
    "            metrics = self.__evaluate_fold(classifier, train_index, test_index, fold_num)\n",
    "            results.append(metrics)\n",
    "        # Return the results as a DataFrame\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def pool_evaluation(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the evaluation for each classifier defined in self.__classifiers.\n",
    "        \"\"\"\n",
    "        # Run the evaluation for each classifier defined in self.__classifiers\n",
    "        for classifier_name, classifier in self.__classifiers.items():\n",
    "            print(f\"\\nTesting classifier: {classifier_name}\\n\")\n",
    "            # Evaluate the classifier and get the metrics DataFrame\n",
    "            metrics_df = self.__stratified_k_fold_cv(classifier)\n",
    "            # Save the results using the provided utility function\n",
    "            save_results(metrics_df, f\"{classifier_name}.csv\")"
   ],
   "id": "7f640cbd483a02c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = ClassifiersPoolEvaluator(DATASET)\n",
    "evaluator.pool_evaluation()"
   ],
   "id": "78f1c6b6c981fd3c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
