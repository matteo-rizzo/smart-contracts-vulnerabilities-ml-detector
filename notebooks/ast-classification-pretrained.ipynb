{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:43.755665Z",
     "start_time": "2024-06-14T16:48:43.748466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, SAGEConv\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_undirected, add_self_loops\n",
    "from transformers import AutoModel, AutoConfig"
   ],
   "id": "311bb7cd7997232d",
   "outputs": [],
   "execution_count": 316
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:43.771431Z",
     "start_time": "2024-06-14T16:48:43.758544Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "def extract_features(node, depth=0):\n",
    "    # List of node types to one-hot encode\n",
    "    node_types = [\n",
    "        'FunctionDefinition', 'VariableDeclaration', 'Literal', 'ExpressionStatement',\n",
    "        'IfStatement', 'ForStatement', 'WhileStatement', 'ReturnStatement', 'Block',\n",
    "        'Assignment', 'BinaryOperation', 'UnaryOperation', 'ParameterList',\n",
    "        'FunctionCall', 'Identifier', 'IndexAccess', 'MemberAccess', 'Unknown'\n",
    "    ]\n",
    "\n",
    "    # One-hot encoding for node type\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    type_vector = [1 if node_type == t else 0 for t in node_types]\n",
    "\n",
    "    # Depth feature\n",
    "    depth_feature = [depth]\n",
    "\n",
    "    # Name feature (hashed)\n",
    "    name_feature = [hash(node.get('name', '')) % 1000] if 'name' in node else [0]\n",
    "\n",
    "    # Value feature (hashed)\n",
    "    value = node.get('value', None)\n",
    "    if isinstance(value, dict):\n",
    "        value_feature = [hash(str(value)) % 1000]\n",
    "    elif value is not None:\n",
    "        value_feature = [hash(str(value)) % 1000]\n",
    "    else:\n",
    "        value_feature = [0]\n",
    "\n",
    "    # Children count\n",
    "    children_count = len(node.get('children', []))\n",
    "    children_feature = [children_count]\n",
    "\n",
    "    # Position features (line and column, if available)\n",
    "    position = node.get('src', None)\n",
    "    if position:\n",
    "        line, column = map(int, position.split(':')[:2])\n",
    "        position_feature = [line, column]\n",
    "    else:\n",
    "        position_feature = [0, 0]\n",
    "\n",
    "    # Type descriptions (if available)\n",
    "    type_descriptions = node.get('typeDescriptions', {})\n",
    "    type_descriptions_feature = [hash(str(type_descriptions)) % 1000]\n",
    "\n",
    "    # Other custom properties\n",
    "    other_properties = ['visibility', 'stateMutability', 'constant', 'payable']\n",
    "    other_features = [hash(node.get(prop, '')) % 1000 if prop in node else 0 for prop in other_properties]\n",
    "\n",
    "    # Combine all features\n",
    "    return type_vector + depth_feature + name_feature + value_feature + children_feature + position_feature + type_descriptions_feature + other_features\n",
    "\n",
    "\n",
    "def ast_to_graph(ast_json):\n",
    "    graph = nx.DiGraph()\n",
    "    node_id = 0\n",
    "\n",
    "    def add_nodes_edges(node, parent=None, depth=0):\n",
    "        nonlocal node_id\n",
    "        current_node_id = node_id\n",
    "        graph.add_node(current_node_id, features=extract_features(node, depth))\n",
    "        if parent is not None:\n",
    "            graph.add_edge(parent, current_node_id)\n",
    "        node_id += 1\n",
    "        for key, value in node.items():\n",
    "            if isinstance(value, dict):\n",
    "                add_nodes_edges(value, current_node_id, depth + 1)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        add_nodes_edges(item, current_node_id, depth + 1)\n",
    "\n",
    "    add_nodes_edges(ast_json)\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    x = torch.stack([torch.tensor(graph.nodes[n]['features'], dtype=torch.float) for n in graph.nodes])\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return add_masks_to_data(data)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_features(node):\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    return [hash(node_type) % 1000]  # Simple feature vector from node type\n",
    "\n",
    "\n",
    "def ast_to_pyg_data(ast_json):\n",
    "    nodes = []\n",
    "    edges = []\n",
    "    node_id = 0\n",
    "    node_map = {}\n",
    "\n",
    "    def add_nodes_edges(node, parent=None):\n",
    "        nonlocal node_id\n",
    "        current_node_id = node_id\n",
    "        node_map[id(node)] = current_node_id\n",
    "        nodes.append(extract_features(node))\n",
    "        if parent is not None:\n",
    "            edges.append((node_map[id(parent)], current_node_id))\n",
    "        node_id += 1\n",
    "        for key, value in node.items():\n",
    "            if isinstance(value, dict):\n",
    "                add_nodes_edges(value, node)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        add_nodes_edges(item, node)\n",
    "\n",
    "    add_nodes_edges(ast_json)\n",
    "    x = torch.tensor(nodes, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_index, _ = add_self_loops(edge_index)\n",
    "    edge_index = to_undirected(edge_index)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "def add_masks_to_data(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    num_nodes = data.x.size(0)\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    train_size = int(num_nodes * train_ratio)\n",
    "    val_size = int(num_nodes * val_ratio)\n",
    "\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_label_map(ast_directory):\n",
    "    label_map = {}\n",
    "    label_index = 0\n",
    "    for category in os.listdir(ast_directory):\n",
    "        category_path = os.path.join(ast_directory, category)\n",
    "        if os.path.isdir(category_path):  # Ensure it's a directory\n",
    "            label_map[category] = label_index\n",
    "            label_index += 1\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def load_data(ast_directory, label_map):\n",
    "    dataset, labels = [], []\n",
    "    for root, dirs, files in os.walk(ast_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    ast = json.load(f)\n",
    "                data = ast_to_pyg_data(ast)\n",
    "                label_folder = os.path.basename(root)\n",
    "                label = label_map[label_folder]\n",
    "                data.y = torch.tensor([label] * data.x.size(0), dtype=torch.long)  # Assign label to all nodes\n",
    "                dataset.append(data)\n",
    "                labels.append(label)\n",
    "    return dataset, labels\n"
   ],
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:45.204381Z",
     "start_time": "2024-06-14T16:48:43.772394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ast_directory = '../dataset/aisc/ast'\n",
    "label_map = generate_label_map(ast_directory)\n",
    "\n",
    "# Load all data\n",
    "dataset, labels = load_data(ast_directory, label_map)"
   ],
   "id": "da71c9fcaaa90f83",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:45.207705Z",
     "start_time": "2024-06-14T16:48:45.205628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stratified_split(dataset, labels):\n",
    "    # Split data into training + validation and test data\n",
    "    train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "        dataset, labels, test_size=0.1, stratify=labels, random_state=42)\n",
    "\n",
    "    # Split training + validation into actual training and validation data\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "        train_val_data, train_val_labels, test_size=0.11, stratify=train_val_labels,\n",
    "        random_state=42)  # 0.11 * 0.9 â‰ˆ 0.1\n",
    "\n",
    "    return train_data, val_data, test_data"
   ],
   "id": "558b0c2713f839ac",
   "outputs": [],
   "execution_count": 319
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:45.216805Z",
     "start_time": "2024-06-14T16:48:45.208452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, num_classes, num_heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_node_features, hidden_channels, heads=num_heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, num_classes, heads=1, concat=False, dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, hidden_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(hidden_feats, out_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        h = self.conv1(g, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = out[data.train_mask].argmax(dim=1)\n",
    "        y_true.extend(data.y[data.train_mask].cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return total_loss / len(loader), accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, mask_type):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            mask = getattr(data, mask_type)\n",
    "            pred = out[mask].argmax(dim=1)\n",
    "            y_true.extend(data.y[mask].cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc\n"
   ],
   "id": "10e7fbd4113fb273",
   "outputs": [],
   "execution_count": 320
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:45.221224Z",
     "start_time": "2024-06-14T16:48:45.217535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_graphormer_inputs(data, num_heads):\n",
    "    # Add batch dimension and ensure types\n",
    "    input_nodes = data.x.unsqueeze(0).long()  # shape: [1, num_nodes, feature_size]\n",
    "    input_edges = data.edge_index.unsqueeze(0).long()  # shape: [1, 2, num_edges]\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # Initialize attention bias for Graphormer\n",
    "    attn_bias = torch.zeros((1, num_heads, num_nodes + 1, num_nodes + 1),\n",
    "                            dtype=torch.float)  # shape: [1, num_heads, num_nodes+1, num_nodes+1]\n",
    "\n",
    "    # Degree tensors with batch dimension\n",
    "    in_degree = torch.zeros((1, num_nodes + 1), dtype=torch.long)\n",
    "    out_degree = torch.zeros((1, num_nodes + 1), dtype=torch.long)\n",
    "    for node in data.edge_index[1]:\n",
    "        in_degree[0][node + 1] += 1  # +1 to accommodate the CLS token\n",
    "    for node in data.edge_index[0]:\n",
    "        out_degree[0][node + 1] += 1  # +1 to accommodate the CLS token\n",
    "\n",
    "    # Spatial position tensor, initialized to zero\n",
    "    spatial_pos = torch.zeros((1, num_nodes + 1, num_nodes + 1), dtype=torch.long)\n",
    "\n",
    "    # Attention edge type tensor\n",
    "    attn_edge_type = torch.zeros((1, input_edges.size(2)), dtype=torch.long)\n",
    "\n",
    "    # Debug prints\n",
    "    print(f\"input_nodes shape: {input_nodes.shape}, dtype: {input_nodes.dtype}\")\n",
    "    print(f\"input_edges shape: {input_edges.shape}, dtype: {input_edges.dtype}\")\n",
    "    print(f\"attn_bias shape: {attn_bias.shape}, dtype: {attn_bias.dtype}\")\n",
    "    print(f\"in_degree shape: {in_degree.shape}, dtype: {in_degree.dtype}\")\n",
    "    print(f\"out_degree shape: {out_degree.shape}, dtype: {out_degree.dtype}\")\n",
    "    print(f\"spatial_pos shape: {spatial_pos.shape}, dtype: {spatial_pos.dtype}\")\n",
    "    print(f\"attn_edge_type shape: {attn_edge_type.shape}, dtype: {attn_edge_type.dtype}\")\n",
    "\n",
    "    return {\n",
    "        'input_nodes': input_nodes,\n",
    "        'input_edges': input_edges,\n",
    "        'attn_bias': attn_bias,\n",
    "        'in_degree': in_degree,\n",
    "        'out_degree': out_degree,\n",
    "        'spatial_pos': spatial_pos,\n",
    "        'attn_edge_type': attn_edge_type\n",
    "    }\n"
   ],
   "id": "4207c1cfd71670d2",
   "outputs": [],
   "execution_count": 321
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:45.227397Z",
     "start_time": "2024-06-14T16:48:45.222235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(model, loader, optimizer, criterion, num_heads):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        inputs = prepare_graphormer_inputs(data, num_heads)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**inputs)\n",
    "        print(f\"outputs logits shape: {outputs.logits.shape}\")\n",
    "        print(f\"data.y shape: {data.y.shape}, dtype: {data.y.dtype}\")\n",
    "        loss = criterion(outputs.logits, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = outputs.logits.argmax(dim=1)\n",
    "        y_true.extend(data.y.cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return total_loss / len(loader), accuracy, precision, recall, f1, roc_auc\n",
    "\n",
    "\n",
    "def evaluate(model, loader, num_heads):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        inputs = prepare_graphormer_inputs(data, num_heads)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        pred = outputs.logits.argmax(dim=1)\n",
    "        y_true.extend(data.y.cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='weighted', multi_class='ovo')\n",
    "\n",
    "    return accuracy, precision, recall, f1, roc_auc\n"
   ],
   "id": "aecd46b68707f8bd",
   "outputs": [],
   "execution_count": 322
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:47.441355Z",
     "start_time": "2024-06-14T16:48:45.228244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Split data\n",
    "train_data, val_data, test_data = stratified_split(dataset, labels)\n",
    "\n",
    "# Convert to DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=True)\n",
    "\n",
    "num_features = len(extract_features({}))  # Determine number of features dynamically\n",
    "num_features = 1\n",
    "num_classes = len(label_map)  # Number of classes from the label map\n",
    "\n",
    "# model = GCN(num_features=num_features, hidden_channels=16, num_classes=num_classes).to(device)\n",
    "# model = GAT(num_node_features=num_features, hidden_channels=16, num_classes=num_classes, num_heads=4).to(device)\n",
    "model_name = \"clefourrier/graphormer-base-pcqm4mv2\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, config=config).to(device)\n",
    "\n",
    "# Extract the number of attention heads from the configuration\n",
    "num_heads = config.num_attention_heads\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc = train(model, train_loader, optimizer,\n",
    "                                                                                  criterion, num_heads)\n",
    "    val_acc, val_prec, val_rec, val_f1, val_roc_auc = evaluate(model, val_loader, num_heads)\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Train Precision: {train_prec:.4f}, Train Recall: {train_rec:.4f}, Train F1 Score: {train_f1:.4f}, Train ROC-AUC: {train_roc_auc:.4f}')\n",
    "    print(\n",
    "        f'Val Accuracy: {val_acc:.4f}, Val Precision: {val_prec:.4f}, Val Recall: {val_rec:.4f}, Val F1 Score: {val_f1:.4f}, Val ROC-AUC: {val_roc_auc:.4f}')\n",
    "\n",
    "test_acc, test_prec, test_rec, test_f1, test_roc_auc = evaluate(model, test_loader, num_heads)\n",
    "print(\n",
    "    f'Test Accuracy: {test_acc:.4f}, Test Precision: {test_prec:.4f}, Test Recall: {test_rec:.4f}, Test F1 Score: {test_f1:.4f}, Test ROC-AUC: {test_roc_auc:.4f}')\n",
    "\n",
    "\"\"\"\n",
    "for epoch in range(200):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    accuracy, precision, recall, f1, roc_auc = evaluate(model, val_loader, 'val_mask')\n",
    "    print(\n",
    "        f'Epoch: {epoch + 1}, Train Loss: {train_loss:.4f}, Val Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}')\n",
    "\n",
    "accuracy, precision, recall, f1, roc_auc = evaluate(model, test_loader, 'test_mask')\n",
    "print(\n",
    "    f'Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}')\n",
    "\n",
    "\"\"\""
   ],
   "id": "faa02a2008d19f3f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GraphormerModel were not initialized from the model checkpoint at clefourrier/graphormer-base-pcqm4mv2 and are newly initialized: ['graph_encoder.emb_layer_norm.bias', 'graph_encoder.emb_layer_norm.weight', 'graph_encoder.graph_attn_bias.edge_dis_encoder.weight', 'graph_encoder.graph_attn_bias.edge_encoder.weight', 'graph_encoder.graph_attn_bias.graph_token_virtual_distance.weight', 'graph_encoder.graph_attn_bias.spatial_pos_encoder.weight', 'graph_encoder.graph_node_feature.atom_encoder.weight', 'graph_encoder.graph_node_feature.graph_token.weight', 'graph_encoder.graph_node_feature.in_degree_encoder.weight', 'graph_encoder.graph_node_feature.out_degree_encoder.weight', 'graph_encoder.layers.0.fc1.bias', 'graph_encoder.layers.0.fc1.weight', 'graph_encoder.layers.0.fc2.bias', 'graph_encoder.layers.0.fc2.weight', 'graph_encoder.layers.0.final_layer_norm.bias', 'graph_encoder.layers.0.final_layer_norm.weight', 'graph_encoder.layers.0.self_attn.k_proj.bias', 'graph_encoder.layers.0.self_attn.k_proj.weight', 'graph_encoder.layers.0.self_attn.out_proj.bias', 'graph_encoder.layers.0.self_attn.out_proj.weight', 'graph_encoder.layers.0.self_attn.q_proj.bias', 'graph_encoder.layers.0.self_attn.q_proj.weight', 'graph_encoder.layers.0.self_attn.v_proj.bias', 'graph_encoder.layers.0.self_attn.v_proj.weight', 'graph_encoder.layers.0.self_attn_layer_norm.bias', 'graph_encoder.layers.0.self_attn_layer_norm.weight', 'graph_encoder.layers.1.fc1.bias', 'graph_encoder.layers.1.fc1.weight', 'graph_encoder.layers.1.fc2.bias', 'graph_encoder.layers.1.fc2.weight', 'graph_encoder.layers.1.final_layer_norm.bias', 'graph_encoder.layers.1.final_layer_norm.weight', 'graph_encoder.layers.1.self_attn.k_proj.bias', 'graph_encoder.layers.1.self_attn.k_proj.weight', 'graph_encoder.layers.1.self_attn.out_proj.bias', 'graph_encoder.layers.1.self_attn.out_proj.weight', 'graph_encoder.layers.1.self_attn.q_proj.bias', 'graph_encoder.layers.1.self_attn.q_proj.weight', 'graph_encoder.layers.1.self_attn.v_proj.bias', 'graph_encoder.layers.1.self_attn.v_proj.weight', 'graph_encoder.layers.1.self_attn_layer_norm.bias', 'graph_encoder.layers.1.self_attn_layer_norm.weight', 'graph_encoder.layers.10.fc1.bias', 'graph_encoder.layers.10.fc1.weight', 'graph_encoder.layers.10.fc2.bias', 'graph_encoder.layers.10.fc2.weight', 'graph_encoder.layers.10.final_layer_norm.bias', 'graph_encoder.layers.10.final_layer_norm.weight', 'graph_encoder.layers.10.self_attn.k_proj.bias', 'graph_encoder.layers.10.self_attn.k_proj.weight', 'graph_encoder.layers.10.self_attn.out_proj.bias', 'graph_encoder.layers.10.self_attn.out_proj.weight', 'graph_encoder.layers.10.self_attn.q_proj.bias', 'graph_encoder.layers.10.self_attn.q_proj.weight', 'graph_encoder.layers.10.self_attn.v_proj.bias', 'graph_encoder.layers.10.self_attn.v_proj.weight', 'graph_encoder.layers.10.self_attn_layer_norm.bias', 'graph_encoder.layers.10.self_attn_layer_norm.weight', 'graph_encoder.layers.11.fc1.bias', 'graph_encoder.layers.11.fc1.weight', 'graph_encoder.layers.11.fc2.bias', 'graph_encoder.layers.11.fc2.weight', 'graph_encoder.layers.11.final_layer_norm.bias', 'graph_encoder.layers.11.final_layer_norm.weight', 'graph_encoder.layers.11.self_attn.k_proj.bias', 'graph_encoder.layers.11.self_attn.k_proj.weight', 'graph_encoder.layers.11.self_attn.out_proj.bias', 'graph_encoder.layers.11.self_attn.out_proj.weight', 'graph_encoder.layers.11.self_attn.q_proj.bias', 'graph_encoder.layers.11.self_attn.q_proj.weight', 'graph_encoder.layers.11.self_attn.v_proj.bias', 'graph_encoder.layers.11.self_attn.v_proj.weight', 'graph_encoder.layers.11.self_attn_layer_norm.bias', 'graph_encoder.layers.11.self_attn_layer_norm.weight', 'graph_encoder.layers.2.fc1.bias', 'graph_encoder.layers.2.fc1.weight', 'graph_encoder.layers.2.fc2.bias', 'graph_encoder.layers.2.fc2.weight', 'graph_encoder.layers.2.final_layer_norm.bias', 'graph_encoder.layers.2.final_layer_norm.weight', 'graph_encoder.layers.2.self_attn.k_proj.bias', 'graph_encoder.layers.2.self_attn.k_proj.weight', 'graph_encoder.layers.2.self_attn.out_proj.bias', 'graph_encoder.layers.2.self_attn.out_proj.weight', 'graph_encoder.layers.2.self_attn.q_proj.bias', 'graph_encoder.layers.2.self_attn.q_proj.weight', 'graph_encoder.layers.2.self_attn.v_proj.bias', 'graph_encoder.layers.2.self_attn.v_proj.weight', 'graph_encoder.layers.2.self_attn_layer_norm.bias', 'graph_encoder.layers.2.self_attn_layer_norm.weight', 'graph_encoder.layers.3.fc1.bias', 'graph_encoder.layers.3.fc1.weight', 'graph_encoder.layers.3.fc2.bias', 'graph_encoder.layers.3.fc2.weight', 'graph_encoder.layers.3.final_layer_norm.bias', 'graph_encoder.layers.3.final_layer_norm.weight', 'graph_encoder.layers.3.self_attn.k_proj.bias', 'graph_encoder.layers.3.self_attn.k_proj.weight', 'graph_encoder.layers.3.self_attn.out_proj.bias', 'graph_encoder.layers.3.self_attn.out_proj.weight', 'graph_encoder.layers.3.self_attn.q_proj.bias', 'graph_encoder.layers.3.self_attn.q_proj.weight', 'graph_encoder.layers.3.self_attn.v_proj.bias', 'graph_encoder.layers.3.self_attn.v_proj.weight', 'graph_encoder.layers.3.self_attn_layer_norm.bias', 'graph_encoder.layers.3.self_attn_layer_norm.weight', 'graph_encoder.layers.4.fc1.bias', 'graph_encoder.layers.4.fc1.weight', 'graph_encoder.layers.4.fc2.bias', 'graph_encoder.layers.4.fc2.weight', 'graph_encoder.layers.4.final_layer_norm.bias', 'graph_encoder.layers.4.final_layer_norm.weight', 'graph_encoder.layers.4.self_attn.k_proj.bias', 'graph_encoder.layers.4.self_attn.k_proj.weight', 'graph_encoder.layers.4.self_attn.out_proj.bias', 'graph_encoder.layers.4.self_attn.out_proj.weight', 'graph_encoder.layers.4.self_attn.q_proj.bias', 'graph_encoder.layers.4.self_attn.q_proj.weight', 'graph_encoder.layers.4.self_attn.v_proj.bias', 'graph_encoder.layers.4.self_attn.v_proj.weight', 'graph_encoder.layers.4.self_attn_layer_norm.bias', 'graph_encoder.layers.4.self_attn_layer_norm.weight', 'graph_encoder.layers.5.fc1.bias', 'graph_encoder.layers.5.fc1.weight', 'graph_encoder.layers.5.fc2.bias', 'graph_encoder.layers.5.fc2.weight', 'graph_encoder.layers.5.final_layer_norm.bias', 'graph_encoder.layers.5.final_layer_norm.weight', 'graph_encoder.layers.5.self_attn.k_proj.bias', 'graph_encoder.layers.5.self_attn.k_proj.weight', 'graph_encoder.layers.5.self_attn.out_proj.bias', 'graph_encoder.layers.5.self_attn.out_proj.weight', 'graph_encoder.layers.5.self_attn.q_proj.bias', 'graph_encoder.layers.5.self_attn.q_proj.weight', 'graph_encoder.layers.5.self_attn.v_proj.bias', 'graph_encoder.layers.5.self_attn.v_proj.weight', 'graph_encoder.layers.5.self_attn_layer_norm.bias', 'graph_encoder.layers.5.self_attn_layer_norm.weight', 'graph_encoder.layers.6.fc1.bias', 'graph_encoder.layers.6.fc1.weight', 'graph_encoder.layers.6.fc2.bias', 'graph_encoder.layers.6.fc2.weight', 'graph_encoder.layers.6.final_layer_norm.bias', 'graph_encoder.layers.6.final_layer_norm.weight', 'graph_encoder.layers.6.self_attn.k_proj.bias', 'graph_encoder.layers.6.self_attn.k_proj.weight', 'graph_encoder.layers.6.self_attn.out_proj.bias', 'graph_encoder.layers.6.self_attn.out_proj.weight', 'graph_encoder.layers.6.self_attn.q_proj.bias', 'graph_encoder.layers.6.self_attn.q_proj.weight', 'graph_encoder.layers.6.self_attn.v_proj.bias', 'graph_encoder.layers.6.self_attn.v_proj.weight', 'graph_encoder.layers.6.self_attn_layer_norm.bias', 'graph_encoder.layers.6.self_attn_layer_norm.weight', 'graph_encoder.layers.7.fc1.bias', 'graph_encoder.layers.7.fc1.weight', 'graph_encoder.layers.7.fc2.bias', 'graph_encoder.layers.7.fc2.weight', 'graph_encoder.layers.7.final_layer_norm.bias', 'graph_encoder.layers.7.final_layer_norm.weight', 'graph_encoder.layers.7.self_attn.k_proj.bias', 'graph_encoder.layers.7.self_attn.k_proj.weight', 'graph_encoder.layers.7.self_attn.out_proj.bias', 'graph_encoder.layers.7.self_attn.out_proj.weight', 'graph_encoder.layers.7.self_attn.q_proj.bias', 'graph_encoder.layers.7.self_attn.q_proj.weight', 'graph_encoder.layers.7.self_attn.v_proj.bias', 'graph_encoder.layers.7.self_attn.v_proj.weight', 'graph_encoder.layers.7.self_attn_layer_norm.bias', 'graph_encoder.layers.7.self_attn_layer_norm.weight', 'graph_encoder.layers.8.fc1.bias', 'graph_encoder.layers.8.fc1.weight', 'graph_encoder.layers.8.fc2.bias', 'graph_encoder.layers.8.fc2.weight', 'graph_encoder.layers.8.final_layer_norm.bias', 'graph_encoder.layers.8.final_layer_norm.weight', 'graph_encoder.layers.8.self_attn.k_proj.bias', 'graph_encoder.layers.8.self_attn.k_proj.weight', 'graph_encoder.layers.8.self_attn.out_proj.bias', 'graph_encoder.layers.8.self_attn.out_proj.weight', 'graph_encoder.layers.8.self_attn.q_proj.bias', 'graph_encoder.layers.8.self_attn.q_proj.weight', 'graph_encoder.layers.8.self_attn.v_proj.bias', 'graph_encoder.layers.8.self_attn.v_proj.weight', 'graph_encoder.layers.8.self_attn_layer_norm.bias', 'graph_encoder.layers.8.self_attn_layer_norm.weight', 'graph_encoder.layers.9.fc1.bias', 'graph_encoder.layers.9.fc1.weight', 'graph_encoder.layers.9.fc2.bias', 'graph_encoder.layers.9.fc2.weight', 'graph_encoder.layers.9.final_layer_norm.bias', 'graph_encoder.layers.9.final_layer_norm.weight', 'graph_encoder.layers.9.self_attn.k_proj.bias', 'graph_encoder.layers.9.self_attn.k_proj.weight', 'graph_encoder.layers.9.self_attn.out_proj.bias', 'graph_encoder.layers.9.self_attn.out_proj.weight', 'graph_encoder.layers.9.self_attn.q_proj.bias', 'graph_encoder.layers.9.self_attn.q_proj.weight', 'graph_encoder.layers.9.self_attn.v_proj.bias', 'graph_encoder.layers.9.self_attn.v_proj.weight', 'graph_encoder.layers.9.self_attn_layer_norm.bias', 'graph_encoder.layers.9.self_attn_layer_norm.weight', 'layer_norm.bias', 'layer_norm.weight', 'lm_head_transform_weight.bias', 'lm_head_transform_weight.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_nodes shape: torch.Size([1, 2192, 1]), dtype: torch.int64\n",
      "input_edges shape: torch.Size([1, 2, 6544]), dtype: torch.int64\n",
      "attn_bias shape: torch.Size([1, 32, 2193, 2193]), dtype: torch.float32\n",
      "in_degree shape: torch.Size([1, 2193]), dtype: torch.int64\n",
      "out_degree shape: torch.Size([1, 2193]), dtype: torch.int64\n",
      "spatial_pos shape: torch.Size([1, 2193, 2193]), dtype: torch.int64\n",
      "attn_edge_type shape: torch.Size([1, 6544]), dtype: torch.int64\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[323], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m200\u001B[39m):\n\u001B[0;32m---> 28\u001B[0m     train_loss, train_acc, train_prec, train_rec, train_f1, train_roc_auc \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m                                                                                  \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m     val_acc, val_prec, val_rec, val_f1, val_roc_auc \u001B[38;5;241m=\u001B[39m evaluate(model, val_loader, num_heads)\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     33\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_acc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_prec\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_rec\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train F1 Score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_f1\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Train ROC-AUC: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_roc_auc\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[322], line 11\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, criterion, num_heads)\u001B[0m\n\u001B[1;32m      9\u001B[0m inputs \u001B[38;5;241m=\u001B[39m prepare_graphormer_inputs(data, num_heads)\n\u001B[1;32m     10\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 11\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutputs logits shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00moutputs\u001B[38;5;241m.\u001B[39mlogits\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata.y shape: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, dtype: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39my\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/graphormer/modeling_graphormer.py:820\u001B[0m, in \u001B[0;36mGraphormerModel.forward\u001B[0;34m(self, input_nodes, input_edges, attn_bias, in_degree, out_degree, spatial_pos, attn_edge_type, perturb, masked_tokens, return_dict, **unused)\u001B[0m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    805\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    806\u001B[0m     input_nodes: torch\u001B[38;5;241m.\u001B[39mLongTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    816\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused,\n\u001B[1;32m    817\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Union[Tuple[torch\u001B[38;5;241m.\u001B[39mLongTensor], BaseModelOutputWithNoAttention]:\n\u001B[1;32m    818\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 820\u001B[0m     inner_states, graph_rep \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    821\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_nodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_edges\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_degree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_degree\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_edge_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mperturb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mperturb\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    824\u001B[0m     \u001B[38;5;66;03m# last inner state, then revert Batch and Graph len\u001B[39;00m\n\u001B[1;32m    825\u001B[0m     input_nodes \u001B[38;5;241m=\u001B[39m inner_states[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/graphormer/modeling_graphormer.py:645\u001B[0m, in \u001B[0;36mGraphormerGraphEncoder.forward\u001B[0;34m(self, input_nodes, input_edges, attn_bias, in_degree, out_degree, spatial_pos, attn_edge_type, perturb, last_state_only, token_embeddings, attn_mask)\u001B[0m\n\u001B[1;32m    642\u001B[0m padding_mask_cls \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(n_graph, \u001B[38;5;241m1\u001B[39m, device\u001B[38;5;241m=\u001B[39mpadding_mask\u001B[38;5;241m.\u001B[39mdevice, dtype\u001B[38;5;241m=\u001B[39mpadding_mask\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[1;32m    643\u001B[0m padding_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat((padding_mask_cls, padding_mask), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 645\u001B[0m attn_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_attn_bias\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_nodes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_bias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspatial_pos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_edges\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_edge_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    647\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m token_embeddings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    648\u001B[0m     input_nodes \u001B[38;5;241m=\u001B[39m token_embeddings\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/graphormer/modeling_graphormer.py:253\u001B[0m, in \u001B[0;36mGraphormerGraphAttnBias.forward\u001B[0;34m(self, input_nodes, attn_bias, spatial_pos, input_edges, attn_edge_type)\u001B[0m\n\u001B[1;32m    251\u001B[0m n_graph, n_node \u001B[38;5;241m=\u001B[39m input_nodes\u001B[38;5;241m.\u001B[39msize()[:\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    252\u001B[0m graph_attn_bias \u001B[38;5;241m=\u001B[39m attn_bias\u001B[38;5;241m.\u001B[39mclone()\n\u001B[0;32m--> 253\u001B[0m graph_attn_bias \u001B[38;5;241m=\u001B[39m \u001B[43mgraph_attn_bias\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrepeat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[1;32m    255\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# [n_graph, n_head, n_node+1, n_node+1]\u001B[39;00m\n\u001B[1;32m    257\u001B[0m \u001B[38;5;66;03m# spatial pos\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\u001B[39;00m\n\u001B[1;32m    259\u001B[0m spatial_pos_bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspatial_pos_encoder(spatial_pos)\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Number of dimensions of repeat dims can not be smaller than number of dimensions of tensor"
     ]
    }
   ],
   "execution_count": 323
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T16:48:47.442763Z",
     "start_time": "2024-06-14T16:48:47.442706Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a47b081396d59fbe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
