{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "\n",
    "neo4j_log = logging.getLogger(\"neo4j\")\n",
    "neo4j_log.setLevel(logging.CRITICAL)"
   ],
   "id": "cb3417e6bd4f8af0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.classes.utils.DebugLogger import DebugLogger\n",
    "from src.classes.utils.EnvLoader import EnvLoader\n",
    "import os\n",
    "import nest_asyncio\n",
    "import nltk\n",
    "import json\n",
    "from llama_index.core import PropertyGraphIndex, Settings\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "import time\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.core.vector_stores.types import VectorStore\n",
    "from llama_index.core.base.response.schema import Response\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SimpleFileNodeParser\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.schema import TransformComponent, NodeWithScore, Document\n",
    "from llama_index.graph_stores.neo4j import Neo4jPropertyGraphStore\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.vector_stores.neo4jvector import Neo4jVectorStore\n",
    "from llama_index.core.graph_stores import PropertyGraphStore\n",
    "from neo4j import GraphDatabase\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional\n",
    "from typing import List, Tuple, Type, Literal\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "from llama_index.core.chat_engine.types import BaseChatEngine, AgentChatResponse\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "nest_asyncio.apply()\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "EnvLoader(env_dir=\"../src/config\").load_env_files()\n",
    "\n",
    "logger = DebugLogger(use_panel_for_errors=True)\n",
    "\n",
    "LLM_MODE = \"openai\"\n",
    "EMBEDDING_MODE = \"local\""
   ],
   "id": "8b08278fb3f46541",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ModelManager:\n",
    "    \"\"\"\n",
    "    Manages the configuration and lazy initialization of LLMs and embedding models\n",
    "    for OpenAI, Azure OpenAI, and Hugging Face. Reads all configuration parameters\n",
    "    from environment variables with sensible defaults.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # Load configuration from environment variables\n",
    "        self.openai_model = os.getenv(\"OPENAI_MODEL_NAME_CHAT\")\n",
    "\n",
    "        self.azure_model = os.getenv(\"OPENAI_MODEL_NAME_CHAT\")\n",
    "        self.azure_deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "        self.azure_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "        self.azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        self.azure_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "        self.huggingface_model_name = os.getenv(\"HUGGINGFACE_LLM_MODEL\")\n",
    "        self.huggingface_embed_model_name = os.getenv(\"HUGGINGFACE_EMBED_MODEL\")\n",
    "\n",
    "        # Lazily initialized models\n",
    "        self._openai_llm = None\n",
    "        self._azure_llm = None\n",
    "        self._local_llm = None\n",
    "        self._openai_embed_model = None\n",
    "        self._local_embed_model = None\n",
    "\n",
    "    @property\n",
    "    def openai_llm(self) -> OpenAI:\n",
    "        \"\"\"Lazy initialization of the OpenAI LLM.\"\"\"\n",
    "        if self._openai_llm is None:\n",
    "            self._openai_llm = OpenAI(model=self.openai_model)\n",
    "        return self._openai_llm\n",
    "\n",
    "    @property\n",
    "    def azure_llm(self) -> AzureOpenAI:\n",
    "        \"\"\"Lazy initialization of the Azure OpenAI LLM.\"\"\"\n",
    "        if self._azure_llm is None:\n",
    "            self._azure_llm = AzureOpenAI(\n",
    "                model=self.azure_model,\n",
    "                deployment_name=self.azure_deployment_name,\n",
    "                api_key=self.azure_api_key,\n",
    "                azure_endpoint=self.azure_endpoint,\n",
    "                api_version=self.azure_api_version,\n",
    "            )\n",
    "        return self._azure_llm\n",
    "\n",
    "    @property\n",
    "    def local_llm(self) -> HuggingFaceLLM:\n",
    "        \"\"\"Lazy initialization of the Hugging Face LLM.\"\"\"\n",
    "        if self._local_llm is None:\n",
    "            self._local_llm = HuggingFaceLLM(model_name=self.huggingface_model_name, device_map=\"auto\")\n",
    "        return self._local_llm\n",
    "\n",
    "    @property\n",
    "    def openai_embed_model(self) -> OpenAIEmbedding:\n",
    "        \"\"\"Lazy initialization of the OpenAI embedding model.\"\"\"\n",
    "        if self._openai_embed_model is None:\n",
    "            self._openai_embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
    "        return self._openai_embed_model\n",
    "\n",
    "    @property\n",
    "    def local_embed_model(self) -> HuggingFaceEmbedding:\n",
    "        \"\"\"Lazy initialization of the Hugging Face embedding model.\"\"\"\n",
    "        if self._local_embed_model is None:\n",
    "            self._local_embed_model = HuggingFaceEmbedding(model_name=self.huggingface_embed_model_name)\n",
    "        return self._local_embed_model\n",
    "\n",
    "    def get_llm(self, llm_type: str) -> Union[OpenAI, AzureOpenAI, HuggingFaceLLM]:\n",
    "        \"\"\"\n",
    "        Retrieve the desired LLM based on the specified type.\n",
    "\n",
    "        :param llm_type: The type of LLM to retrieve (\"openai\", \"azure\", \"local\").\n",
    "        :return: The requested LLM instance.\n",
    "        :raises ValueError: If an invalid llm_type is provided.\n",
    "        \"\"\"\n",
    "        if llm_type == \"openai\":\n",
    "            return self.openai_llm\n",
    "        elif llm_type == \"azure\":\n",
    "            return self.azure_llm\n",
    "        elif llm_type == \"local\":\n",
    "            return self.local_llm\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid llm_type '{llm_type}'. Expected 'openai', 'azure', or 'local'.\")\n",
    "\n",
    "    def get_embedding_model(self, embed_type: str) -> Union[OpenAIEmbedding, HuggingFaceEmbedding]:\n",
    "        \"\"\"\n",
    "        Retrieve the desired embedding model based on the specified type.\n",
    "\n",
    "        :param embed_type: The type of embedding model to retrieve (\"openai\", \"local\").\n",
    "        :return: The requested embedding model instance.\n",
    "        :raises ValueError: If an invalid embed_type is provided.\n",
    "        \"\"\"\n",
    "        if embed_type == \"openai\":\n",
    "            return self.openai_embed_model\n",
    "        elif embed_type == \"local\":\n",
    "            return self.local_embed_model\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid embed_type '{embed_type}'. Expected 'openai' or 'local'.\")"
   ],
   "id": "1a6974aae3a9adc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Settings.llm = ModelManager().get_llm(LLM_MODE)\n",
    "Settings.embed_model = ModelManager().get_embedding_model(EMBEDDING_MODE)"
   ],
   "id": "89a825dc7af14a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Neo4jDBManager:\n",
    "    \"\"\"\n",
    "    Manages the configuration and creation of Neo4j graph and vector stores.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url: str = None, username: str = None, password: str = None, database: str = None):\n",
    "        \"\"\"\n",
    "        Initialize Neo4j connection parameters.\n",
    "\n",
    "        :param url: The URL for the Neo4j instance, defaults to \"bolt://localhost:7687\".\n",
    "        :param username: Username for Neo4j authentication, defaults to \"neo4j\".\n",
    "        :param password: Password for Neo4j authentication, retrieved from environment if not provided.\n",
    "        :param database: Name of the Neo4j database, defaults to \"neo4j\".\n",
    "        \"\"\"\n",
    "        self.logger = DebugLogger(use_panel_for_errors=True)\n",
    "        self.url = url or os.getenv(\"NEO4J_URL\", \"bolt://localhost:7687\")\n",
    "        self.username = username or os.getenv(\"NEO4J_USERNAME\", \"neo4j\")\n",
    "        self.password = password or os.getenv(\"NEO4J_PASSWORD\")\n",
    "        self.database = database or os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "        self._validate_password()\n",
    "        self.logger.success(f\"Neo4jDBManager initialized with URL: '{self.url}', Database: '{self.database}'.\")\n",
    "\n",
    "    def _validate_password(self):\n",
    "        \"\"\"\n",
    "        Validate that a Neo4j password is set.\n",
    "\n",
    "        :raises ValueError: If the password is not provided.\n",
    "        \"\"\"\n",
    "        if not self.password:\n",
    "            error_message = (\n",
    "                \"Neo4j password is required. Set it in the environment or pass it directly.\"\n",
    "            )\n",
    "            self.logger.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "    def create_graph_store(self) -> Neo4jPropertyGraphStore:\n",
    "        \"\"\"\n",
    "        Create and return a Neo4jPropertyGraphStore instance.\n",
    "\n",
    "        :return: Configured Neo4jPropertyGraphStore instance.\n",
    "        \"\"\"\n",
    "        return self._create_store(Neo4jPropertyGraphStore, \"Neo4jPropertyGraphStore\")\n",
    "\n",
    "    def create_vector_store(self, embedding_dimension: int = 384, hybrid_search: bool = True) -> Neo4jVectorStore:\n",
    "        \"\"\"\n",
    "        Create and return a Neo4jVectorStore instance.\n",
    "\n",
    "        :param embedding_dimension: Dimension of embeddings, defaults to 1536.\n",
    "        :param hybrid_search: Enables hybrid search, defaults to True.\n",
    "        :return: Configured Neo4jVectorStore instance.\n",
    "        \"\"\"\n",
    "        return self._create_store(\n",
    "            Neo4jVectorStore,\n",
    "            \"Neo4jVectorStore\",\n",
    "            embedding_dimension=embedding_dimension,\n",
    "            hybrid_search=hybrid_search,\n",
    "        )\n",
    "\n",
    "    def _create_store(self, store_class: type, store_name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper method to create a store instance with the provided configuration.\n",
    "\n",
    "        :param store_class: The class of the store to be created.\n",
    "        :param store_name: The name of the store, used for logging purposes.\n",
    "        :param kwargs: Additional configuration parameters for the store.\n",
    "        :return: Configured store instance.\n",
    "        :raises RuntimeError: If store creation fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            store_instance = store_class(\n",
    "                username=self.username,\n",
    "                password=self.password,\n",
    "                url=self.url,\n",
    "                database=self.database,\n",
    "                **kwargs,\n",
    "            )\n",
    "            self.logger.success(f\"{store_name} instance created successfully.\")\n",
    "            return store_instance\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to create {store_name}: {e}\"\n",
    "            self.logger.error(error_message)\n",
    "            raise RuntimeError(error_message) from e"
   ],
   "id": "dbf8ac109349a64a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define your database configuration\n",
    "db_config = Neo4jDBManager()\n",
    "\n",
    "# Connect to the Neo4j database\n",
    "driver = GraphDatabase.driver(db_config.url, auth=(db_config.username, db_config.password))"
   ],
   "id": "1d36a1999c9e50ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class UnstructuredTransform(TransformComponent):\n",
    "    def __call__(self, docs, **kwargs):\n",
    "        pipeline = IngestionPipeline(transformations=[SimpleFileNodeParser()])\n",
    "        base_nodes = pipeline.run(documents=docs, show_progress=True)\n",
    "        for node in base_nodes:\n",
    "            print(node)\n",
    "        return base_nodes"
   ],
   "id": "327961e21e30c4e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SchemaHandler:\n",
    "    \"\"\"\n",
    "    Handles schema definitions for knowledge graph validation, including entities, relations,\n",
    "    and validation schemas specifically for smart contract reentrancy detection.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_validation_schema() -> List[Tuple[str, str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieve the validation schema defining valid triples in the knowledge graph for reentrancy detection.\n",
    "\n",
    "        :return: A list of tuples representing valid (entity, relation, entity) triples.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            # Smart Contract-specific triples\n",
    "            (\"SMART_CONTRACT\", \"CONTAINS\", \"FUNCTION\"),\n",
    "            (\"SMART_CONTRACT\", \"DEPLOYS\", \"SMART_CONTRACT\"),\n",
    "            (\"SMART_CONTRACT\", \"INTERACTS_WITH\", \"EXTERNAL_CONTRACT\"),\n",
    "            (\"SMART_CONTRACT\", \"USES\", \"VARIABLE\"),\n",
    "            (\"SMART_CONTRACT\", \"CALLS\", \"FUNCTION\"),\n",
    "            (\"SMART_CONTRACT\", \"SUFFERED_FROM\", \"VULNERABILITY\"),\n",
    "            (\"SMART_CONTRACT\", \"ASSOCIATED_WITH\", \"REENTRANCY_PATTERN\"),\n",
    "\n",
    "            # Function-related triples\n",
    "            (\"FUNCTION\", \"CALLS\", \"FUNCTION\"),\n",
    "            (\"FUNCTION\", \"CALLS\", \"EXTERNAL_FUNCTION\"),\n",
    "            (\"FUNCTION\", \"CONTAINS\", \"STATE_CHANGE\"),\n",
    "            (\"FUNCTION\", \"READS\", \"VARIABLE\"),\n",
    "            (\"FUNCTION\", \"WRITES\", \"VARIABLE\"),\n",
    "            (\"FUNCTION\", \"USES\", \"REENTRANCY_PATTERN\"),\n",
    "            (\"FUNCTION\", \"TRIGGERED_BY\", \"TRANSACTION\"),\n",
    "\n",
    "            # Vulnerability-related triples\n",
    "            (\"VULNERABILITY\", \"AFFECTS\", \"FUNCTION\"),\n",
    "            (\"VULNERABILITY\", \"RELATED_TO\", \"REENTRANCY_PATTERN\"),\n",
    "            (\"VULNERABILITY\", \"EXPLOITS\", \"STATE_CHANGE\"),\n",
    "            (\"VULNERABILITY\", \"FOUND_IN\", \"SMART_CONTRACT\"),\n",
    "\n",
    "            # State Change-related triples\n",
    "            (\"STATE_CHANGE\", \"MODIFIES\", \"VARIABLE\"),\n",
    "            (\"STATE_CHANGE\", \"LEADS_TO\", \"VULNERABILITY\"),\n",
    "            (\"STATE_CHANGE\", \"TRIGGERED_BY\", \"CALL\"),\n",
    "\n",
    "            # Variable-related triples\n",
    "            (\"VARIABLE\", \"MODIFIED_BY\", \"FUNCTION\"),\n",
    "            (\"VARIABLE\", \"READ_BY\", \"FUNCTION\"),\n",
    "            (\"VARIABLE\", \"AFFECTED_BY\", \"STATE_CHANGE\"),\n",
    "\n",
    "            # Call-related triples\n",
    "            (\"CALL\", \"MAKES\", \"EXTERNAL_CALL\"),\n",
    "            (\"CALL\", \"RETURNS\", \"VALUE\"),\n",
    "            (\"CALL\", \"RESULTS_IN\", \"STATE_CHANGE\"),\n",
    "\n",
    "            # Reentrancy-related triples\n",
    "            (\"REENTRANCY_PATTERN\", \"IDENTIFIED_IN\", \"FUNCTION\"),\n",
    "            (\"REENTRANCY_PATTERN\", \"LEADS_TO\", \"VULNERABILITY\"),\n",
    "            (\"REENTRANCY_PATTERN\", \"EXPLOITED_BY\", \"CALL\"),\n",
    "\n",
    "            # Transaction-related triples\n",
    "            (\"TRANSACTION\", \"TRIGGERS\", \"FUNCTION\"),\n",
    "            (\"TRANSACTION\", \"LEADS_TO\", \"STATE_CHANGE\"),\n",
    "            (\"TRANSACTION\", \"RESULTS_IN\", \"VULNERABILITY\"),\n",
    "            (\"TRANSACTION\", \"SENT_TO\", \"SMART_CONTRACT\"),\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_entities() -> Type[str]:\n",
    "        \"\"\"\n",
    "        Retrieve the list of possible entity types for the knowledge graph.\n",
    "\n",
    "        :return: A Literal type representing the valid entity types.\n",
    "        \"\"\"\n",
    "        return Literal[\n",
    "            \"SMART_CONTRACT\", \"FUNCTION\", \"EXTERNAL_FUNCTION\", \"VARIABLE\",\n",
    "            \"STATE_CHANGE\", \"CALL\", \"EXTERNAL_CALL\", \"REENTRANCY_PATTERN\",\n",
    "            \"VULNERABILITY\", \"TRANSACTION\", \"EXTERNAL_CONTRACT\", \"VALUE\"\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_relations() -> Type[str]:\n",
    "        \"\"\"\n",
    "        Retrieve the list of possible relation types for the knowledge graph.\n",
    "\n",
    "        :return: A Literal type representing the valid relation types.\n",
    "        \"\"\"\n",
    "        return Literal[\n",
    "            \"CONTAINS\", \"CALLS\", \"READS\", \"WRITES\", \"MODIFIES\", \"TRIGGERED_BY\",\n",
    "            \"LEADS_TO\", \"RESULTS_IN\", \"AFFECTS\", \"RELATED_TO\", \"FOUND_IN\",\n",
    "            \"IDENTIFIED_IN\", \"EXPLOITED_BY\", \"SUFFERED_FROM\", \"DEPLOYS\",\n",
    "            \"INTERACTS_WITH\", \"USES\", \"MAKES\", \"RETURNS\", \"SENT_TO\"\n",
    "        ]"
   ],
   "id": "a923b0617156a931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class KnowledgeManager(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for managing indexing, retrieval, and querying of knowledge data.\n",
    "    Provides methods for indexing documents, creating a chat engine, and executing queries.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, store: PropertyGraphStore, storage_context: StorageContext) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the KnowledgeManager with a store and storage context.\n",
    "\n",
    "        :param store: The storage backend for managing knowledge data.\n",
    "        :param storage_context: Configuration context for the storage backend.\n",
    "        \"\"\"\n",
    "        self.store = store\n",
    "        self.storage_context = storage_context\n",
    "        self.persist_dir: str = \"\"\n",
    "        self.index: Optional[BaseIndex] = None\n",
    "        self.chat_engine: Optional[BaseChatEngine] = None\n",
    "        self.logger = DebugLogger(use_panel_for_errors=True)\n",
    "\n",
    "    def get_index(self) -> Optional[BaseIndex]:\n",
    "        \"\"\"\n",
    "        Get the current index, if available.\n",
    "\n",
    "        :return: The current index or None if not initialized.\n",
    "        \"\"\"\n",
    "        return self.index\n",
    "\n",
    "    def get_query_engine(self) -> Optional[BaseChatEngine]:\n",
    "        \"\"\"\n",
    "        Get the current chat engine, if available.\n",
    "\n",
    "        :return: The current chat engine or None if not initialized.\n",
    "        \"\"\"\n",
    "        return self.chat_engine\n",
    "\n",
    "    def index_documents(self, documents: List[Document], reload_index: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Index a list of documents into the knowledge store.\n",
    "\n",
    "        :param documents: List of Document objects to be indexed.\n",
    "        :param reload_index: Whether to reload an existing index if available.\n",
    "        :raises Exception: If an error occurs during indexing.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if reload_index and self.load_index():\n",
    "                self.logger.success(\"Index loaded successfully. Skipping re-indexing.\")\n",
    "                return\n",
    "\n",
    "            if self.index:\n",
    "                self.refresh_index(documents)\n",
    "\n",
    "            self.logger.info(\"Starting document indexing... This may take a while.\")\n",
    "            self.create_index(documents)\n",
    "            self.logger.success(\"Document indexing completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error during document indexing:\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def refresh_index(self, documents: List[Document]):\n",
    "        try:\n",
    "            if not self.index and self.load_index():\n",
    "                self.logger.success(\"Index loaded successfully.\")\n",
    "\n",
    "            self.logger.info(\"Re-indexing with new documents.\")\n",
    "            self.index.refresh(documents, transformations=[UnstructuredTransform()])\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error during new document indexing:\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    def create_chat_engine(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the chat engine using the current retriever.\n",
    "\n",
    "        :raises ValueError: If the chat engine setup fails.\n",
    "        \"\"\"\n",
    "        if not self.index:\n",
    "            error_message = \"Cannot create chat engine: Index is not initialized.\"\n",
    "            self.logger.error(error_message)\n",
    "            raise ValueError(error_message)\n",
    "\n",
    "        try:\n",
    "            self.chat_engine = self.index.as_chat_engine(verbose=False)\n",
    "            self.logger.success(\"Query engine initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            error_message = f\"Failed to initialize chat engine: {e}\"\n",
    "            self.logger.error(error_message, exc_info=True)\n",
    "            raise ValueError(error_message) from e\n",
    "\n",
    "    def execute_query(self, query: str) -> Optional[AgentChatResponse]:\n",
    "        \"\"\"\n",
    "        Execute a query on the knowledge store and return the result.\n",
    "\n",
    "        :param query: The query string to execute.\n",
    "        :return: The response object if the query is successful, or None otherwise.\n",
    "        \"\"\"\n",
    "        if not self.chat_engine:\n",
    "            self.logger.info(\"Query engine not initialized. Creating a new chat engine...\")\n",
    "            self.create_chat_engine()\n",
    "\n",
    "        try:\n",
    "            return self.chat_engine.chat(query)\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Query execution failed:\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    @abstractmethod\n",
    "    def create_index(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Abstract method to create an index from a list of documents.\n",
    "\n",
    "        :param documents: List of Document objects to be indexed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Abstract method to load an existing index from storage.\n",
    "\n",
    "        :return: True if the index was loaded successfully, False otherwise.\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "id": "65743035dbabd46",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GraphManager(KnowledgeManager):\n",
    "    \"\"\"\n",
    "    Manages graph store operations, including cleaning, indexing, and query execution.\n",
    "    Extends KnowledgeManager to support configurations specific to graph-based indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, store: PropertyGraphStore, storage_context: StorageContext) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the GraphManager with a graph-based store and storage context.\n",
    "\n",
    "        :param store: The graph data storage backend.\n",
    "        :param storage_context: Context or configuration settings for storage management.\n",
    "        \"\"\"\n",
    "        super().__init__(store, storage_context)\n",
    "        self.persist_dir = \"graph_index\"\n",
    "        self.logger.info(\"GraphManager initialized with a graph store.\")\n",
    "\n",
    "    def create_index(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Index a list of documents into the knowledge graph with specified configurations.\n",
    "\n",
    "        :param documents: List of Document objects to be indexed.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting document indexing into the graph store. This may take some time.\")\n",
    "\n",
    "        try:\n",
    "            self.index = PropertyGraphIndex.from_documents(\n",
    "                documents=documents,\n",
    "                kg_extractors=[\n",
    "                    #SimpleLLMPathExtractor(),\n",
    "                    SchemaLLMPathExtractor(\n",
    "                        llm=ModelManager().get_llm(LLM_MODE),\n",
    "                        possible_entities=SchemaHandler.get_entities(),\n",
    "                        possible_relations=SchemaHandler.get_relations(),\n",
    "                        kg_validation_schema=SchemaHandler.get_validation_schema(),\n",
    "                        strict=False,\n",
    "                        max_triplets_per_chunk=3\n",
    "                    ),\n",
    "                ],\n",
    "                property_graph_store=self.store,\n",
    "                storage_context=self.storage_context,\n",
    "                embed_kg_nodes=True,\n",
    "                show_progress=True,\n",
    "                transformations=[UnstructuredTransform()],\n",
    "            )\n",
    "            self.logger.success(\"Document indexing completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error during document indexing:\", exc_info=True)\n",
    "            raise RuntimeError(\"Graph indexing failed.\") from e\n",
    "\n",
    "    def load_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load the index from the graph store if available.\n",
    "\n",
    "        :return: True if the index was successfully loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        if not self.store:\n",
    "            self.logger.warning(\"No graph store is available. Unable to load index.\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            self.logger.info(\"Attempting to load the index from the graph store.\")\n",
    "            self.index = PropertyGraphIndex.from_existing(\n",
    "                property_graph_store=self.store, embed_kg_nodes=True\n",
    "            )\n",
    "            self.logger.success(\"Index loaded successfully from the graph store.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error while loading the index:\", exc_info=True)\n",
    "            return False"
   ],
   "id": "82414e0fb9f68439",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VectorManager(KnowledgeManager):\n",
    "    \"\"\"\n",
    "    Manages vector store operations, including document indexing, retrieval,\n",
    "    and querying within a vector-based storage system.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, store: VectorStore, storage_context: StorageContext) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the VectorManager with a vector storage backend and context configuration.\n",
    "\n",
    "        :param store: The vector store used for managing indexed data.\n",
    "        :param storage_context: Context or configuration for managing vector storage.\n",
    "        \"\"\"\n",
    "        super().__init__(store, storage_context)\n",
    "        self.persist_dir = \"vector_index\"\n",
    "        self.logger.info(\"VectorManager initialized with vector storage backend.\")\n",
    "\n",
    "    def create_index(self, documents: List[Document]) -> None:\n",
    "        \"\"\"\n",
    "        Index a list of documents into the vector store.\n",
    "\n",
    "        :param documents: List of Document objects to be indexed.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting vector document indexing... This may take a while.\")\n",
    "\n",
    "        try:\n",
    "            # Create the vector index with the given documents and context\n",
    "            self.index = VectorStoreIndex.from_documents(\n",
    "                documents=documents,\n",
    "                storage_context=self.storage_context,\n",
    "                show_progress=True,\n",
    "                store_nodes_override=True,\n",
    "                transformations=[UnstructuredTransform()]\n",
    "            )\n",
    "\n",
    "            # Debugging output for indexed documents\n",
    "            self.logger.debug(f\"Indexed documents: {self.index.storage_context.docstore.docs}\")\n",
    "            self.logger.success(\"Vector document indexing completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error occurred during vector document indexing:\", exc_info=True)\n",
    "            raise RuntimeError(\"Vector document indexing failed.\") from e\n",
    "\n",
    "    def load_index(self) -> bool:\n",
    "        \"\"\"\n",
    "        Load the index from the vector store if available.\n",
    "\n",
    "        :return: True if the index was successfully loaded, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Attempting to load the index from the vector store.\")\n",
    "            self.index = VectorStoreIndex.from_vector_store(vector_store=self.store)\n",
    "            self.logger.success(\"Index loaded successfully from the vector store.\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            self.logger.error(\"Error occurred while loading the index:\", exc_info=True)\n",
    "            return False"
   ],
   "id": "c65cc5d83e3b536a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class RAG:\n",
    "    \"\"\"\n",
    "    A hybrid retrieval pipeline utilizing Neo4j for both vector-based document retrieval\n",
    "    and knowledge graph storage. Supports structured and unstructured query handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logger: Optional[DebugLogger] = None, db_manager: Optional[Neo4jDBManager] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the RAG pipeline with components for graph and vector-based retrieval.\n",
    "\n",
    "        :param db_manager: Instance of Neo4jDBManager for database interaction (optional).\n",
    "        \"\"\"\n",
    "        self.logger = logger or DebugLogger(use_panel_for_errors=True)\n",
    "        self.db_manager = db_manager or Neo4jDBManager()\n",
    "        self.chat_engine = None\n",
    "        self.knowledge_manager = None\n",
    "        self._initialize_managers()\n",
    "\n",
    "    def _initialize_managers(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes necessary managers (e.g., RAG Manager).\n",
    "        Placeholder for future initialization logic.\n",
    "        \"\"\"\n",
    "        # Example initialization (Replace with actual manager setup)\n",
    "        self.logger.info(\"Initializing RAG managers...\")\n",
    "        # self.knowledge_manager = RAGManager(self.db_manager)\n",
    "\n",
    "    def load_and_index_documents(self, folder_path: str, reload_index: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Loads, chunks, and indexes documents into the Neo4j vector store and knowledge graph.\n",
    "\n",
    "        :param folder_path: Path to the folder containing document files.\n",
    "        :param reload_index: If True, reloads the index regardless of existing data.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"{'Reloading' if reload_index else 'Loading'} documents from: {folder_path}\")\n",
    "\n",
    "        docs = [] if reload_index else self._load_docs(folder_path)\n",
    "\n",
    "        if not docs and not reload_index:\n",
    "            self.logger.warning(\"No documents available for indexing.\")\n",
    "            return\n",
    "\n",
    "        self._index(docs, reload_index=reload_index)\n",
    "\n",
    "    @staticmethod\n",
    "    @DebugLogger.profile\n",
    "    def _load_docs(folder_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Loads documents from the specified folder.\n",
    "\n",
    "        :param folder_path: Path to the folder containing document files.\n",
    "        :return: List of loaded Document objects.\n",
    "        \"\"\"\n",
    "        category = folder_path.split(os.sep)[-1]\n",
    "        metadata_fn = lambda x: {\"filename\": x.split(os.sep)[-1], \"category\": category}\n",
    "        reader = SimpleDirectoryReader(input_dir=folder_path, errors=\"strict\", encoding=\"latin-1\",\n",
    "                                       file_metadata=metadata_fn)\n",
    "        docs = reader.load_data(show_progress=True)\n",
    "        for doc in docs:\n",
    "            print(doc)\n",
    "        return docs\n",
    "\n",
    "    @DebugLogger.profile\n",
    "    def _index(self, docs: List[Document], reload_index: bool) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the given documents in the vector store.\n",
    "\n",
    "        :param docs: List of Document objects to index.\n",
    "        :param reload_index: If True, reloads the index.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Indexing documents into Neo4j...\")\n",
    "            self.knowledge_manager.index_documents(docs, reload_index)\n",
    "            self.logger.success(\"Document indexing completed successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during indexing: {e}\", exc_info=True)\n",
    "\n",
    "    def _initialize_chat_engine(self) -> None:\n",
    "        \"\"\"\n",
    "        Sets up the RAG chat engine for document retrieval.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Initializing chat engine...\")\n",
    "            self.knowledge_manager.create_chat_engine()\n",
    "            self.chat_engine = self.knowledge_manager.get_query_engine()\n",
    "            self.logger.success(\"Query engine initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize chat engine: {e}\", exc_info=True)\n",
    "\n",
    "    def as_chat_engine(self) -> RetrieverQueryEngine:\n",
    "        if not self.chat_engine:\n",
    "            self._initialize_chat_engine()\n",
    "        return self.chat_engine\n",
    "\n",
    "    @DebugLogger.profile\n",
    "    def query(self, question: str) -> Optional[Response]:\n",
    "        \"\"\"\n",
    "        Executes a query using the vector store.\n",
    "\n",
    "        :param question: The input query as a string.\n",
    "        :return: Query response or None in case of an error.\n",
    "        \"\"\"\n",
    "        if not self.chat_engine:\n",
    "            self._initialize_chat_engine()\n",
    "\n",
    "        try:\n",
    "            self.logger.debug(f\"Executing query: {question}\")\n",
    "            response = self.knowledge_manager.execute_query(question)\n",
    "            self.logger.success(\"Query executed successfully.\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during query execution: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_sources(nodes: List[NodeWithScore]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Filters nodes by similarity score and extracts unique source filenames.\n",
    "\n",
    "        :param nodes: List of nodes from the query response.\n",
    "        :return: List of unique filenames from filtered nodes.\n",
    "        \"\"\"\n",
    "        processor = SimilarityPostprocessor(similarity_cutoff=0.75)\n",
    "        filtered_nodes = processor.postprocess_nodes(nodes)\n",
    "        print(filtered_nodes[0])\n",
    "        exit()\n",
    "        return list({node.node.metadata[\"file_name\"] for node in filtered_nodes if \"file_name\" in node.node.metadata})"
   ],
   "id": "2515d44e6518235",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GraphRAG(RAG):\n",
    "    \"\"\"\n",
    "    A specialized implementation of the RAG (Retrieval-Augmented Generation) pipeline\n",
    "    that leverages Neo4j for both vector-based document retrieval and knowledge graph storage.\n",
    "\n",
    "    This class supports structured and unstructured query handling by integrating graph-based\n",
    "    storage and retrieval mechanisms with the RAG pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the GraphRAG pipeline with components for graph-based and vector-based retrieval.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def _initialize_managers(self) -> None:\n",
    "        \"\"\"\n",
    "        Configures the graph manager and its associated storage contexts.\n",
    "\n",
    "        This method sets up the graph store and initializes the graph manager, enabling\n",
    "        efficient graph-based storage and retrieval operations. It ensures that the pipeline\n",
    "        can handle both graph structures and their integration with vector-based retrieval.\n",
    "\n",
    "        :raises Exception:\n",
    "            If the initialization of the graph manager fails, an error is logged with details.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Initializing graph manager...\")\n",
    "\n",
    "            # Create the graph store and its associated storage context\n",
    "            graph_store = self.db_manager.create_graph_store()\n",
    "            graph_storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "            # Initialize the GraphManager\n",
    "            self.knowledge_manager = GraphManager(graph_store, graph_storage_context)\n",
    "\n",
    "            self.logger.success(\"Graph manager initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize graph manager: {e}\", exc_info=True)"
   ],
   "id": "6e0f6c6a811f4031",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class VectorRAG(RAG):\n",
    "    \"\"\"\n",
    "    A specialized implementation of the Retrieval-Augmented Generation (RAG) pipeline\n",
    "    that focuses on vector-based document retrieval.\n",
    "\n",
    "    This class integrates vector-based storage and retrieval functionality, supporting\n",
    "    efficient query execution and management of vector embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the VectorRAG pipeline by setting up components for vector-based retrieval.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def _initialize_managers(self) -> None:\n",
    "        \"\"\"\n",
    "        Configures the vector manager and its associated storage context.\n",
    "\n",
    "        This method sets up the vector store and initializes the vector manager,\n",
    "        enabling efficient storage, retrieval, and management of vector embeddings.\n",
    "\n",
    "        :raises Exception:\n",
    "            Logs any errors encountered during the initialization process.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Initializing vector manager...\")\n",
    "\n",
    "            # Create the vector store and its associated storage context\n",
    "            vector_store = self.db_manager.create_vector_store()\n",
    "            vector_storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "            # Initialize the VectorManager with the configured store and context\n",
    "            self.knowledge_manager = VectorManager(vector_store, vector_storage_context)\n",
    "\n",
    "            self.logger.success(\"Vector manager initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize vector manager: {e}\", exc_info=True)"
   ],
   "id": "47f6e9c599638900",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rag = VectorRAG()",
   "id": "d077a96c201a513d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_to_reentrant_train = os.path.join(\"..\", \"dataset\", \"manually-verified-train\", \"reentrant\")\n",
    "rag.load_and_index_documents(path_to_reentrant_train, reload_index=False)"
   ],
   "id": "c4bd2bb00d086dad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "path_to_safe_train = os.path.join(\"..\", \"dataset\", \"manually-verified-train\", \"safe\")\n",
    "rag.load_and_index_documents(path_to_safe_train, reload_index=False)"
   ],
   "id": "9f2ed9f8068152ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"\n",
    "\n",
    "You must follow these steps:\n",
    "\n",
    "1. **Retrieve Examples**:\n",
    "   Search your knowledge base for relevant examples of Solidity smart contracts labeled as **reentrant** or **non-reentrant**. Focus on:\n",
    "   - Contracts with **reentrancy vulnerabilities**, such as making external calls (`call`, `delegatecall`, `transfer`) before updating state variables.\n",
    "   - Contracts that use **mitigations** like the *checks-effects-interactions* pattern, `ReentrancyGuard` modifiers, or mutex locks.\n",
    "\n",
    "   Provide **contract snippets** and **explanations** of why these examples were labeled as reentrant or non-reentrant.\n",
    "\n",
    "2. **Analyze the Target Contract**:\n",
    "   Carefully analyze the **input Solidity contract** to identify:\n",
    "   - Use of external calls (`msg.sender.call`, `delegatecall`, `send`, etc.).\n",
    "   - Whether state variables are updated **before** or **after** the external call.\n",
    "   - Reentrancy mitigations like `ReentrancyGuard` modifiers or the *checks-effects-interactions* pattern.\n",
    "\n",
    "3. **Classify**:\n",
    "   Based on the retrieved examples and your analysis, classify the target contract as:\n",
    "   - **Reentrant**: If it contains vulnerabilities that allow external calls before updating state variables.\n",
    "   - **Non-Reentrant**: If it uses proper safeguards or patterns to prevent reentrancy.\n",
    "\n",
    "4. **Justify the Classification**:\n",
    "   Explain your reasoning in detail. Compare the patterns you observed in the target contract with the retrieved examples. Highlight specific lines or functions that led to your conclusion.\n",
    "\n",
    "5. **Output**:\n",
    "   Return the result in the following structured JSON format:\n",
    "\n",
    "---\n",
    "\n",
    "### Output Format\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"classification\": \"Reentrant / Non-Reentrant\",\n",
    "  \"analysis\": \"Key observations about the target contract, including function behaviors, external calls, and state updates.\",\n",
    "  \"justification\": \"Provide a detailed explanation of your reasoning.\"\n",
    "}\n",
    "```\n",
    "\n",
    "Important: The output must be the JSON only.\n",
    "\n",
    "---\n",
    "\n",
    "### Input\n",
    "\n",
    "\"\"\""
   ],
   "id": "b8078e6949e0bdbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_and_parse_json(text):\n",
    "    \"\"\" Extracts and parses the first valid JSON object from a string with extra content. \"\"\"\n",
    "    try:\n",
    "        # Find the first balanced JSON object using a brace counter\n",
    "        start = text.find(\"{\")\n",
    "        if start == -1:\n",
    "            raise ValueError(\"No JSON found in the input text.\")\n",
    "\n",
    "        brace_count = 0\n",
    "        json_str = \"\"\n",
    "\n",
    "        # Iterate over characters to find balanced braces\n",
    "        for i in range(start, len(text)):\n",
    "            char = text[i]\n",
    "            json_str += char\n",
    "\n",
    "            if char == \"{\":\n",
    "                brace_count += 1\n",
    "            elif char == \"}\":\n",
    "                brace_count -= 1\n",
    "\n",
    "            # Stop when all braces are balanced\n",
    "            if brace_count == 0:\n",
    "                break\n",
    "\n",
    "        # Parse the extracted JSON string\n",
    "        parsed_json = json.loads(json_str)\n",
    "        print(f\"Extracted JSON: {json_str}\")\n",
    "        return parsed_json\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise"
   ],
   "id": "2157ab339b466714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def test_rag(path_to_contracts, rag, prompt):\n",
    "    \"\"\" Tests the RAG model's ability to classify Solidity contracts in a directory. \"\"\"\n",
    "    correct = 0\n",
    "\n",
    "    # Extract ground truth category from the path\n",
    "    gt_category = os.path.basename(path_to_contracts)\n",
    "\n",
    "    # Create a unique log directory\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    path_to_log = os.path.join(\"log\", f\"test_{gt_category}_{timestamp}\")\n",
    "    os.makedirs(path_to_log, exist_ok=True)\n",
    "\n",
    "    # Cache list of files to avoid multiple os.listdir calls\n",
    "    files = [f for f in os.listdir(path_to_contracts)\n",
    "             if f.endswith(\".sol\") and os.path.isfile(os.path.join(path_to_contracts, f))]\n",
    "    total_files = len(files)\n",
    "\n",
    "    logger.info(f\"Testing RAG model on {total_files} files from category: {gt_category}\")\n",
    "    logger.info(f\"Results will be logged in: {path_to_log}\")\n",
    "\n",
    "    for index, filename in enumerate(files, start=48):\n",
    "        try:\n",
    "            # Build the full path to the file\n",
    "            path_to_file = os.path.join(path_to_contracts, filename)\n",
    "\n",
    "            # Read contract content\n",
    "            with open(path_to_file, 'r', encoding='latin-1') as file:\n",
    "                contract_content = file.read()\n",
    "\n",
    "            # Query the RAG system\n",
    "            answer = rag.query(prompt + contract_content)\n",
    "            sources = rag.fetch_sources(answer.source_nodes)\n",
    "\n",
    "            logger.debug(f\"[{index}/{total_files}] Processing file: {filename}\")\n",
    "            logger.debug(f\"*** ANSWER ***:\\n{answer}\\n --> SOURCES: {sources}\")\n",
    "\n",
    "            # Extract and parse JSON from the answer\n",
    "            json_answer = extract_and_parse_json(str(answer))\n",
    "            json_answer[\"sources\"] = sources\n",
    "\n",
    "            # Write the JSON output to a file\n",
    "            output_path = os.path.join(path_to_log, f\"{filename}.json\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "                json.dump(json_answer, output_file, indent=4)\n",
    "\n",
    "            # Check classification accuracy\n",
    "            if json_answer.get(\"classification\") == gt_category:\n",
    "                correct += 1\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {path_to_file}\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"Error decoding JSON for file {filename}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred processing {filename}: {e}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total_files if total_files > 0 else 0\n",
    "    logger.info(f\"Classification Accuracy for '{gt_category}': {accuracy:.2%}\")\n",
    "\n",
    "    # Summary log\n",
    "    print(f\"Processed {total_files} files. Accuracy: {accuracy:.2%}\")\n"
   ],
   "id": "48f7fdca5033182d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_to_reentrant_test = os.path.join(\"..\", \"dataset\", \"manually-verified-preprocessed-test\", \"source\", \"reentrant\")\n",
    "test_rag(path_to_reentrant_test, rag, prompt)"
   ],
   "id": "9cdd0e41d5ced949",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path_to_safe_test = os.path.join(\"..\", \"dataset\", \"manually-verified-preprocessed-test\", \"source\", \"safe\")\n",
    "test_rag(path_to_safe_test, rag, prompt)"
   ],
   "id": "6c5e5a21839b34bd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
