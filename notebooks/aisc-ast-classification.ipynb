{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:43:00.636652Z",
     "start_time": "2024-06-28T10:43:00.627824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "23581b0e55d01ac3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Extraction",
   "id": "412905987be26ead"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:43:02.120623Z",
     "start_time": "2024-06-28T10:43:02.113345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hash_feature(value, num_bins=1000):\n",
    "    \"\"\"Helper function to hash a value into a fixed number of bins.\"\"\"\n",
    "    return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % num_bins\n",
    "\n",
    "\n",
    "def extract_features(node):\n",
    "    # Initialize features with default values\n",
    "    name_feature, value_feature = [0], [0]\n",
    "    src_feature, type_desc_features = [0, 0], [0, 0]\n",
    "    state_mutability_feature, visibility_feature = [0], [0]\n",
    "\n",
    "    # Extract basic features\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    type_feature = [hash_feature(node_type)]\n",
    "\n",
    "    # Extract additional features if they exist\n",
    "    if 'name' in node:\n",
    "        name_feature = [hash_feature(node.get('name', ''))]\n",
    "    if 'value' in node:\n",
    "        value_feature = [hash_feature(node.get('value', ''))]\n",
    "\n",
    "    # Extract src features (start, end, and length if available)\n",
    "    if 'src' in node:\n",
    "        start, length, *_ = map(int, node['src'].split(':'))\n",
    "        src_feature = [start, length]\n",
    "\n",
    "    # Extract typeDescriptions features if they exist\n",
    "    if 'typeDescriptions' in node:\n",
    "        type_desc = node['typeDescriptions']\n",
    "        type_desc_features = [hash_feature(type_desc.get('typeString', '')),\n",
    "                              hash_feature(type_desc.get('typeIdentifier', ''))]\n",
    "\n",
    "    # Extract stateMutability if it exists\n",
    "    if 'stateMutability' in node:\n",
    "        state_mutability_feature = [hash_feature(node.get('stateMutability', ''))]\n",
    "\n",
    "    # Extract visibility if it exists\n",
    "    if 'visibility' in node:\n",
    "        visibility_feature = [hash_feature(node.get('visibility', ''))]\n",
    "\n",
    "    # Combine all features into a single feature vector\n",
    "    return (type_feature + name_feature + value_feature + src_feature +\n",
    "            type_desc_features + state_mutability_feature + visibility_feature)\n"
   ],
   "id": "43457b36d1175282",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Loading",
   "id": "a6a935e8ee0ab0c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:43:05.815331Z",
     "start_time": "2024-06-28T10:43:03.378639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_masks_to_data(data, train_ratio=0.8, val_ratio=0.1):\n",
    "    num_nodes = data.x.size(0)\n",
    "    indices = torch.randperm(num_nodes)\n",
    "\n",
    "    train_size, val_size = int(num_nodes * train_ratio), int(num_nodes * val_ratio)\n",
    "\n",
    "    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "    train_mask[indices[:train_size]] = True\n",
    "    val_mask[indices[train_size:train_size + val_size]] = True\n",
    "    test_mask[indices[train_size + val_size:]] = True\n",
    "\n",
    "    data.train_mask, data.val_mask, data.test_mask = train_mask, val_mask, test_mask\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def ast_to_graph(ast_json):\n",
    "    graph = nx.DiGraph()\n",
    "    node_id = 0\n",
    "\n",
    "    def add_nodes_edges(node, parent=None):\n",
    "        nonlocal node_id\n",
    "        current_node_id = node_id\n",
    "        graph.add_node(current_node_id, features=extract_features(node))\n",
    "        if parent is not None:\n",
    "            graph.add_edge(parent, current_node_id)\n",
    "        node_id += 1\n",
    "        for key, value in node.items():\n",
    "            if isinstance(value, dict):\n",
    "                add_nodes_edges(value, current_node_id)\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, dict):\n",
    "                        add_nodes_edges(item, current_node_id)\n",
    "\n",
    "    add_nodes_edges(ast_json)\n",
    "    edge_index = torch.tensor(list(graph.edges)).t().contiguous()\n",
    "    x = torch.stack([torch.tensor(graph.nodes[n]['features'], dtype=torch.float) for n in graph.nodes])\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return add_masks_to_data(data)\n",
    "\n",
    "\n",
    "def generate_label_map(ast_directory):\n",
    "    label_map = {}\n",
    "    label_index = 0\n",
    "    for category in os.listdir(ast_directory):\n",
    "        category_path = os.path.join(ast_directory, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            label_map[category] = label_index\n",
    "            label_index += 1\n",
    "    return label_map\n",
    "\n",
    "\n",
    "def load_data(ast_directory, label_map):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for category in os.listdir(ast_directory):\n",
    "        category_path = os.path.join(ast_directory, category)\n",
    "        if os.path.isdir(category_path):\n",
    "            for root, _, files in os.walk(category_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.json'):\n",
    "                        filepath = os.path.join(root, file)\n",
    "                        with open(filepath, 'r') as f:\n",
    "                            ast = json.load(f)\n",
    "                        data = ast_to_graph(ast)\n",
    "                        label = label_map[category]\n",
    "                        data.y = torch.tensor([label] * data.x.size(0), dtype=torch.long)  # Assign label to all nodes\n",
    "                        dataset.append(data)\n",
    "                        labels.append(label)\n",
    "    print(f\"Loaded {len(dataset)} samples from {ast_directory}\")\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "ast_directory = '../dataset/aisc/ast'\n",
    "label_map = generate_label_map(ast_directory)\n",
    "dataset, labels = load_data(ast_directory, label_map)\n",
    "\n",
    "if len(dataset) == 0:\n",
    "    print(\"No data loaded. Please check the dataset directory and files.\")"
   ],
   "id": "a003fe9d27ab8c41",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1020 samples from ../dataset/aisc/ast\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data Augmentation",
   "id": "abeead762cbb1622"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:43:05.828938Z",
     "start_time": "2024-06-28T10:43:05.816654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def substitute_nodes(ast, substitutions):\n",
    "    \"\"\"\n",
    "    Substitute certain nodes in the AST with other semantically equivalent nodes.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param substitutions: A dictionary where keys are node types to be replaced, and values are the replacements.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key in substitutions:\n",
    "                ast[key] = substitutions[key]\n",
    "            else:\n",
    "                ast[key] = substitute_nodes(value, substitutions)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = substitute_nodes(ast[i], substitutions)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def insert_nodes(ast, insertions):\n",
    "    \"\"\"\n",
    "    Insert certain nodes into the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param insertions: A dictionary where keys are locations to insert, and values are the nodes to be inserted.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key in insertions:\n",
    "                ast[key] = [value, insertions[key]] if isinstance(value, list) else [value, insertions[key]]\n",
    "            else:\n",
    "                ast[key] = insert_nodes(value, insertions)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = insert_nodes(ast[i], insertions)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def delete_nodes(ast, deletions):\n",
    "    \"\"\"\n",
    "    Delete certain nodes from the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param deletions: A set of node types to be deleted.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        keys_to_delete = [key for key in ast if key in deletions]\n",
    "        for key in keys_to_delete:\n",
    "            del ast[key]\n",
    "        for key, value in ast.items():\n",
    "            ast[key] = delete_nodes(value, deletions)\n",
    "    elif isinstance(ast, list):\n",
    "        ast = [delete_nodes(item, deletions) for item in ast if item not in deletions]\n",
    "    return ast\n",
    "\n",
    "\n",
    "def rename_identifiers(ast, renames):\n",
    "    \"\"\"\n",
    "    Rename variables/functions in the AST.\n",
    "    :param ast: The AST to be modified.\n",
    "    :param renames: A dictionary where keys are the original names and values are the new names.\n",
    "    :return: The modified AST.\n",
    "    \"\"\"\n",
    "    if isinstance(ast, dict):\n",
    "        for key, value in ast.items():\n",
    "            if key == 'name' and value in renames:\n",
    "                ast[key] = renames[value]\n",
    "            else:\n",
    "                ast[key] = rename_identifiers(value, renames)\n",
    "    elif isinstance(ast, list):\n",
    "        for i in range(len(ast)):\n",
    "            ast[i] = rename_identifiers(ast[i], renames)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def reorder_statements(ast):\n",
    "    if isinstance(ast, dict) and 'body' in ast:\n",
    "        if isinstance(ast['body'], list):\n",
    "            random.shuffle(ast['body'])\n",
    "        else:\n",
    "            reorder_statements(ast['body'])\n",
    "    elif isinstance(ast, list):\n",
    "        for item in ast:\n",
    "            reorder_statements(item)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def add_no_op_statements(ast):\n",
    "    no_op_statement = {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}\n",
    "    if isinstance(ast, dict) and 'body' in ast:\n",
    "        if isinstance(ast['body'], list):\n",
    "            ast['body'].append(no_op_statement)\n",
    "        else:\n",
    "            add_no_op_statements(ast['body'])\n",
    "    elif isinstance(ast, list):\n",
    "        for item in ast:\n",
    "            add_no_op_statements(item)\n",
    "    return ast\n",
    "\n",
    "\n",
    "def apply_augmentation(ast):\n",
    "    # Define your augmentation strategies\n",
    "    substitutions = {'FunctionDefinition': 'ModifierDefinition'}\n",
    "    insertions = {'body': {'nodeType': 'ExpressionStatement', 'expression': {'nodeType': 'Literal', 'value': '0'}}}\n",
    "    deletions = {'ModifierDefinition'}\n",
    "    renames = {'oldVarName': 'newVarName', 'oldFuncName': 'newFuncName'}\n",
    "\n",
    "    # Apply augmentations randomly\n",
    "    if random.random() > 0.5:\n",
    "        ast = substitute_nodes(ast, substitutions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = insert_nodes(ast, insertions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = delete_nodes(ast, deletions)\n",
    "    if random.random() > 0.5:\n",
    "        ast = rename_identifiers(ast, renames)\n",
    "    if random.random() > 0.5:\n",
    "        ast = reorder_statements(ast)\n",
    "    if random.random() > 0.5:\n",
    "        ast = add_no_op_statements(ast)\n",
    "\n",
    "    return ast\n",
    "\n",
    "\n",
    "def generate_augmented_asts(dataset, num_augmentations=5):\n",
    "    augmented_dataset = []\n",
    "    for ast in dataset:\n",
    "        augmented_dataset.append(ast)\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_ast = apply_augmentation(copy.deepcopy(ast))\n",
    "            augmented_dataset.append(augmented_ast)\n",
    "    return augmented_dataset\n",
    "\n"
   ],
   "id": "919e05313241418a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Graph Neural Network",
   "id": "f6c4e9bc867252e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:56:25.404814Z",
     "start_time": "2024-06-28T00:56:25.389384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class AugmentedASTDataset(Dataset):\n",
    "    def __init__(self, dataset, apply_augmentation_func, num_augmentations=5, train=True):\n",
    "        self.dataset = dataset\n",
    "        self.apply_augmentation_func = apply_augmentation_func\n",
    "        self.num_augmentations = num_augmentations\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset) * (self.num_augmentations if self.train else 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.dataset)\n",
    "        ast = self.dataset[original_idx]\n",
    "\n",
    "        if self.train:\n",
    "            augmented_ast = self.apply_augmentation_func(copy.deepcopy(ast))\n",
    "            return augmented_ast\n",
    "        else:\n",
    "            return ast\n",
    "\n",
    "\n",
    "def prepare_augmented_dataloader(dataset, apply_augmentation_func, batch_size=32, num_augmentations=5, shuffle=True):\n",
    "    augmented_dataset = AugmentedASTDataset(dataset, apply_augmentation_func, num_augmentations=num_augmentations,\n",
    "                                            train=True)\n",
    "    return DataLoader(augmented_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def prepare_dataloader(dataset, batch_size=32, shuffle=True):\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "\n",
    "def stratified_split(dataset, labels):\n",
    "    # Split data into training + validation and test data\n",
    "    train_val_data, test_data, train_val_labels, test_labels = train_test_split(dataset, labels,\n",
    "                                                                                test_size=0.1,\n",
    "                                                                                stratify=labels,\n",
    "                                                                                random_state=42)\n",
    "\n",
    "    # Split training + validation into actual training and validation data\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(train_val_data, train_val_labels,\n",
    "                                                                      test_size=0.11,\n",
    "                                                                      stratify=train_val_labels,\n",
    "                                                                      random_state=42)  # 0.11 * 0.9 â‰ˆ 0.1\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        pred = out[data.train_mask].argmax(dim=1)\n",
    "        y_true.extend(data.y[data.train_mask].cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "        y_score.extend(F.softmax(out[data.train_mask], dim=1).cpu().detach().numpy())\n",
    "\n",
    "    y_true, y_pred, y_score = np.array(y_true), np.array(y_pred), np.array(y_score)\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(loader),\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_score, average='weighted', multi_class='ovo')\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate(model, loader, mask_type):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_score = [], [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(data)\n",
    "            mask = getattr(data, mask_type)\n",
    "            pred = out[mask].argmax(dim=1)\n",
    "            y_true.extend(data.y[mask].cpu().numpy())\n",
    "            y_pred.extend(pred.cpu().numpy())\n",
    "            y_score.extend(F.softmax(out[mask], dim=1).cpu().numpy())\n",
    "\n",
    "    y_true, y_pred, y_score = np.array(y_true), np.array(y_pred), np.array(y_score)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_score, average='weighted', multi_class='ovo')\n",
    "    }\n"
   ],
   "id": "a47b081396d59fbe",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T00:58:05.708144Z",
     "start_time": "2024-06-28T00:57:23.259491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, val_data, test_data = stratified_split(dataset, labels)\n",
    "\n",
    "print(f\"Train data size ........ : {len(train_data)}\")\n",
    "print(f\"Validation data size ... : {len(val_data)}\")\n",
    "print(f\"Test data size ......... : {len(test_data)}\")\n",
    "\n",
    "train_loader = prepare_augmented_dataloader(train_data, apply_augmentation, batch_size=32, num_augmentations=5)\n",
    "val_loader = prepare_dataloader(val_data, batch_size=32)\n",
    "test_loader = prepare_dataloader(test_data, batch_size=32)\n",
    "\n",
    "num_features = train_data[0].x.shape[1]\n",
    "num_classes = len(label_map)\n",
    "\n",
    "model = GCN(num_features=num_features, hidden_channels=64, num_classes=num_classes).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1500):\n",
    "    train_metrics = train(model, train_loader, optimizer, criterion)\n",
    "    valid_metrics = evaluate(model, val_loader, 'val_mask')\n",
    "\n",
    "    print('-----------------------------------------------------------------------------------------------------')\n",
    "    print(f'EPOCH: {epoch + 1} -> Loss: {train_metrics[\"loss\"]:.4f}')\n",
    "    print(f'(Train) {\", \".join([\"{}: {:.4f}\".format(k, v) for k, v in train_metrics.items() if k != \"loss\"])}')\n",
    "    print(f'(Valid) {\", \".join([\"{}: {:.4f}\".format(k, v) for k, v in valid_metrics.items()])}')\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------------')\n",
    "test_metrics = evaluate(model, test_loader, 'test_mask')\n",
    "print(f'(Test) {\", \".join([\"{}: {:.4f}\".format(k, v) for k, v in test_metrics.items()])}')\n"
   ],
   "id": "6d732648de0dc590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size ........ : 817\n",
      "Validation data size ... : 101\n",
      "Test data size ......... : 102\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 1 -> Loss: 15.8837\n",
      "(Train) accuracy: 0.1298, precision: 0.1156, recall: 0.1298, f1: 0.0906, roc_auc: 0.4984\n",
      "(Valid) accuracy: 0.1879, precision: 0.0878, recall: 0.1879, f1: 0.0642, roc_auc: 0.5010\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 2 -> Loss: 2.2757\n",
      "(Train) accuracy: 0.1552, precision: 0.2173, recall: 0.1552, f1: 0.0494, roc_auc: 0.4863\n",
      "(Valid) accuracy: 0.1893, precision: 0.1049, recall: 0.1893, f1: 0.0646, roc_auc: 0.5048\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 3 -> Loss: 2.2699\n",
      "(Train) accuracy: 0.1561, precision: 0.2395, recall: 0.1561, f1: 0.0520, roc_auc: 0.4886\n",
      "(Valid) accuracy: 0.1914, precision: 0.1184, recall: 0.1914, f1: 0.0675, roc_auc: 0.5061\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 4 -> Loss: 2.2679\n",
      "(Train) accuracy: 0.1574, precision: 0.2671, recall: 0.1574, f1: 0.0540, roc_auc: 0.4901\n",
      "(Valid) accuracy: 0.1921, precision: 0.1141, recall: 0.1921, f1: 0.0677, roc_auc: 0.5065\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 5 -> Loss: 2.2649\n",
      "(Train) accuracy: 0.1575, precision: 0.2564, recall: 0.1575, f1: 0.0540, roc_auc: 0.4950\n",
      "(Valid) accuracy: 0.1914, precision: 0.1158, recall: 0.1914, f1: 0.0676, roc_auc: 0.5059\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 6 -> Loss: 2.2638\n",
      "(Train) accuracy: 0.1581, precision: 0.2770, recall: 0.1581, f1: 0.0553, roc_auc: 0.4833\n",
      "(Valid) accuracy: 0.1865, precision: 0.0969, recall: 0.1865, f1: 0.0640, roc_auc: 0.5132\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "EPOCH: 7 -> Loss: 2.2628\n",
      "(Train) accuracy: 0.1587, precision: 0.2521, recall: 0.1587, f1: 0.0571, roc_auc: 0.4884\n",
      "(Valid) accuracy: 0.1907, precision: 0.1169, recall: 0.1907, f1: 0.0674, roc_auc: 0.5185\n",
      "-----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 20\u001B[0m\n\u001B[1;32m     17\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss()\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1500\u001B[39m):\n\u001B[0;32m---> 20\u001B[0m     train_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m     valid_metrics \u001B[38;5;241m=\u001B[39m evaluate(model, val_loader, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_mask\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-----------------------------------------------------------------------------------------------------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[5], line 90\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, loader, optimizer, criterion)\u001B[0m\n\u001B[1;32m     80\u001B[0m     y_score\u001B[38;5;241m.\u001B[39mextend(F\u001B[38;5;241m.\u001B[39msoftmax(out[data\u001B[38;5;241m.\u001B[39mtrain_mask], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m     82\u001B[0m y_true, y_pred, y_score \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(y_true), np\u001B[38;5;241m.\u001B[39marray(y_pred), np\u001B[38;5;241m.\u001B[39marray(y_score)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: total_loss \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(loader),\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m: accuracy_score(y_true, y_pred),\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m\"\u001B[39m: precision_score(y_true, y_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweighted\u001B[39m\u001B[38;5;124m'\u001B[39m, zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m\"\u001B[39m: recall_score(y_true, y_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweighted\u001B[39m\u001B[38;5;124m'\u001B[39m, zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m\"\u001B[39m: f1_score(y_true, y_pred, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweighted\u001B[39m\u001B[38;5;124m'\u001B[39m, zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m),\n\u001B[0;32m---> 90\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mroc_auc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mroc_auc_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweighted\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43movo\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m }\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:634\u001B[0m, in \u001B[0;36mroc_auc_score\u001B[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001B[0m\n\u001B[1;32m    632\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m multi_class \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti_class must be in (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124movo\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124movr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 634\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_multiclass_roc_auc_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\n\u001B[1;32m    636\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    638\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y_true)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:765\u001B[0m, in \u001B[0;36m_multiclass_roc_auc_score\u001B[0;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001B[0m\n\u001B[1;32m    763\u001B[0m     y_true_encoded \u001B[38;5;241m=\u001B[39m _encode(y_true, uniques\u001B[38;5;241m=\u001B[39mclasses)\n\u001B[1;32m    764\u001B[0m     \u001B[38;5;66;03m# Hand & Till (2001) implementation (ovo)\u001B[39;00m\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_average_multiclass_ovo_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    766\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_binary_roc_auc_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_true_encoded\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\n\u001B[1;32m    767\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    768\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;66;03m# ovr is same as multi-label\u001B[39;00m\n\u001B[1;32m    770\u001B[0m     y_true_multilabel \u001B[38;5;241m=\u001B[39m label_binarize(y_true, classes\u001B[38;5;241m=\u001B[39mclasses)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_base.py:196\u001B[0m, in \u001B[0;36m_average_multiclass_ovo_score\u001B[0;34m(binary_metric, y_true, y_score, average)\u001B[0m\n\u001B[1;32m    193\u001B[0m     b_true \u001B[38;5;241m=\u001B[39m b_mask[ab_mask]\n\u001B[1;32m    195\u001B[0m     a_true_score \u001B[38;5;241m=\u001B[39m binary_metric(a_true, y_score[ab_mask, a])\n\u001B[0;32m--> 196\u001B[0m     b_true_score \u001B[38;5;241m=\u001B[39m \u001B[43mbinary_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m[\u001B[49m\u001B[43mab_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    197\u001B[0m     pair_scores[ix] \u001B[38;5;241m=\u001B[39m (a_true_score \u001B[38;5;241m+\u001B[39m b_true_score) \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m\n\u001B[1;32m    199\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39maverage(pair_scores, weights\u001B[38;5;241m=\u001B[39mprevalence)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:387\u001B[0m, in \u001B[0;36m_binary_roc_auc_score\u001B[0;34m(y_true, y_score, sample_weight, max_fpr)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39munique(y_true)) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    382\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    383\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOnly one class present in y_true. ROC AUC score \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    384\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis not defined in that case.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    385\u001B[0m     )\n\u001B[0;32m--> 387\u001B[0m fpr, tpr, _ \u001B[38;5;241m=\u001B[39m \u001B[43mroc_curve\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_fpr \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m max_fpr \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m auc(fpr, tpr)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[1;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:1108\u001B[0m, in \u001B[0;36mroc_curve\u001B[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001B[0m\n\u001B[1;32m   1006\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[1;32m   1007\u001B[0m     {\n\u001B[1;32m   1008\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1017\u001B[0m     y_true, y_score, \u001B[38;5;241m*\u001B[39m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, drop_intermediate\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1018\u001B[0m ):\n\u001B[1;32m   1019\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001B[39;00m\n\u001B[1;32m   1020\u001B[0m \n\u001B[1;32m   1021\u001B[0m \u001B[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;124;03m    array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\u001B[39;00m\n\u001B[1;32m   1107\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1108\u001B[0m     fps, tps, thresholds \u001B[38;5;241m=\u001B[39m \u001B[43m_binary_clf_curve\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1109\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[1;32m   1110\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1112\u001B[0m     \u001B[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001B[39;00m\n\u001B[1;32m   1113\u001B[0m     \u001B[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001B[39;00m\n\u001B[1;32m   1114\u001B[0m     \u001B[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1119\u001B[0m     \u001B[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001B[39;00m\n\u001B[1;32m   1120\u001B[0m     \u001B[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001B[39;00m\n\u001B[1;32m   1121\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m drop_intermediate \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(fps) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:840\u001B[0m, in \u001B[0;36m_binary_clf_curve\u001B[0;34m(y_true, y_score, pos_label, sample_weight)\u001B[0m\n\u001B[1;32m    837\u001B[0m y_true \u001B[38;5;241m=\u001B[39m y_true \u001B[38;5;241m==\u001B[39m pos_label\n\u001B[1;32m    839\u001B[0m \u001B[38;5;66;03m# sort scores and corresponding truth values\u001B[39;00m\n\u001B[0;32m--> 840\u001B[0m desc_score_indices \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margsort\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmergesort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    841\u001B[0m y_score \u001B[38;5;241m=\u001B[39m y_score[desc_score_indices]\n\u001B[1;32m    842\u001B[0m y_true \u001B[38;5;241m=\u001B[39m y_true[desc_score_indices]\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1133\u001B[0m, in \u001B[0;36margsort\u001B[0;34m(a, axis, kind, order)\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;129m@array_function_dispatch\u001B[39m(_argsort_dispatcher)\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21margsort\u001B[39m(a, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, kind\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, order\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;124;03m    Returns the indices that would sort an array.\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1131\u001B[0m \n\u001B[1;32m   1132\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1133\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_wrapfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43margsort\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[0;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbound\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001B[39;00m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001B[39;00m\n\u001B[1;32m     67\u001B[0m     \u001B[38;5;66;03m# exception has a traceback chain.\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Traditional Models",
   "id": "cde6ff44ab2a22ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T10:43:14.086654Z",
     "start_time": "2024-06-28T10:43:14.077482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pad_features_to_fixed_length(features, fixed_length=4716):\n",
    "    \"\"\"Pad feature vectors to a fixed length.\"\"\"\n",
    "    padding_length = fixed_length - len(features)\n",
    "    if padding_length > 0:\n",
    "        return np.concatenate([features, np.zeros(padding_length)])\n",
    "    else:\n",
    "        return features[:fixed_length]\n",
    "\n",
    "\n",
    "def extract_graph_features(graph, fixed_length=4716):\n",
    "    node_features = [graph.nodes[n]['features'] for n in graph.nodes]\n",
    "    max_length = max(len(f) for f in node_features)\n",
    "    padded_features = [pad_features_to_fixed_length(f, max_length) for f in node_features]\n",
    "    flat_features = np.array(padded_features).flatten()\n",
    "    flat_features = pad_features_to_fixed_length(flat_features, fixed_length)\n",
    "    feature_to_node_map = []\n",
    "    for node_id in range(len(graph.nodes)):\n",
    "        node_start_idx = node_id * max_length\n",
    "        node_end_idx = node_start_idx + max_length\n",
    "        feature_to_node_map.append((node_start_idx, node_end_idx, node_id))\n",
    "    return flat_features, feature_to_node_map\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, fixed_length=4716):\n",
    "    features = []\n",
    "    labels = []\n",
    "    feature_node_mappings = []\n",
    "    for data in dataset:\n",
    "        graph = nx.DiGraph()\n",
    "        for node_id in range(data.x.size(0)):\n",
    "            graph.add_node(node_id, features=data.x[node_id].numpy())\n",
    "        graph_features, feature_to_node_map = extract_graph_features(graph, fixed_length)\n",
    "        features.append(graph_features)\n",
    "        feature_node_mappings.append(feature_to_node_map)\n",
    "        if data.y.numel() == 1:\n",
    "            labels.append(data.y.item())\n",
    "        else:\n",
    "            labels.append(data.y[0].item())\n",
    "    return np.array(features), np.array(labels), feature_node_mappings\n",
    "\n",
    "\n",
    "def evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_score = clf.predict_proba(X_test) if hasattr(clf, \"predict_proba\") else clf.decision_function(X_test)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_score, average='weighted', multi_class='ovo') if hasattr(clf,\n",
    "                                                                                                    \"predict_proba\") else None\n",
    "    }\n"
   ],
   "id": "78484160a0bf493c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T12:56:28.755263Z",
     "start_time": "2024-06-28T10:43:38.939634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare dataset with a fixed length of 4716 features\n",
    "fixed_length = 4716\n",
    "features, labels, _ = prepare_dataset(dataset, fixed_length)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "classifiers = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "metrics = {\n",
    "    'RandomForest': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'SVM': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'DecisionTree': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'GaussianNB': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []},\n",
    "    'GradientBoosting': {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'roc_auc': []}\n",
    "}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "\n",
    "    print(f\"*** {name} ***\\n\")\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(features, labels)):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "        # Generate augmented training data\n",
    "        train_dataset = [dataset[i] for i in train_index]\n",
    "        augmented_train_dataset = generate_augmented_asts(train_dataset, num_augmentations=5)\n",
    "        X_train_augmented, y_train_augmented, _ = prepare_dataset(augmented_train_dataset, fixed_length)\n",
    "\n",
    "        eval_metrics = evaluate_classifier(clf, X_train_augmented, y_train_augmented, X_test, y_test)\n",
    "        for k, v in eval_metrics.items():\n",
    "            metrics[name][k].append(v)\n",
    "\n",
    "        print(f'Fold {fold + 1} - {\", \".join([\"{}: {:.4f}\".format(k, v) for k, v in eval_metrics.items()])}')\n",
    "\n",
    "    print(\"........................................................\\n\")\n",
    "\n",
    "print(\"--------------------------------------------------------\")\n",
    "\n",
    "for name in classifiers.keys():\n",
    "    avg_accuracy = np.mean(metrics[name]['accuracy'])\n",
    "    avg_precision = np.mean(metrics[name]['precision'])\n",
    "    avg_recall = np.mean(metrics[name]['recall'])\n",
    "    avg_f1 = np.mean(metrics[name]['f1'])\n",
    "    avg_roc_auc = np.mean([x for x in metrics[name]['roc_auc'] if x is not None])\n",
    "\n",
    "    print(f\"\\n{name} Average Metrics over 10 folds:\")\n",
    "    print(f\"Accuracy .... : {avg_accuracy:.4f}\")\n",
    "    print(f\"Precision ... : {avg_precision:.4f}\")\n",
    "    print(f\"Recall ...... : {avg_recall:.4f}\")\n",
    "    print(f\"F1 Score .... : {avg_f1:.4f}\")\n",
    "    print(f\"ROC-AUC ..... : {avg_roc_auc:.4f}\")\n"
   ],
   "id": "5427d6c8076a81b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** RandomForest ***\n",
      "\n",
      "Fold 1 - accuracy: 0.9216, precision: 0.9383, recall: 0.9216, f1: 0.9227, roc_auc: 0.9959\n",
      "Fold 2 - accuracy: 0.9216, precision: 0.9264, recall: 0.9216, f1: 0.9213, roc_auc: 0.9973\n",
      "Fold 3 - accuracy: 0.9118, precision: 0.9193, recall: 0.9118, f1: 0.9113, roc_auc: 0.9955\n",
      "Fold 4 - accuracy: 0.9510, precision: 0.9569, recall: 0.9510, f1: 0.9489, roc_auc: 0.9956\n",
      "Fold 5 - accuracy: 0.9706, precision: 0.9724, recall: 0.9706, f1: 0.9705, roc_auc: 0.9992\n",
      "Fold 6 - accuracy: 0.9314, precision: 0.9362, recall: 0.9314, f1: 0.9306, roc_auc: 0.9968\n",
      "Fold 7 - accuracy: 0.9314, precision: 0.9495, recall: 0.9314, f1: 0.9325, roc_auc: 0.9974\n",
      "Fold 8 - accuracy: 0.9412, precision: 0.9439, recall: 0.9412, f1: 0.9404, roc_auc: 0.9946\n",
      "Fold 9 - accuracy: 0.9118, precision: 0.9219, recall: 0.9118, f1: 0.9109, roc_auc: 0.9956\n",
      "Fold 10 - accuracy: 0.9216, precision: 0.9317, recall: 0.9216, f1: 0.9233, roc_auc: 0.9942\n",
      "........................................................\n",
      "\n",
      "*** SVM ***\n",
      "\n",
      "Fold 1 - accuracy: 0.8529, precision: 0.8698, recall: 0.8529, f1: 0.8506, roc_auc: 0.9905\n",
      "Fold 2 - accuracy: 0.9118, precision: 0.9203, recall: 0.9118, f1: 0.9105, roc_auc: 0.9942\n",
      "Fold 3 - accuracy: 0.8824, precision: 0.8898, recall: 0.8824, f1: 0.8817, roc_auc: 0.9887\n",
      "Fold 4 - accuracy: 0.9118, precision: 0.9271, recall: 0.9118, f1: 0.9114, roc_auc: 0.9921\n",
      "Fold 5 - accuracy: 0.9020, precision: 0.9113, recall: 0.9020, f1: 0.9024, roc_auc: 0.9969\n",
      "Fold 6 - accuracy: 0.8725, precision: 0.8841, recall: 0.8725, f1: 0.8728, roc_auc: 0.9905\n",
      "Fold 7 - accuracy: 0.8725, precision: 0.9051, recall: 0.8725, f1: 0.8792, roc_auc: 0.9836\n",
      "Fold 8 - accuracy: 0.8824, precision: 0.8947, recall: 0.8824, f1: 0.8826, roc_auc: 0.9942\n",
      "Fold 9 - accuracy: 0.8922, precision: 0.9027, recall: 0.8922, f1: 0.8908, roc_auc: 0.9903\n",
      "Fold 10 - accuracy: 0.8922, precision: 0.9053, recall: 0.8922, f1: 0.8954, roc_auc: 0.9928\n",
      "........................................................\n",
      "\n",
      "*** DecisionTree ***\n",
      "\n",
      "Fold 1 - accuracy: 0.8824, precision: 0.8865, recall: 0.8824, f1: 0.8821, roc_auc: 0.9340\n",
      "Fold 2 - accuracy: 0.8824, precision: 0.8924, recall: 0.8824, f1: 0.8831, roc_auc: 0.9340\n",
      "Fold 3 - accuracy: 0.8725, precision: 0.8750, recall: 0.8725, f1: 0.8724, roc_auc: 0.9285\n",
      "Fold 4 - accuracy: 0.9020, precision: 0.9086, recall: 0.9020, f1: 0.9007, roc_auc: 0.9450\n",
      "Fold 5 - accuracy: 0.9216, precision: 0.9258, recall: 0.9216, f1: 0.9213, roc_auc: 0.9564\n",
      "Fold 6 - accuracy: 0.8824, precision: 0.8919, recall: 0.8824, f1: 0.8830, roc_auc: 0.9345\n",
      "Fold 7 - accuracy: 0.8725, precision: 0.8796, recall: 0.8725, f1: 0.8718, roc_auc: 0.9295\n",
      "Fold 8 - accuracy: 0.9118, precision: 0.9198, recall: 0.9118, f1: 0.9113, roc_auc: 0.9504\n",
      "Fold 9 - accuracy: 0.8922, precision: 0.9026, recall: 0.8922, f1: 0.8909, roc_auc: 0.9394\n",
      "Fold 10 - accuracy: 0.8431, precision: 0.8447, recall: 0.8431, f1: 0.8413, roc_auc: 0.9119\n",
      "........................................................\n",
      "\n",
      "*** GaussianNB ***\n",
      "\n",
      "Fold 1 - accuracy: 0.6667, precision: 0.7795, recall: 0.6667, f1: 0.6754, roc_auc: 0.8138\n",
      "Fold 2 - accuracy: 0.7353, precision: 0.8290, recall: 0.7353, f1: 0.7522, roc_auc: 0.8525\n",
      "Fold 3 - accuracy: 0.6863, precision: 0.7689, recall: 0.6863, f1: 0.6933, roc_auc: 0.8239\n",
      "Fold 4 - accuracy: 0.8039, precision: 0.8543, recall: 0.8039, f1: 0.8115, roc_auc: 0.8920\n",
      "Fold 5 - accuracy: 0.7843, precision: 0.8398, recall: 0.7843, f1: 0.7949, roc_auc: 0.8842\n",
      "Fold 6 - accuracy: 0.7843, precision: 0.8345, recall: 0.7843, f1: 0.7927, roc_auc: 0.8804\n",
      "Fold 7 - accuracy: 0.7353, precision: 0.8168, recall: 0.7353, f1: 0.7490, roc_auc: 0.8519\n",
      "Fold 8 - accuracy: 0.7647, precision: 0.8201, recall: 0.7647, f1: 0.7704, roc_auc: 0.8679\n",
      "Fold 9 - accuracy: 0.8235, precision: 0.8546, recall: 0.8235, f1: 0.8286, roc_auc: 0.9014\n",
      "Fold 10 - accuracy: 0.7941, precision: 0.8234, recall: 0.7941, f1: 0.7945, roc_auc: 0.8844\n",
      "........................................................\n",
      "\n",
      "*** GradientBoosting ***\n",
      "\n",
      "Fold 1 - accuracy: 0.8824, precision: 0.8899, recall: 0.8824, f1: 0.8777, roc_auc: 0.9800\n",
      "Fold 2 - accuracy: 0.9118, precision: 0.9142, recall: 0.9118, f1: 0.9117, roc_auc: 0.9942\n",
      "Fold 3 - accuracy: 0.9216, precision: 0.9300, recall: 0.9216, f1: 0.9227, roc_auc: 0.9869\n",
      "Fold 4 - accuracy: 0.9510, precision: 0.9584, recall: 0.9510, f1: 0.9499, roc_auc: 0.9955\n",
      "Fold 5 - accuracy: 0.9412, precision: 0.9442, recall: 0.9412, f1: 0.9410, roc_auc: 0.9957\n",
      "Fold 6 - accuracy: 0.9216, precision: 0.9314, recall: 0.9216, f1: 0.9236, roc_auc: 0.9859\n",
      "Fold 7 - accuracy: 0.9216, precision: 0.9337, recall: 0.9216, f1: 0.9233, roc_auc: 0.9863\n",
      "Fold 8 - accuracy: 0.9314, precision: 0.9399, recall: 0.9314, f1: 0.9311, roc_auc: 0.9950\n",
      "Fold 9 - accuracy: 0.8824, precision: 0.8903, recall: 0.8824, f1: 0.8816, roc_auc: 0.9912\n",
      "Fold 10 - accuracy: 0.9314, precision: 0.9351, recall: 0.9314, f1: 0.9311, roc_auc: 0.9959\n",
      "........................................................\n",
      "\n",
      "--------------------------------------------------------\n",
      "\n",
      "RandomForest Average Metrics over 10 folds:\n",
      "Accuracy .... : 0.9314\n",
      "Precision ... : 0.9396\n",
      "Recall ...... : 0.9314\n",
      "F1 Score .... : 0.9313\n",
      "ROC-AUC ..... : 0.9962\n",
      "\n",
      "SVM Average Metrics over 10 folds:\n",
      "Accuracy .... : 0.8873\n",
      "Precision ... : 0.9010\n",
      "Recall ...... : 0.8873\n",
      "F1 Score .... : 0.8877\n",
      "ROC-AUC ..... : 0.9914\n",
      "\n",
      "DecisionTree Average Metrics over 10 folds:\n",
      "Accuracy .... : 0.8863\n",
      "Precision ... : 0.8927\n",
      "Recall ...... : 0.8863\n",
      "F1 Score .... : 0.8858\n",
      "ROC-AUC ..... : 0.9364\n",
      "\n",
      "GaussianNB Average Metrics over 10 folds:\n",
      "Accuracy .... : 0.7578\n",
      "Precision ... : 0.8221\n",
      "Recall ...... : 0.7578\n",
      "F1 Score .... : 0.7662\n",
      "ROC-AUC ..... : 0.8653\n",
      "\n",
      "GradientBoosting Average Metrics over 10 folds:\n",
      "Accuracy .... : 0.9196\n",
      "Precision ... : 0.9267\n",
      "Recall ...... : 0.9196\n",
      "F1 Score .... : 0.9194\n",
      "ROC-AUC ..... : 0.9907\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explainability",
   "id": "5bc99c15c92610bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T10:17:47.837671Z",
     "start_time": "2024-06-27T10:17:47.784581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "global_node_id = 0\n",
    "\n",
    "\n",
    "def reset_global_node_id():\n",
    "    global global_node_id\n",
    "    global_node_id = 0\n",
    "\n",
    "\n",
    "def assign_unique_id(node):\n",
    "    global global_node_id\n",
    "    node['_id'] = global_node_id\n",
    "    global_node_id += 1\n",
    "\n",
    "\n",
    "def traverse_and_assign_ids(node):\n",
    "    if isinstance(node, dict):\n",
    "        assign_unique_id(node)\n",
    "        for key, value in node.items():\n",
    "            traverse_and_assign_ids(value)\n",
    "    elif isinstance(node, list):\n",
    "        for item in node:\n",
    "            traverse_and_assign_ids(item)\n",
    "\n",
    "\n",
    "def load_data(ast_directory, label_map):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(ast_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    ast = json.load(f)\n",
    "                reset_global_node_id()\n",
    "                traverse_and_assign_ids(ast)\n",
    "                dataset.append(ast)\n",
    "                label_folder = root.split(os.sep)[-1]\n",
    "                label = label_map[label_folder]\n",
    "                labels.append(label)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "def hash_feature(value, num_bins=1000):\n",
    "    return int(hashlib.md5(str(value).encode()).hexdigest(), 16) % num_bins\n",
    "\n",
    "\n",
    "def extract_features(node):\n",
    "    node_type = node.get('nodeType', 'Unknown')\n",
    "    type_feature = [hash_feature(node_type)]\n",
    "    name_feature = [hash_feature(node.get('name', ''))] if 'name' in node else [0]\n",
    "    value_feature = [hash_feature(node.get('value', ''))] if 'value' in node else [0]\n",
    "    src_feature = [0, 0]\n",
    "    if 'src' in node:\n",
    "        start, length, *_ = map(int, node['src'].split(':'))\n",
    "        src_feature = [start, length]\n",
    "    type_desc_features = []\n",
    "    if 'typeDescriptions' in node:\n",
    "        type_desc = node['typeDescriptions']\n",
    "        type_desc_features.append(hash_feature(type_desc.get('typeString', '')))\n",
    "        type_desc_features.append(hash_feature(type_desc.get('typeIdentifier', '')))\n",
    "    state_mutability_feature = [hash_feature(node.get('stateMutability', ''))] if 'stateMutability' in node else [0]\n",
    "    visibility_feature = [hash_feature(node.get('visibility', ''))] if 'visibility' in node else [0]\n",
    "    features = type_feature + name_feature + value_feature + src_feature + type_desc_features + state_mutability_feature + visibility_feature\n",
    "    return features\n",
    "\n",
    "\n",
    "def pad_features_to_fixed_length(features, fixed_length=4716):\n",
    "    padding_length = fixed_length - len(features)\n",
    "    if padding_length > 0:\n",
    "        return np.concatenate([features, np.zeros(padding_length)])\n",
    "    else:\n",
    "        return features[:fixed_length]\n",
    "\n",
    "\n",
    "def extract_graph_features_with_mapping(ast, fixed_length=4716):\n",
    "    graph = nx.DiGraph()\n",
    "    node_features = []\n",
    "    feature_to_node_map = []\n",
    "\n",
    "    for node in ast:\n",
    "        if isinstance(node, dict) and '_id' in node:\n",
    "            node_id = node['_id']\n",
    "            features = extract_features(node)\n",
    "            graph.add_node(node_id, features=features)\n",
    "            node_features.append(features)\n",
    "            feature_to_node_map.append((len(node_features) - 1, len(node_features), node_id))\n",
    "\n",
    "    max_length = max(len(f) for f in node_features)\n",
    "    padded_features = [pad_features_to_fixed_length(f, max_length) for f in node_features]\n",
    "    flat_features = np.array(padded_features).flatten()\n",
    "    flat_features = pad_features_to_fixed_length(flat_features, fixed_length)\n",
    "\n",
    "    return flat_features, feature_to_node_map\n",
    "\n",
    "\n",
    "def prepare_dataset_with_mapping(dataset, fixed_length=4716):\n",
    "    features = []\n",
    "    labels = []\n",
    "    feature_node_mappings = []\n",
    "\n",
    "    for ast in dataset:\n",
    "        graph_features, feature_to_node_map = extract_graph_features_with_mapping(ast, fixed_length)\n",
    "        features.append(graph_features)\n",
    "        feature_node_mappings.append(feature_to_node_map)\n",
    "        labels.append(ast[0]['y'])\n",
    "\n",
    "    return np.array(features), np.array(labels), feature_node_mappings\n",
    "\n",
    "\n",
    "def get_important_nodes(feature_importances, feature_to_node_map, num_top_features=10):\n",
    "    important_features_indices = np.argsort(feature_importances)[-num_top_features:]\n",
    "    important_nodes = set()\n",
    "    for feature_idx in important_features_indices:\n",
    "        for start_idx, end_idx, node_id in feature_to_node_map:\n",
    "            if start_idx <= feature_idx < end_idx:\n",
    "                important_nodes.add(node_id)\n",
    "                break\n",
    "    return important_nodes\n",
    "\n",
    "\n",
    "def extract_lines_of_code_from_nodes(ast, important_nodes):\n",
    "    lines_of_code = set()\n",
    "\n",
    "    def traverse_ast(node):\n",
    "        if isinstance(node, dict):\n",
    "            node_id = node['_id']\n",
    "            if node_id in important_nodes:\n",
    "                if 'src' in node:\n",
    "                    start, length, *_ = map(int, node['src'].split(':'))\n",
    "                    lines_of_code.add((start, length))\n",
    "            for key, value in node.items():\n",
    "                traverse_ast(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                traverse_ast(item)\n",
    "\n",
    "    traverse_ast(ast)\n",
    "    return lines_of_code\n",
    "\n",
    "\n",
    "def highlight_important_lines(ast, feature_importances, feature_to_node_map, num_top_features=10):\n",
    "    important_nodes = get_important_nodes(feature_importances, feature_to_node_map, num_top_features)\n",
    "    important_lines = extract_lines_of_code_from_nodes(ast, important_nodes)\n",
    "    return important_lines\n",
    "\n",
    "\n",
    "def get_feature_importance(clf):\n",
    "    if hasattr(clf, 'feature_importances_'):\n",
    "        return clf.feature_importances_\n",
    "    else:\n",
    "        raise ValueError(f\"Model of type {type(clf)} does not support feature importances.\")\n"
   ],
   "id": "d1ec035bdae8c8b0",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T10:17:50.956511Z",
     "start_time": "2024-06-27T10:17:48.895031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    ast_directory = '../dataset/aisc/ast'\n",
    "    label_map = generate_label_map(ast_directory)\n",
    "    dataset, labels = load_data(ast_directory, label_map)\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        print(\"No data loaded. Please check the dataset directory and files.\")\n",
    "        return\n",
    "\n",
    "    # Prepare dataset with a fixed length of 4716 features and get feature-to-node mappings\n",
    "    fixed_length = 4716\n",
    "    features, labels, feature_node_mappings = prepare_dataset_with_mapping(dataset, fixed_length)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    clf.fit(features, labels)\n",
    "\n",
    "    feature_importances = get_feature_importance(clf)\n",
    "\n",
    "    important_lines_per_ast = []\n",
    "    for i, ast in enumerate(dataset):\n",
    "        important_lines = highlight_important_lines(ast, feature_importances, feature_node_mappings[i])\n",
    "        important_lines_per_ast.append(important_lines)\n",
    "\n",
    "    for i, important_lines in enumerate(important_lines_per_ast):\n",
    "        print(f\"AST {i} Important lines: {important_lines}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "1c70618f414538e5",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 29\u001B[0m\n\u001B[1;32m     25\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAST \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m Important lines: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimportant_lines\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 29\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 12\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Prepare dataset with a fixed length of 4716 features and get feature-to-node mappings\u001B[39;00m\n\u001B[1;32m     11\u001B[0m fixed_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4716\u001B[39m\n\u001B[0;32m---> 12\u001B[0m features, labels, feature_node_mappings \u001B[38;5;241m=\u001B[39m \u001B[43mprepare_dataset_with_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m clf \u001B[38;5;241m=\u001B[39m RandomForestClassifier(n_estimators\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m     15\u001B[0m clf\u001B[38;5;241m.\u001B[39mfit(features, labels)\n",
      "Cell \u001B[0;32mIn[21], line 108\u001B[0m, in \u001B[0;36mprepare_dataset_with_mapping\u001B[0;34m(dataset, fixed_length)\u001B[0m\n\u001B[1;32m    105\u001B[0m feature_node_mappings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ast \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[0;32m--> 108\u001B[0m     graph_features, feature_to_node_map \u001B[38;5;241m=\u001B[39m \u001B[43mextract_graph_features_with_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mast\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfixed_length\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    109\u001B[0m     features\u001B[38;5;241m.\u001B[39mappend(graph_features)\n\u001B[1;32m    110\u001B[0m     feature_node_mappings\u001B[38;5;241m.\u001B[39mappend(feature_to_node_map)\n",
      "Cell \u001B[0;32mIn[21], line 94\u001B[0m, in \u001B[0;36mextract_graph_features_with_mapping\u001B[0;34m(ast, fixed_length)\u001B[0m\n\u001B[1;32m     91\u001B[0m         node_features\u001B[38;5;241m.\u001B[39mappend(features)\n\u001B[1;32m     92\u001B[0m         feature_to_node_map\u001B[38;5;241m.\u001B[39mappend((\u001B[38;5;28mlen\u001B[39m(node_features) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(node_features), node_id))\n\u001B[0;32m---> 94\u001B[0m max_length \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnode_features\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     95\u001B[0m padded_features \u001B[38;5;241m=\u001B[39m [pad_features_to_fixed_length(f, max_length) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m node_features]\n\u001B[1;32m     96\u001B[0m flat_features \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(padded_features)\u001B[38;5;241m.\u001B[39mflatten()\n",
      "\u001B[0;31mValueError\u001B[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bdef5539d0664143"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
