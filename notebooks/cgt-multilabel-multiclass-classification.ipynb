{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "172880abc3d187d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-18T12:40:04.267512Z",
     "start_time": "2024-07-18T12:39:51.570052Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc1ec8b396d0a08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T12:41:22.850107Z",
     "start_time": "2024-07-18T12:41:22.831468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the device for torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device {DEVICE}\")\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(f\"Random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# Path to dataset\n",
    "PATH_TO_DATASET = os.path.join(\"..\", \"dataset\", \"cgt\")\n",
    "print(f\"Path to dataset: {PATH_TO_DATASET}\")\n",
    "\n",
    "# Model and training configurations\n",
    "BERT_MODEL_TYPE = 'microsoft/codebert-base'\n",
    "print(f\"BERT model type: {BERT_MODEL_TYPE}\")\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "print(f\"Max features: {MAX_FEATURES}\")\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "print(f\"Number of folds: {NUM_FOLDS}\")\n",
    "\n",
    "NUM_EPOCHS = 15\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "NUM_LABELS = 20  # 20 for CodeSmell, 160 overall\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "LR = 0.001\n",
    "print(f\"Learning rate: {LR}\")\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "print(f\"Test size: {TEST_SIZE}\")\n",
    "\n",
    "# File configurations\n",
    "FILE_TYPE = \"runtime\"  # Can be 'source', 'runtime', 'bytecode'\n",
    "FILE_EXT = None\n",
    "if FILE_TYPE == \"sol\":\n",
    "    FILE_EXT = \"sol\"\n",
    "elif FILE_TYPE == \"runtime\":\n",
    "    FILE_EXT = \".rt.hex\"\n",
    "elif FILE_TYPE == \"bytecode\":\n",
    "    FILE_EXT = \".rt\"\n",
    "FILE_ID = FILE_TYPE  # Can be 'sol2' for 'source'\n",
    "\n",
    "print(f\"File type: {FILE_TYPE}\")\n",
    "print(f\"File extension: {FILE_EXT}\")\n",
    "print(f\"File ID: {FILE_ID}\")\n",
    "\n",
    "# Creating the log directory if it doesn't exist\n",
    "LOG_DIR = os.path.join(\"log\", FILE_TYPE)\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "    print(f\"Log directory created at {LOG_DIR}\")\n",
    "else:\n",
    "    print(f\"Log directory already exists at {LOG_DIR}\")"
   ],
   "id": "1374f63db7c6cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cpu\n",
      "Random seeds set to 0\n",
      "Path to dataset: ../dataset/cgt\n",
      "BERT model type: microsoft/codebert-base\n",
      "Max features: 500\n",
      "Batch size: 8\n",
      "Number of folds: 10\n",
      "Number of epochs: 15\n",
      "Number of labels: 20\n",
      "Learning rate: 0.001\n",
      "Test size: 0.1\n",
      "File type: runtime\n",
      "File extension: .rt.hex\n",
      "File ID: runtime\n",
      "Log directory already exists at log/runtime\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "a424b60aff524ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "24ae2bd4455509d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T12:40:10.696870Z",
     "start_time": "2024-07-18T12:40:10.690937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_hex(hex_data: str) -> str:\n",
    "    # Reads a hex file and converts it to a byte string\n",
    "    byte_data = bytes.fromhex(hex_data.strip())\n",
    "\n",
    "    # Convert byte data to a readable ASCII string, ignoring non-ASCII characters\n",
    "    return ' '.join(f'{byte:02x}' for byte in byte_data)\n",
    "\n",
    "\n",
    "def preprocess_solidity_code(code: str) -> str:\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove blank lines (lines only containing whitespace)\n",
    "    lines = code.split('\\n')\n",
    "    non_blank_lines = [line for line in lines if line.strip() != '']\n",
    "    code = '\\n'.join(non_blank_lines)\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def preprocess(data: str) -> str:\n",
    "    return preprocess_solidity_code(data) if FILE_TYPE == \"source\" else preprocess_hex(data)"
   ],
   "id": "41e2ed4561254c43",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labels Management",
   "id": "7e084d9053915673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T12:40:11.575543Z",
     "start_time": "2024-07-18T12:40:11.569167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_inputs_and_gt(data: pd.DataFrame) -> Tuple:\n",
    "    \"\"\"\n",
    "    Initialize inputs, labels, and groundtruth (gt) from the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :return: A tuple containing the list of inputs, labels dictionary, and gt dictionary.\n",
    "    \"\"\"\n",
    "    inputs, labels, gt = {}, {}, {}\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Initializing inputs and groundtruth data\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Initialize the documents\n",
    "            inputs[item_id] = preprocess(open(path_to_file, 'r', encoding=\"utf8\").read())\n",
    "\n",
    "            # Initialize the label\n",
    "            labels[item_id] = [0] * NUM_LABELS\n",
    "\n",
    "            # Initialize the groundtruth\n",
    "            prop = row[\"property\"].lower()\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "\n",
    "    return list(inputs.values()), labels, gt\n",
    "\n",
    "\n",
    "def set_labels(data: pd.DataFrame, labels: Dict, gt: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Set up the labels based on the groundtruth (gt) for the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :param labels: A dictionary where keys are item IDs and values are lists representing labels.\n",
    "    :param gt: A dictionary where keys are properties and values are their corresponding indices.\n",
    "    :return: A list of labels values.\n",
    "    \"\"\"\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Setting up the labels\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Set label   \n",
    "            prop = row[\"property\"].lower()\n",
    "            if row['property_holds'] == 't':\n",
    "                labels[item_id][gt[prop]] = 1\n",
    "\n",
    "    return list(labels.values())\n"
   ],
   "id": "1511840b0951d215",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization of the dataset",
   "id": "bac1ca9644eca1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T12:41:08.842994Z",
     "start_time": "2024-07-18T12:40:54.821129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the dataset from CSV\n",
    "dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "\n",
    "# Count the frequency of each item in the \"property\" column\n",
    "print(\"Counting frequency of each item in the 'property' column...\")\n",
    "property_frequency = dataset[\"property\"].value_counts()\n",
    "print(\"Frequency of each item in the 'property' column:\")\n",
    "print(property_frequency)\n",
    "\n",
    "# Print the number of unique values in the \"property\" column\n",
    "unique_properties_count = dataset[\"property\"].nunique()\n",
    "print(f\"\\nThe number of unique values in the 'property' column is {unique_properties_count}.\")\n",
    "\n",
    "# Count the frequency of each item in the \"dataset\" column\n",
    "print(\"\\nCounting frequency of each item in the 'dataset' column...\")\n",
    "dataset_frequency = dataset[\"dataset\"].value_counts()\n",
    "print(\"Frequency of each item in the 'dataset' column:\")\n",
    "print(dataset_frequency)\n",
    "\n",
    "# Find the item with the maximum occurrence in the \"dataset\" column\n",
    "most_frequent_item = dataset_frequency.idxmax()\n",
    "most_frequent_count = dataset_frequency.max()\n",
    "print(f\"\\nThe most frequent dataset is '{most_frequent_item}' and it appears {most_frequent_count} times.\")\n",
    "\n",
    "# Exclude outliers from the dataset\n",
    "dataset = dataset[dataset[\"dataset\"] == most_frequent_item]\n",
    "\n",
    "# Initialize the documents and the groundtruth\n",
    "print(\"\\nInitializing the documents and the ground truth...\")\n",
    "INPUTS, LABELS, gt = init_inputs_and_gt(dataset)\n",
    "\n",
    "# Set the labels for the multilabel classification problem\n",
    "print(\"Setting the labels for the multilabel classification problem...\")\n",
    "LABELS = set_labels(dataset, LABELS, gt)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "print(\"Initializing the TF-IDF vectorizer...\")\n",
    "VECTORIZER = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "print(\"Initialization complete!\")"
   ],
   "id": "fb254a69e43e824e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting frequency of each item in the 'property' column...\n",
      "Frequency of each item in the 'property' column:\n",
      "property\n",
      "Reentrancy                       1531\n",
      "Tx_State_Dep                     1064\n",
      "Unchkd_send                      1058\n",
      "Blk_State_Dep                    1057\n",
      "Failed_send                      1057\n",
      "                                 ... \n",
      "short_addresses                     1\n",
      "Honeypot Uninitialised struct       1\n",
      "Honeypot Balance disorder           1\n",
      "Honeypot Straw man contract         1\n",
      "Honeypot Type overflow              1\n",
      "Name: count, Length: 160, dtype: int64\n",
      "\n",
      "The number of unique values in the 'property' column is 160.\n",
      "\n",
      "Counting frequency of each item in the 'dataset' column...\n",
      "Frequency of each item in the 'dataset' column:\n",
      "dataset\n",
      "CodeSmells        10395\n",
      "Zeus               7315\n",
      "eThor               702\n",
      "ContractFuzzer      367\n",
      "SolidiFI            343\n",
      "EverEvolvingG       292\n",
      "Doublade            276\n",
      "NPChecker           212\n",
      "JiuZhou             165\n",
      "SBcurated           129\n",
      "SWCregistry         116\n",
      "EthRacer            109\n",
      "NotSoSmartC          34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The most frequent dataset is 'CodeSmells' and it appears 10395 times.\n",
      "\n",
      "Initializing the documents and the ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing inputs and groundtruth data: 10395it [00:13, 770.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the labels for the multilabel classification problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up the labels: 10395it [00:00, 29463.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the TF-IDF vectorizer...\n",
      "Initialization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross Validation",
   "id": "546491e26d424c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T12:40:39.929924Z",
     "start_time": "2024-07-18T12:40:39.921986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels: List[Any], pred_labels: List[Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the given true and predicted labels.\n",
    "\n",
    "    :param true_labels: The ground truth labels.\n",
    "    :param pred_labels: The predicted labels.\n",
    "    :return: A dictionary containing precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the results to a CSV file.\n",
    "\n",
    "    :param results: The results to save, typically a list of dictionaries.\n",
    "    :param filename: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(LOG_DIR, filename), index=False)\n",
    "    print(f\"All fold results saved to '{LOG_DIR}'/'{filename}'\")\n"
   ],
   "id": "795c276e2fb6e8b1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:28:34.698405Z",
     "start_time": "2024-07-18T10:28:34.671533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for handling the training and evaluation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model, loss criterion, and optimizer.\n",
    "\n",
    "        :param model: The neural network model to be trained.\n",
    "        \"\"\"\n",
    "        self._untrained_model = model.to(DEVICE)\n",
    "        self._model = model.to(DEVICE)\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "        self._optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    def reset_model(self):\n",
    "        self._model = self._untrained_model\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch elements to the appropriate device (CPU/GPU)\n",
    "        batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "        # Prepare the inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(labels.cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def run_epoch(self, dataloader: DataLoader, train_mode: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single epoch of training or evaluation.\n",
    "\n",
    "        :param dataloader: DataLoader providing the data for the epoch.\n",
    "        :param train_mode: Boolean flag indicating whether to train or evaluate.\n",
    "        :return: A tuple containing the average loss and a dictionary of average metrics.\n",
    "        \"\"\"\n",
    "        # Set the mode for the epoch (Training or Testing)\n",
    "        phase = 'Training' if train_mode else 'Testing'\n",
    "        self._model.train() if train_mode else self._model.eval()\n",
    "\n",
    "        losses, metrics_list = [], []\n",
    "\n",
    "        # Iterate over the data loader\n",
    "        for batch in tqdm(dataloader, desc=phase):\n",
    "            # Move batch elements to the appropriate device\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            loss, batch_metrics = self._train_batch(batch) if train_mode else self._evaluate_batch(batch)\n",
    "\n",
    "            # Accumulate the loss and metrics\n",
    "            losses.append(loss)\n",
    "            metrics_list.append(batch_metrics)\n",
    "\n",
    "        # Compute average loss and metrics for the epoch\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "\n",
    "        return avg_loss, avg_metrics\n"
   ],
   "id": "7753f6da9628de81",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T10:28:35.030617Z",
     "start_time": "2024-07-18T10:28:35.010829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    CrossValidator class for handling k-fold cross-validation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer: Trainer, train_data: TensorDataset, test_data: TensorDataset):\n",
    "        \"\"\"\n",
    "        Initialize the CrossValidator with trainer, training data, and test data.\n",
    "\n",
    "        :param trainer: An instance of the Trainer class.\n",
    "        :param train_data: The training dataset.\n",
    "        :param test_data: The test dataset.\n",
    "        \"\"\"\n",
    "        self.__trainer = trainer\n",
    "        self.__train_data = train_data\n",
    "        self.__test_data = test_data\n",
    "\n",
    "    def __train_and_evaluate(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a specified number of epochs.\n",
    "\n",
    "        :param train_dataloader: DataLoader for the training data.\n",
    "        :param test_dataloader: DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n --- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "            # Train the model and print training metrics\n",
    "            avg_train_loss, avg_train_metrics = self.__trainer.run_epoch(train_dataloader, train_mode=True)\n",
    "            print(f\"\\n TRAIN | Loss: {avg_train_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_train_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_train_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_train_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "            # Evaluate the model on the validation set and print validation metrics\n",
    "            avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "            print(f\" VALID | Loss: {avg_test_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "    def __evaluate_on_test_set(self, test_dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "\n",
    "        :param test_dataloader: DataLoader for the test data.\n",
    "        :return: A dictionary of test set metrics.\n",
    "        \"\"\"\n",
    "        avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "\n",
    "        # Print test set metrics\n",
    "        print(f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} |\"\n",
    "              f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "              f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "              f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "        return avg_test_metrics\n",
    "\n",
    "    def k_fold_cv(self, log_id: str = \"bert\") -> None:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        :param log_id: Identifier for logging purposes, typically the model name.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Iterate over each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.__train_data)):\n",
    "            # Create data loaders for training and validation sets\n",
    "            train_subsampler = Subset(self.__train_data, train_idx)\n",
    "            val_subsampler = Subset(self.__train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_subsampler,\n",
    "                sampler=RandomSampler(train_subsampler),\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subsampler,\n",
    "                batch_size=BATCH_SIZE  # No need for shuffling\n",
    "            )\n",
    "\n",
    "            print(f\"Starting Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "            # Train and evaluate the model for the current fold\n",
    "            self.__train_and_evaluate(train_loader, val_loader)\n",
    "\n",
    "            # Evaluate on the test set after each fold\n",
    "            metrics = self.__evaluate_on_test_set(DataLoader(self.__test_data, batch_size=BATCH_SIZE, shuffle=False))\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "            # Reset the model to untrained\n",
    "            self.__trainer.reset_model()\n",
    "\n",
    "        # Calculate average and standard deviation of each metric across all folds\n",
    "        metric_keys = fold_metrics[0].keys()  # Assuming all metrics dictionaries have the same structure\n",
    "        average_metrics = {key: np.mean([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "        std_dev_metrics = {key: np.std([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "\n",
    "        # Print average metrics and their standard deviations\n",
    "        print(\"Average Metrics Over All Folds:\")\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f\"{key}: {value:.4f} (±{std_dev_metrics[key]:.4f})\")\n",
    "\n",
    "        # Save metrics to CSV file\n",
    "        save_results(fold_metrics, filename=f\"{log_id}.csv\")\n"
   ],
   "id": "543bd399383e5b7e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "9ae8b68dbe64b7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT",
   "id": "f2934d41ac3e7b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.064229Z",
     "start_time": "2024-07-16T08:53:14.057957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERTModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BERTModelTrainer class for handling the training and evaluation of a BERT-based model.\n",
    "    Inherits from the Trainer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BERTModelTrainer with model, optimizer, and loss function.\n",
    "\n",
    "        :param model: The BERT model to be trained.\n",
    "        \"\"\"\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Initialize the optimizer with model parameters and a learning rate\n",
    "        self._optimizer = AdamW(self._model.parameters(), lr=LR)\n",
    "\n",
    "        # Define the loss function for binary classification with logits\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare the inputs for the model\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(**inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs.logits).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(batch[2].detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n"
   ],
   "id": "c2c756331f70649e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:44.296480Z",
     "start_time": "2024-07-16T08:53:14.065140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(BERT_MODEL_TYPE, num_labels=20, ignore_mismatched_sizes=True)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "model.to(DEVICE)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_TYPE, ignore_mismatched_sizes=True)\n",
    "\n",
    "x, y = tokenizer(\n",
    "    INPUTS,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "), LABELS\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x['input_ids'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Split attention masks for training and test sets\n",
    "train_masks, test_masks, _, _ = train_test_split(x['attention_mask'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_data = TensorDataset(x_train, train_masks, torch.tensor(y_train).float())\n",
    "test_data = TensorDataset(x_test, test_masks, torch.tensor(y_test).float())\n",
    "CrossValidator(BERTModelTrainer(model), train_data, test_data).k_fold_cv(log_id=\"bert\")"
   ],
   "id": "cfb9656b31564534",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.7066 | Precision: 0.1548, Recall: 0.5000, F1: 0.2361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.4882 | Precision: 1.0000, Recall: 0.6750, F1: 0.8036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.5146 | Precision: 0.8333, Recall: 0.5500, F1: 0.6607\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.5039 | Precision: 1.0000, Recall: 0.6750, F1: 0.8036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.3668 | Precision: 0.6667, Recall: 0.5833, F1: 0.6190\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.3557 | Precision: 0.8333, Recall: 0.5500, F1: 0.6607\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.8333 (±0.0000)\n",
      "recall: 0.5500 (±0.0000)\n",
      "f1: 0.6607 (±0.0000)\n",
      "All fold results saved to 'bert.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FFNN",
   "id": "523fb217e9d16400"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:44.312058Z",
     "start_time": "2024-07-16T08:53:44.301400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Neural Network with three fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network layers.\n",
    "        \"\"\"\n",
    "        super(FFNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(MAX_FEATURES, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, NUM_LABELS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :return: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ],
   "id": "c35b14720c73a4bc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:46.024303Z",
     "start_time": "2024-07-16T08:53:44.315238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FFNNClassifier()\n",
    "\n",
    "x = torch.FloatTensor(VECTORIZER.fit_transform(INPUTS).toarray())\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"ffnn\")"
   ],
   "id": "785da51a7681dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8886 | Precision: 0.1750, Recall: 1.0000, F1: 0.2971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8883 | Precision: 0.1750, Recall: 1.0000, F1: 0.2971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8629 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8999 | Precision: 0.1500, Recall: 1.0000, F1: 0.2609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8615 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8616 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.2250 (±0.0000)\n",
      "recall: 1.0000 (±0.0000)\n",
      "f1: 0.3667 (±0.0000)\n",
      "All fold results saved to 'ffnn.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "212af9b00ac25811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:46.036512Z",
     "start_time": "2024-07-16T08:53:46.026655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier for text classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pretrained_embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM Classifier.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of the embedding vectors.\n",
    "        :param hidden_dim: Dimension of the hidden layer.\n",
    "        :param pretrained_embeddings: Pretrained embeddings to initialize the embedding layer.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Optionally freeze the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, NUM_LABELS)\n",
    "\n",
    "    def forward(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM Classifier.\n",
    "\n",
    "        :param text: Input tensor containing text data.\n",
    "        :return: Output tensor after passing through the LSTM and fully connected layers.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text.long())\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.fc(hidden)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file.\n",
    "\n",
    "    :param glove_file: Path to the GloVe embeddings file.\n",
    "    :return: Dictionary mapping words to their corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Loading GloVe Embeddings\"):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n"
   ],
   "id": "f90bc4ba8d73a74f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:57.285148Z",
     "start_time": "2024-07-16T08:53:46.043090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_embeddings = load_glove_embeddings(os.path.join(\"..\", \"asset\", \"glove.6B.100d.txt\"))\n",
    "\n",
    "# Tokenization and vocabulary creation\n",
    "word_count = Counter(word for sentence in INPUTS for word in sentence.lower().split())\n",
    "vocabulary = {word: i + 1 for i, word in enumerate(word_count)}  # start indexing from 1\n",
    "vocabulary['<PAD>'] = 0  # Padding value\n",
    "\n",
    "# Embedding matrix creation\n",
    "embedding_dim = 100  # Dimensionality of GloVe embeddings used\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "for word, i in tqdm(vocabulary.items(), desc='Creating Embedding Matrix'):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "sequences = [[vocabulary[word] for word in text.lower().split()] for text in INPUTS]\n",
    "\n",
    "# Finding the longest sequence\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "seq_padded = [seq + [vocabulary['<PAD>']] * (max_seq_len - len(seq)) for seq in sequences]"
   ],
   "id": "ed53b7c6c9f480c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embeddings: 400000it [00:08, 48500.56it/s]\n",
      "Creating Embedding Matrix: 100%|██████████| 257/257 [00:00<00:00, 115398.37it/s]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:56:39.140581Z",
     "start_time": "2024-07-16T08:56:34.829175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMClassifier(len(vocabulary), embedding_dim, 64, embedding_matrix)\n",
    "\n",
    "x_tensor = torch.FloatTensor(seq_padded)\n",
    "y_tensor = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"lstm\")"
   ],
   "id": "6086b0ad8cb21eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8965 | Precision: 0.1500, Recall: 1.0000, F1: 0.2609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8707 | Precision: 0.2000, Recall: 1.0000, F1: 0.3304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8594 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8707 | Precision: 0.2000, Recall: 1.0000, F1: 0.3333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8706 | Precision: 0.2000, Recall: 1.0000, F1: 0.3333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8587 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.2250 (±0.0000)\n",
      "recall: 1.0000 (±0.0000)\n",
      "f1: 0.3667 (±0.0000)\n",
      "All fold results saved to 'lstm.csv'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVM, Random Forest, Gradient Boosting",
   "id": "ccd9856e5cf86718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:42:33.859828Z",
     "start_time": "2024-07-18T13:42:33.843042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassifiersPoolEvaluator:\n",
    "    \"\"\"\n",
    "    ClassifiersPoolEvaluator class for evaluating a pool of classifiers using TF-IDF features and k-fold cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifiersPoolEvaluator with TF-IDF vectorizer and a dictionary of classifiers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a dictionary of classifiers to evaluate\n",
    "        self.classifiers = {\n",
    "            \"svm\": OneVsRestClassifier(SVC(kernel='linear', probability=True)),\n",
    "            \"random_forest\": OneVsRestClassifier(\n",
    "                RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)),\n",
    "            \"gradient_boosting\": OneVsRestClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3))\n",
    "        }\n",
    "\n",
    "        # Transform the documents into TF-IDF features\n",
    "        self.X = VECTORIZER.fit_transform(INPUTS)\n",
    "\n",
    "        # Transform the labels using MultiLabelBinarizer\n",
    "        self.y = np.array(LABELS)\n",
    "\n",
    "    def __evaluate_fold(self, classifier: OneVsRestClassifier, train_index: List[int], test_index: List[int],\n",
    "                        fold_num: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a classifier on a single fold of cross-validation.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :param train_index: Indices for the training data.\n",
    "        :param test_index: Indices for the test data.\n",
    "        :param fold_num: The fold number.\n",
    "        :return: A dictionary of computed metrics.\n",
    "        \"\"\"\n",
    "        X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "        y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Make predictions on the test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "\n",
    "        # Compute metrics using the provided utility function\n",
    "        metrics = compute_metrics(y_test, predictions)\n",
    "        print(f\"Results for fold {fold_num} | \"\n",
    "              f\"Precision: {metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['recall']:.4f}, \"\n",
    "              f\"F1: {metrics['f1']:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "    def __k_fold_cv(self, classifier: OneVsRestClassifier) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation on a given classifier.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :return: A DataFrame containing the results of each fold.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "        # Evaluate the classifier on each fold and collect the results\n",
    "        results = []\n",
    "        for fold_num, (train_index, test_index) in enumerate(kf.split(self.X), 1):\n",
    "            metrics = self.__evaluate_fold(classifier, train_index, test_index, fold_num)\n",
    "            results.append(metrics)\n",
    "        # Return the results as a DataFrame\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def pool_evaluation(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the evaluation for each classifier defined in self.classifiers.\n",
    "        \"\"\"\n",
    "        # Run the evaluation for each classifier defined in self.classifiers\n",
    "        for classifier_name, classifier in self.classifiers.items():\n",
    "            print(f\"\\nTesting classifier: {classifier_name}\\n\")\n",
    "            # Evaluate the classifier and get the metrics DataFrame\n",
    "            metrics_df = self.__k_fold_cv(classifier)\n",
    "            # Save the results using the provided utility function\n",
    "            save_results(metrics_df, f\"{classifier_name}.csv\")\n",
    "            # Print the results\n",
    "            print(f\"Results for {classifier_name}:\\n{metrics_df}\\n\")"
   ],
   "id": "cd228b01fa44096f",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T13:43:25.307762Z",
     "start_time": "2024-07-18T13:42:34.325027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = ClassifiersPoolEvaluator()\n",
    "evaluator.pool_evaluation()"
   ],
   "id": "4f3360bdb68dd76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing classifier: svm\n",
      "\n",
      "Results for fold 1 | Precision: 0.8071,Recall: 0.7161,F1: 0.7386\n",
      "Results for fold 2 | Precision: 0.8182,Recall: 0.6777,F1: 0.7183\n",
      "Results for fold 3 | Precision: 0.8342,Recall: 0.7573,F1: 0.7789\n",
      "Results for fold 4 | Precision: 0.7712,Recall: 0.7785,F1: 0.7464\n",
      "Results for fold 5 | Precision: 0.7930,Recall: 0.7656,F1: 0.7615\n",
      "Results for fold 6 | Precision: 0.8185,Recall: 0.7143,F1: 0.7337\n",
      "Results for fold 7 | Precision: 0.8094,Recall: 0.7787,F1: 0.7653\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[44], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m evaluator \u001B[38;5;241m=\u001B[39m ClassifiersPoolEvaluator()\n\u001B[0;32m----> 2\u001B[0m \u001B[43mevaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpool_evaluation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[43], line 77\u001B[0m, in \u001B[0;36mClassifiersPoolEvaluator.pool_evaluation\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTesting classifier: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassifier_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# Evaluate the classifier and get the metrics DataFrame\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m metrics_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__k_fold_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;66;03m# Save the results using the provided utility function\u001B[39;00m\n\u001B[1;32m     79\u001B[0m save_results(metrics_df, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclassifier_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[43], line 64\u001B[0m, in \u001B[0;36mClassifiersPoolEvaluator.__k_fold_cv\u001B[0;34m(self, classifier)\u001B[0m\n\u001B[1;32m     62\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fold_num, (train_index, test_index) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(kf\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mX), \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 64\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__evaluate_fold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold_num\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(metrics)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;66;03m# Return the results as a DataFrame\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[43], line 41\u001B[0m, in \u001B[0;36mClassifiersPoolEvaluator.__evaluate_fold\u001B[0;34m(self, classifier, train_index, test_index, fold_num)\u001B[0m\n\u001B[1;32m     38\u001B[0m y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my[train_index], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my[test_index]\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Train the classifier on the training data\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[43mclassifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# Make predictions on the test data\u001B[39;00m\n\u001B[1;32m     43\u001B[0m predictions \u001B[38;5;241m=\u001B[39m classifier\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1472\u001B[0m     )\n\u001B[1;32m   1473\u001B[0m ):\n\u001B[0;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/multiclass.py:373\u001B[0m, in \u001B[0;36mOneVsRestClassifier.fit\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    369\u001B[0m columns \u001B[38;5;241m=\u001B[39m (col\u001B[38;5;241m.\u001B[39mtoarray()\u001B[38;5;241m.\u001B[39mravel() \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m Y\u001B[38;5;241m.\u001B[39mT)\n\u001B[1;32m    370\u001B[0m \u001B[38;5;66;03m# In cases where individual estimators are very fast to train setting\u001B[39;00m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;66;03m# n_jobs > 1 in can results in slower performance due to the overhead\u001B[39;00m\n\u001B[1;32m    372\u001B[0m \u001B[38;5;66;03m# of spawning threads.  See joblib issue #112.\u001B[39;00m\n\u001B[0;32m--> 373\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_ \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_binary\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclasses\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mnot \u001B[39;49m\u001B[38;5;132;43;01m%s\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m%\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_binarizer_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses_\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabel_binarizer_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclasses_\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43menumerate\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_features_in_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_features_in_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimators_[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mn_features_in_\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[1;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[1;32m     66\u001B[0m )\n\u001B[0;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/joblib/parallel.py:1918\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1916\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_sequential_output(iterable)\n\u001B[1;32m   1917\u001B[0m     \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 1918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1920\u001B[0m \u001B[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001B[39;00m\n\u001B[1;32m   1921\u001B[0m \u001B[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001B[39;00m\n\u001B[1;32m   1922\u001B[0m \u001B[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001B[39;00m\n\u001B[1;32m   1923\u001B[0m \u001B[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001B[39;00m\n\u001B[1;32m   1924\u001B[0m \u001B[38;5;66;03m# callback.\u001B[39;00m\n\u001B[1;32m   1925\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/joblib/parallel.py:1847\u001B[0m, in \u001B[0;36mParallel._get_sequential_output\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1845\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_batches \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1846\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_dispatched_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m-> 1847\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1848\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_completed_tasks \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1849\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_progress()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/utils/parallel.py:129\u001B[0m, in \u001B[0;36m_FuncWrapper.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    127\u001B[0m     config \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig):\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/multiclass.py:96\u001B[0m, in \u001B[0;36m_fit_binary\u001B[0;34m(estimator, X, y, fit_params, classes)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     95\u001B[0m     estimator \u001B[38;5;241m=\u001B[39m clone(estimator)\n\u001B[0;32m---> 96\u001B[0m     \u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m estimator\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/base.py:1474\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1467\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1469\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1470\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1471\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1472\u001B[0m     )\n\u001B[1;32m   1473\u001B[0m ):\n\u001B[0;32m-> 1474\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:250\u001B[0m, in \u001B[0;36mBaseLibSVM.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    247\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[LibSVM]\u001B[39m\u001B[38;5;124m\"\u001B[39m, end\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    249\u001B[0m seed \u001B[38;5;241m=\u001B[39m rnd\u001B[38;5;241m.\u001B[39mrandint(np\u001B[38;5;241m.\u001B[39miinfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mmax)\n\u001B[0;32m--> 250\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001B[39;00m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape_fit_ \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(X, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m (n_samples,)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/sklearn/svm/_base.py:370\u001B[0m, in \u001B[0;36mBaseLibSVM._sparse_fit\u001B[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001B[0m\n\u001B[1;32m    356\u001B[0m kernel_type \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sparse_kernels\u001B[38;5;241m.\u001B[39mindex(kernel)\n\u001B[1;32m    358\u001B[0m libsvm_sparse\u001B[38;5;241m.\u001B[39mset_verbosity_wrap(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose)\n\u001B[1;32m    360\u001B[0m (\n\u001B[1;32m    361\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_,\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msupport_vectors_,\n\u001B[1;32m    363\u001B[0m     dual_coef_data,\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintercept_,\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_n_support,\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probA,\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_probB,\n\u001B[1;32m    368\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_status_,\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_iter,\n\u001B[0;32m--> 370\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[43mlibsvm_sparse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlibsvm_sparse_train\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindptr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43msolver_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkernel_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdegree\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcoef0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtol\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mC\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mclass_weight_\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnu\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshrinking\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    389\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprobability\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_iter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_seed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_warn_from_fit_status()\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclasses_\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32msklearn/svm/_libsvm_sparse.pyx:219\u001B[0m, in \u001B[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/scipy/sparse/_compressed.py:27\u001B[0m, in \u001B[0;36m_cs_matrix.__init__\u001B[0;34m(self, arg1, shape, dtype, copy)\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01m_cs_matrix\u001B[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001B[1;32m     23\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124;03m    base array/matrix class for compressed row- and column-oriented arrays/matrices\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, arg1, shape\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     28\u001B[0m         _data_matrix\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m issparse(arg1):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
