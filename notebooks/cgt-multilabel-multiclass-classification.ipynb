{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "172880abc3d187d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-18T15:55:56.882375Z",
     "start_time": "2024-07-18T15:55:56.876654Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc1ec8b396d0a08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:55:58.121052Z",
     "start_time": "2024-07-18T15:55:58.102844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the device for torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device {DEVICE}\")\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(f\"Random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# Path to dataset\n",
    "PATH_TO_DATASET = os.path.join(\"..\", \"dataset\", \"cgt\")\n",
    "print(f\"Path to dataset: {PATH_TO_DATASET}\")\n",
    "\n",
    "# Model and training configurations\n",
    "BERT_MODEL_TYPE = 'microsoft/codebert-base'\n",
    "print(f\"BERT model type: {BERT_MODEL_TYPE}\")\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "print(f\"Max features: {MAX_FEATURES}\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "print(f\"Number of folds: {NUM_FOLDS}\")\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "NUM_LABELS = 20  # 20 for CodeSmell, 160 overall\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "LR = 0.001\n",
    "print(f\"Learning rate: {LR}\")\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "print(f\"Test size: {TEST_SIZE}\")\n",
    "\n",
    "# File configurations\n",
    "FILE_TYPE = \"source\"  # Can be 'source', 'runtime', 'bytecode'\n",
    "FILE_EXT = None\n",
    "if FILE_TYPE == \"source\":\n",
    "    FILE_EXT = \".sol\"\n",
    "elif FILE_TYPE == \"runtime\":\n",
    "    FILE_EXT = \".rt.hex\"\n",
    "elif FILE_TYPE == \"bytecode\":\n",
    "    FILE_EXT = \".rt\"\n",
    "FILE_ID = \"sol\"  # Can be 'sol ,'sol2' for 'source', otherwise 'runtime', 'bytecode'\n",
    "\n",
    "print(f\"File type: {FILE_TYPE}\")\n",
    "print(f\"File extension: {FILE_EXT}\")\n",
    "print(f\"File ID: {FILE_ID}\")\n",
    "\n",
    "# Creating the log directory if it doesn't exist\n",
    "LOG_DIR = os.path.join(\"log\", FILE_TYPE)\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "    print(f\"Log directory created at {LOG_DIR}\")\n",
    "else:\n",
    "    print(f\"Log directory already exists at {LOG_DIR}\")"
   ],
   "id": "1374f63db7c6cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cpu\n",
      "Random seeds set to 0\n",
      "Path to dataset: ../dataset/cgt\n",
      "BERT model type: microsoft/codebert-base\n",
      "Max features: 500\n",
      "Batch size: 1\n",
      "Number of folds: 10\n",
      "Number of epochs: 25\n",
      "Number of labels: 20\n",
      "Learning rate: 0.001\n",
      "Test size: 0.1\n",
      "File type: source\n",
      "File extension: .sol\n",
      "File ID: sol\n",
      "Log directory already exists at log/source\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "a424b60aff524ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "24ae2bd4455509d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:00.995318Z",
     "start_time": "2024-07-18T15:56:00.989342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_hex(hex_data: str) -> str:\n",
    "    # Reads a hex file and converts it to a byte string\n",
    "    byte_data = bytes.fromhex(hex_data.strip())\n",
    "\n",
    "    # Convert byte data to a readable ASCII string, ignoring non-ASCII characters\n",
    "    return ' '.join(f'{byte:02x}' for byte in byte_data)\n",
    "\n",
    "\n",
    "def preprocess_solidity_code(code: str) -> str:\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove blank lines (lines only containing whitespace)\n",
    "    lines = code.split('\\n')\n",
    "    non_blank_lines = [line for line in lines if line.strip() != '']\n",
    "    code = '\\n'.join(non_blank_lines)\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def preprocess(data: str) -> str:\n",
    "    return preprocess_solidity_code(data) if FILE_TYPE == \"source\" else preprocess_hex(data)"
   ],
   "id": "41e2ed4561254c43",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labels Management",
   "id": "7e084d9053915673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:08.189173Z",
     "start_time": "2024-07-18T15:56:08.177242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_inputs_and_gt(data: pd.DataFrame) -> Tuple:\n",
    "    \"\"\"\n",
    "    Initialize inputs, labels, and groundtruth (gt) from the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :return: A tuple containing the list of inputs, labels dictionary, and gt dictionary.\n",
    "    \"\"\"\n",
    "    inputs, labels, gt = {}, {}, {}\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Initializing inputs and groundtruth data\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Initialize the documents\n",
    "            inputs[item_id] = preprocess(open(path_to_file, 'r', encoding=\"utf8\").read())\n",
    "\n",
    "            # Initialize the label\n",
    "            labels[item_id] = [0] * NUM_LABELS\n",
    "\n",
    "            # Initialize the groundtruth\n",
    "            prop = row[\"property\"].lower()\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "\n",
    "    return list(inputs.values()), labels, gt\n",
    "\n",
    "\n",
    "def set_labels(data: pd.DataFrame, labels: Dict, gt: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Set up the labels based on the groundtruth (gt) for the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :param labels: A dictionary where keys are item IDs and values are lists representing labels.\n",
    "    :param gt: A dictionary where keys are properties and values are their corresponding indices.\n",
    "    :return: A list of labels values.\n",
    "    \"\"\"\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Setting up the labels\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Set label   \n",
    "            prop = row[\"property\"].lower()\n",
    "            if row['property_holds'] == 't':\n",
    "                labels[item_id][gt[prop]] = 1\n",
    "\n",
    "    return list(labels.values())\n"
   ],
   "id": "1511840b0951d215",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization of the dataset",
   "id": "bac1ca9644eca1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:12.687208Z",
     "start_time": "2024-07-18T15:56:10.078976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the dataset from CSV\n",
    "dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "\n",
    "# Count the frequency of each item in the \"property\" column\n",
    "print(\"Counting frequency of each item in the 'property' column...\")\n",
    "property_frequency = dataset[\"property\"].value_counts()\n",
    "print(\"Frequency of each item in the 'property' column:\")\n",
    "print(property_frequency)\n",
    "\n",
    "# Print the number of unique values in the \"property\" column\n",
    "unique_properties_count = dataset[\"property\"].nunique()\n",
    "print(f\"\\nThe number of unique values in the 'property' column is {unique_properties_count}.\")\n",
    "\n",
    "# Count the frequency of each item in the \"dataset\" column\n",
    "print(\"\\nCounting frequency of each item in the 'dataset' column...\")\n",
    "dataset_frequency = dataset[\"dataset\"].value_counts()\n",
    "print(\"Frequency of each item in the 'dataset' column:\")\n",
    "print(dataset_frequency)\n",
    "\n",
    "# Find the item with the maximum occurrence in the \"dataset\" column\n",
    "most_frequent_item = dataset_frequency.idxmax()\n",
    "most_frequent_count = dataset_frequency.max()\n",
    "print(f\"\\nThe most frequent dataset is '{most_frequent_item}' and it appears {most_frequent_count} times.\")\n",
    "\n",
    "# Exclude outliers from the dataset\n",
    "dataset = dataset[dataset[\"dataset\"] == most_frequent_item]\n",
    "\n",
    "# Initialize the documents and the groundtruth\n",
    "print(\"\\nInitializing the documents and the ground truth...\")\n",
    "INPUTS, LABELS, gt = init_inputs_and_gt(dataset)\n",
    "\n",
    "# Set the labels for the multilabel classification problem\n",
    "print(\"Setting the labels for the multilabel classification problem...\")\n",
    "LABELS = set_labels(dataset, LABELS, gt)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "print(\"Initializing the TF-IDF vectorizer...\")\n",
    "VECTORIZER = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "print(\"Initialization complete!\")"
   ],
   "id": "fb254a69e43e824e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting frequency of each item in the 'property' column...\n",
      "Frequency of each item in the 'property' column:\n",
      "property\n",
      "Reentrancy                       1531\n",
      "Tx_State_Dep                     1064\n",
      "Unchkd_send                      1058\n",
      "Blk_State_Dep                    1057\n",
      "Failed_send                      1057\n",
      "                                 ... \n",
      "short_addresses                     1\n",
      "Honeypot Uninitialised struct       1\n",
      "Honeypot Balance disorder           1\n",
      "Honeypot Straw man contract         1\n",
      "Honeypot Type overflow              1\n",
      "Name: count, Length: 160, dtype: int64\n",
      "\n",
      "The number of unique values in the 'property' column is 160.\n",
      "\n",
      "Counting frequency of each item in the 'dataset' column...\n",
      "Frequency of each item in the 'dataset' column:\n",
      "dataset\n",
      "CodeSmells        10395\n",
      "Zeus               7315\n",
      "eThor               702\n",
      "ContractFuzzer      367\n",
      "SolidiFI            343\n",
      "EverEvolvingG       292\n",
      "Doublade            276\n",
      "NPChecker           212\n",
      "JiuZhou             165\n",
      "SBcurated           129\n",
      "SWCregistry         116\n",
      "EthRacer            109\n",
      "NotSoSmartC          34\n",
      "Name: count, dtype: int64\n",
      "\n",
      "The most frequent dataset is 'CodeSmells' and it appears 10395 times.\n",
      "\n",
      "Initializing the documents and the ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing inputs and groundtruth data: 10395it [00:02, 4887.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the labels for the multilabel classification problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up the labels: 10395it [00:00, 29111.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the TF-IDF vectorizer...\n",
      "Initialization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross Validation",
   "id": "546491e26d424c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:15.140052Z",
     "start_time": "2024-07-18T15:56:15.133252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels: List[Any], pred_labels: List[Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the given true and predicted labels.\n",
    "\n",
    "    :param true_labels: The ground truth labels.\n",
    "    :param pred_labels: The predicted labels.\n",
    "    :return: A dictionary containing precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the results to a CSV file.\n",
    "\n",
    "    :param results: The results to save, typically a list of dictionaries.\n",
    "    :param filename: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(LOG_DIR, filename), index=False)\n",
    "    print(f\"All fold results saved to '{LOG_DIR}'/'{filename}'\")\n"
   ],
   "id": "795c276e2fb6e8b1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:15.730189Z",
     "start_time": "2024-07-18T15:56:15.716614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for handling the training and evaluation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model, loss criterion, and optimizer.\n",
    "\n",
    "        :param model: The neural network model to be trained.\n",
    "        \"\"\"\n",
    "        self.__untrained_model = model\n",
    "        self._model = model.to(DEVICE)\n",
    "        self._optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "\n",
    "    def reset_model(self):\n",
    "        self._model = self.__untrained_model\n",
    "        self._optimizer = optim.Adam(self.__untrained_model.parameters(), lr=LR)\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch elements to the appropriate device (CPU/GPU)\n",
    "        batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "        # Prepare the inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(labels.cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def run_epoch(self, dataloader: DataLoader, train_mode: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single epoch of training or evaluation.\n",
    "\n",
    "        :param dataloader: DataLoader providing the data for the epoch.\n",
    "        :param train_mode: Boolean flag indicating whether to train or evaluate.\n",
    "        :return: A tuple containing the average loss and a dictionary of average metrics.\n",
    "        \"\"\"\n",
    "        # Set the mode for the epoch (Training or Testing)\n",
    "        phase = 'Training' if train_mode else 'Testing'\n",
    "        self._model.train() if train_mode else self._model.eval()\n",
    "\n",
    "        losses, metrics_list = [], []\n",
    "\n",
    "        # Iterate over the data loader\n",
    "        for batch in tqdm(dataloader, desc=phase):\n",
    "            # Move batch elements to the appropriate device\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            loss, batch_metrics = self._train_batch(batch) if train_mode else self._evaluate_batch(batch)\n",
    "\n",
    "            # Accumulate the loss and metrics\n",
    "            losses.append(loss)\n",
    "            metrics_list.append(batch_metrics)\n",
    "\n",
    "        # Compute average loss and metrics for the epoch\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "\n",
    "        return avg_loss, avg_metrics\n"
   ],
   "id": "7753f6da9628de81",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:23.946645Z",
     "start_time": "2024-07-18T15:56:23.936808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    CrossValidator class for handling k-fold cross-validation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer: Trainer, train_data: TensorDataset, test_data: TensorDataset):\n",
    "        \"\"\"\n",
    "        Initialize the CrossValidator with trainer, training data, and test data.\n",
    "\n",
    "        :param trainer: An instance of the Trainer class.\n",
    "        :param train_data: The training dataset.\n",
    "        :param test_data: The test dataset.\n",
    "        \"\"\"\n",
    "        self.__trainer = trainer\n",
    "        self.__train_data = train_data\n",
    "        self.__test_data = test_data\n",
    "\n",
    "    def __train_and_evaluate(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a specified number of epochs.\n",
    "\n",
    "        :param train_dataloader: DataLoader for the training data.\n",
    "        :param test_dataloader: DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n --- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "            # Train the model and print training metrics\n",
    "            avg_train_loss, avg_train_metrics = self.__trainer.run_epoch(train_dataloader, train_mode=True)\n",
    "            print(f\"\\n TRAIN | Loss: {avg_train_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_train_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_train_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_train_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "            # Evaluate the model on the validation set and print validation metrics\n",
    "            avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "            print(f\" VALID | Loss: {avg_test_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "    def __evaluate_on_test_set(self, test_dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "\n",
    "        :param test_dataloader: DataLoader for the test data.\n",
    "        :return: A dictionary of test set metrics.\n",
    "        \"\"\"\n",
    "        avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "\n",
    "        # Print test set metrics\n",
    "        print(f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} |\"\n",
    "              f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "              f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "              f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "        return avg_test_metrics\n",
    "\n",
    "    def k_fold_cv(self, log_id: str = \"bert\") -> None:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        :param log_id: Identifier for logging purposes, typically the model name.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Iterate over each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.__train_data)):\n",
    "            # Create data loaders for training and validation sets\n",
    "            train_subsampler = Subset(self.__train_data, train_idx)\n",
    "            val_subsampler = Subset(self.__train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_subsampler,\n",
    "                sampler=RandomSampler(train_subsampler),\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subsampler,\n",
    "                batch_size=BATCH_SIZE  # No need for shuffling\n",
    "            )\n",
    "\n",
    "            print(f\"Starting Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "            # Train and evaluate the model for the current fold\n",
    "            self.__train_and_evaluate(train_loader, val_loader)\n",
    "\n",
    "            # Evaluate on the test set after each fold\n",
    "            metrics = self.__evaluate_on_test_set(DataLoader(self.__test_data, batch_size=BATCH_SIZE, shuffle=False))\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "            # Reset the model to untrained\n",
    "            self.__trainer.reset_model()\n",
    "\n",
    "        # Calculate average and standard deviation of each metric across all folds\n",
    "        metric_keys = fold_metrics[0].keys()  # Assuming all metrics dictionaries have the same structure\n",
    "        average_metrics = {key: np.mean([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "        std_dev_metrics = {key: np.std([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "\n",
    "        # Print average metrics and their standard deviations\n",
    "        print(\"Average Metrics Over All Folds:\")\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f\"{key}: {value:.4f} (±{std_dev_metrics[key]:.4f})\")\n",
    "\n",
    "        # Save metrics to CSV file\n",
    "        save_results(fold_metrics, filename=f\"{log_id}.csv\")\n"
   ],
   "id": "543bd399383e5b7e",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "9ae8b68dbe64b7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT",
   "id": "f2934d41ac3e7b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:53:29.035843Z",
     "start_time": "2024-07-18T15:53:29.027802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERTModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BERTModelTrainer class for handling the training and evaluation of a BERT-based model.\n",
    "    Inherits from the Trainer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BERTModelTrainer with model, optimizer, and loss function.\n",
    "\n",
    "        :param model: The BERT model to be trained.\n",
    "        \"\"\"\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Initialize the optimizer with model parameters and a learning rate\n",
    "        self._optimizer = AdamW(self._model.parameters(), lr=LR)\n",
    "\n",
    "        # Define the loss function for binary classification with logits\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare the inputs for the model\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(**inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs.logits).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(batch[2].detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n"
   ],
   "id": "c2c756331f70649e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:54:16.635794Z",
     "start_time": "2024-07-18T15:53:29.039365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(BERT_MODEL_TYPE, num_labels=20, ignore_mismatched_sizes=True)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "model.to(DEVICE)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_TYPE, ignore_mismatched_sizes=True)\n",
    "\n",
    "x, y = tokenizer(\n",
    "    INPUTS,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "), LABELS\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x['input_ids'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Split attention masks for training and test sets\n",
    "train_masks, test_masks, _, _ = train_test_split(x['attention_mask'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_data = TensorDataset(x_train, train_masks, torch.tensor(y_train).float())\n",
    "test_data = TensorDataset(x_test, test_masks, torch.tensor(y_test).float())\n",
    "CrossValidator(BERTModelTrainer(model), train_data, test_data).k_fold_cv(log_id=\"bert\")"
   ],
   "id": "cfb9656b31564534",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/10\n",
      "\n",
      " --- Epoch 1/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 11/445 [00:37<24:37,  3.40s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m train_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_train, train_masks, torch\u001B[38;5;241m.\u001B[39mtensor(y_train)\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m     26\u001B[0m test_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_test, test_masks, torch\u001B[38;5;241m.\u001B[39mtensor(y_test)\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m---> 27\u001B[0m \u001B[43mCrossValidator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mBERTModelTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk_fold_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbert\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[8], line 87\u001B[0m, in \u001B[0;36mCrossValidator.k_fold_cv\u001B[0;34m(self, log_id)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_FOLDS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Train and evaluate the model for the current fold\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__train_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set after each fold\u001B[39;00m\n\u001B[1;32m     90\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__evaluate_on_test_set(DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__test_data, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "Cell \u001B[0;32mIn[8], line 29\u001B[0m, in \u001B[0;36mCrossValidator.__train_and_evaluate\u001B[0;34m(self, train_dataloader, test_dataloader)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m --- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model and print training metrics\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m avg_train_loss, avg_train_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m TRAIN | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     32\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Evaluate the model on the validation set and print validation metrics\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[7], line 96\u001B[0m, in \u001B[0;36mTrainer.run_epoch\u001B[0;34m(self, dataloader, train_mode)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39mphase):\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m# Move batch elements to the appropriate device\u001B[39;00m\n\u001B[1;32m     94\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(b\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch)\n\u001B[0;32m---> 96\u001B[0m     loss, batch_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m train_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate_batch(batch)\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;66;03m# Accumulate the loss and metrics\u001B[39;00m\n\u001B[1;32m     99\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "Cell \u001B[0;32mIn[9], line 70\u001B[0m, in \u001B[0;36mBERTModelTrainer._train_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;66;03m# Backward pass and optimize\u001B[39;00m\n\u001B[1;32m     69\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Make predictions and compute metrics\u001B[39;00m\n\u001B[1;32m     73\u001B[0m predictions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(outputs\u001B[38;5;241m.\u001B[39mlogits)\u001B[38;5;241m.\u001B[39mround()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    380\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    381\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    382\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    383\u001B[0m             )\n\u001B[0;32m--> 385\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[1;32m    388\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[0;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:187\u001B[0m, in \u001B[0;36mAdamW.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    174\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    176\u001B[0m     has_complex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[1;32m    177\u001B[0m         group,\n\u001B[1;32m    178\u001B[0m         params_with_grad,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    184\u001B[0m         state_steps,\n\u001B[1;32m    185\u001B[0m     )\n\u001B[0;32m--> 187\u001B[0m     \u001B[43madamw\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    190\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    193\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    194\u001B[0m \u001B[43m        \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    195\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    197\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    198\u001B[0m \u001B[43m        \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m        \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforeach\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mforeach\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcapturable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdifferentiable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfused\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfused\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mgrad_scale\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfound_inf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:339\u001B[0m, in \u001B[0;36madamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    337\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adamw\n\u001B[0;32m--> 339\u001B[0m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m    \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mamsgrad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaximize\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcapturable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapturable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdifferentiable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifferentiable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfound_inf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfound_inf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_complex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/optim/adamw.py:419\u001B[0m, in \u001B[0;36m_single_tensor_adamw\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001B[0m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[1;32m    418\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[0;32m--> 419\u001B[0m \u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmul_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbeta2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[1;32m    421\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[1;32m    422\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FFNN",
   "id": "523fb217e9d16400"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Neural Network with three fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network layers.\n",
    "        \"\"\"\n",
    "        super(FFNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(MAX_FEATURES, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, NUM_LABELS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :return: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ],
   "id": "c35b14720c73a4bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = FFNNClassifier()\n",
    "\n",
    "x = torch.FloatTensor(VECTORIZER.fit_transform(INPUTS).toarray())\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"ffnn\")"
   ],
   "id": "785da51a7681dd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "212af9b00ac25811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:30.301220Z",
     "start_time": "2024-07-18T15:56:30.294086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier for text classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pretrained_embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM Classifier.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of the embedding vectors.\n",
    "        :param hidden_dim: Dimension of the hidden layer.\n",
    "        :param pretrained_embeddings: Pretrained embeddings to initialize the embedding layer.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Optionally freeze the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, NUM_LABELS)\n",
    "\n",
    "    def forward(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM Classifier.\n",
    "\n",
    "        :param text: Input tensor containing text data.\n",
    "        :return: Output tensor after passing through the LSTM and fully connected layers.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text.long())\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.fc(hidden)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file.\n",
    "\n",
    "    :param glove_file: Path to the GloVe embeddings file.\n",
    "    :return: Dictionary mapping words to their corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Loading GloVe Embeddings\"):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n"
   ],
   "id": "f90bc4ba8d73a74f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T15:56:37.938615Z",
     "start_time": "2024-07-18T15:56:31.081208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_embeddings = load_glove_embeddings(os.path.join(\"..\", \"asset\", \"glove.6B.100d.txt\"))\n",
    "\n",
    "# Tokenization and vocabulary creation\n",
    "word_count = Counter(word for sentence in INPUTS for word in sentence.lower().split())\n",
    "vocabulary = {word: i + 1 for i, word in enumerate(word_count)}  # start indexing from 1\n",
    "vocabulary['<PAD>'] = 0  # Padding value\n",
    "\n",
    "# Embedding matrix creation\n",
    "embedding_dim = 100  # Dimensionality of GloVe embeddings used\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "for word, i in tqdm(vocabulary.items(), desc='Creating Embedding Matrix'):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "sequences = [[vocabulary[word] for word in text.lower().split()] for text in INPUTS]\n",
    "\n",
    "# Finding the longest sequence\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "seq_padded = [seq + [vocabulary['<PAD>']] * (max_seq_len - len(seq)) for seq in sequences]"
   ],
   "id": "ed53b7c6c9f480c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embeddings: 400000it [00:06, 61707.82it/s]\n",
      "Creating Embedding Matrix: 100%|██████████| 45901/45901 [00:00<00:00, 1657364.27it/s]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T17:13:36.663435Z",
     "start_time": "2024-07-18T17:13:15.389895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMClassifier(len(vocabulary), embedding_dim, 8, embedding_matrix)\n",
    "\n",
    "x = torch.FloatTensor(seq_padded)\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"lstm\")"
   ],
   "id": "6086b0ad8cb21eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/10\n",
      "\n",
      " --- Epoch 1/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 14/445 [00:19<09:56,  1.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m train_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_train, y_train)\n\u001B[1;32m      8\u001B[0m test_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_test, y_test)\n\u001B[0;32m---> 10\u001B[0m \u001B[43mCrossValidator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk_fold_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlstm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[25], line 87\u001B[0m, in \u001B[0;36mCrossValidator.k_fold_cv\u001B[0;34m(self, log_id)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_FOLDS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Train and evaluate the model for the current fold\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__train_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set after each fold\u001B[39;00m\n\u001B[1;32m     90\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__evaluate_on_test_set(DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__test_data, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "Cell \u001B[0;32mIn[25], line 29\u001B[0m, in \u001B[0;36mCrossValidator.__train_and_evaluate\u001B[0;34m(self, train_dataloader, test_dataloader)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m --- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model and print training metrics\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m avg_train_loss, avg_train_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m TRAIN | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     32\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Evaluate the model on the validation set and print validation metrics\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[24], line 96\u001B[0m, in \u001B[0;36mTrainer.run_epoch\u001B[0;34m(self, dataloader, train_mode)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39mphase):\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m# Move batch elements to the appropriate device\u001B[39;00m\n\u001B[1;32m     94\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(b\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch)\n\u001B[0;32m---> 96\u001B[0m     loss, batch_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m train_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate_batch(batch)\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;66;03m# Accumulate the loss and metrics\u001B[39;00m\n\u001B[1;32m     99\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "Cell \u001B[0;32mIn[24], line 68\u001B[0m, in \u001B[0;36mTrainer._train_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     65\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_fn(outputs, labels)\n\u001B[1;32m     67\u001B[0m \u001B[38;5;66;03m# Backward pass and optimize\u001B[39;00m\n\u001B[0;32m---> 68\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     71\u001B[0m \u001B[38;5;66;03m# Make predictions and compute metrics\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVM, Random Forest, Gradient Boosting",
   "id": "ccd9856e5cf86718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T17:14:33.421072Z",
     "start_time": "2024-07-18T17:14:33.409605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassifiersPoolEvaluator:\n",
    "    \"\"\"\n",
    "    ClassifiersPoolEvaluator class for evaluating a pool of classifiers using TF-IDF features and k-fold cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifiersPoolEvaluator with TF-IDF vectorizer and a dictionary of classifiers.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define a dictionary of classifiers to evaluate\n",
    "        self.classifiers = {\n",
    "            \"svm\": OneVsRestClassifier(SVC(kernel='linear', probability=True)),\n",
    "            \"random_forest\": OneVsRestClassifier(\n",
    "                RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)),\n",
    "            \"gradient_boosting\": OneVsRestClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3))\n",
    "        }\n",
    "\n",
    "        # Transform the documents into TF-IDF features\n",
    "        self.X = VECTORIZER.fit_transform(INPUTS)\n",
    "\n",
    "        # Transform the labels into a numpy array\n",
    "        self.y = np.array(LABELS)\n",
    "\n",
    "    def __evaluate_fold(self, classifier: OneVsRestClassifier, train_index: List[int], test_index: List[int],\n",
    "                        fold_num: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a classifier on a single fold of cross-validation.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :param train_index: Indices for the training data.\n",
    "        :param test_index: Indices for the test data.\n",
    "        :param fold_num: The fold number.\n",
    "        :return: A dictionary of computed metrics.\n",
    "        \"\"\"\n",
    "        X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "        y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Make predictions on the test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "\n",
    "        # Compute metrics using the provided utility function\n",
    "        metrics = compute_metrics(y_test, predictions)\n",
    "        print(f\"Results for fold {fold_num} | \"\n",
    "              f\"Precision: {metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['recall']:.4f}, \"\n",
    "              f\"F1: {metrics['f1']:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "    def __k_fold_cv(self, classifier: OneVsRestClassifier) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation on a given classifier.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :return: A DataFrame containing the results of each fold.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "        # Evaluate the classifier on each fold and collect the results\n",
    "        results = []\n",
    "        for fold_num, (train_index, test_index) in enumerate(kf.split(self.X), 1):\n",
    "            metrics = self.__evaluate_fold(classifier, train_index, test_index, fold_num)\n",
    "            results.append(metrics)\n",
    "        # Return the results as a DataFrame\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def pool_evaluation(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the evaluation for each classifier defined in self.classifiers.\n",
    "        \"\"\"\n",
    "        # Run the evaluation for each classifier defined in self.classifiers\n",
    "        for classifier_name, classifier in self.classifiers.items():\n",
    "            print(f\"\\nTesting classifier: {classifier_name}\\n\")\n",
    "            # Evaluate the classifier and get the metrics DataFrame\n",
    "            metrics_df = self.__k_fold_cv(classifier)\n",
    "            # Save the results using the provided utility function\n",
    "            save_results(metrics_df, f\"{classifier_name}.csv\")\n",
    "            # Print the results\n",
    "            print(f\"Results for {classifier_name}:\\n{metrics_df}\\n\")"
   ],
   "id": "cd228b01fa44096f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T17:14:35.748279Z",
     "start_time": "2024-07-18T17:14:33.422788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = ClassifiersPoolEvaluator()\n",
    "evaluator.pool_evaluation()"
   ],
   "id": "4f3360bdb68dd76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing classifier: svm\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
