{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "172880abc3d187d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-16T08:52:59.521552Z",
     "start_time": "2024-07-16T08:52:55.906619Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import SVC\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc1ec8b396d0a08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:52:59.540794Z",
     "start_time": "2024-07-16T08:52:59.523181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device {DEVICE}\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "LOG_DIR = os.path.join(\"log\")\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "\n",
    "PATH_TO_DATASET = os.path.join(\"..\", \"dataset\", \"cgt\")\n",
    "BERT_MODEL_TYPE = 'microsoft/codebert-base'\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "BATCH_SIZE = 2\n",
    "NUM_FOLDS = 2\n",
    "NUM_EPOCHS = 1\n",
    "NUM_LABELS = 20\n",
    "LR = 0.001\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "FILE_TYPE = \"runtime\"\n",
    "FILE_EXT = \".rt.hex\"\n",
    "FILE_ID = \"runtime\""
   ],
   "id": "1374f63db7c6cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cpu\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Create PyTorch dataset feeding either source code, bytecode or runtime to the models."
   ],
   "id": "a424b60aff524ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "24ae2bd4455509d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:52:59.547701Z",
     "start_time": "2024-07-16T08:52:59.542117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_hex(hex_data: str) -> str:\n",
    "    # Reads a hex file and converts it to a byte string\n",
    "    byte_data = bytes.fromhex(hex_data.strip())\n",
    "\n",
    "    # Convert byte data to a readable ASCII string, ignoring non-ASCII characters\n",
    "    return ' '.join(f'{byte:02x}' for byte in byte_data)\n",
    "\n",
    "\n",
    "def preprocess_solidity_code(code: str) -> str:\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove blank lines (lines only containing whitespace)\n",
    "    lines = code.split('\\n')\n",
    "    non_blank_lines = [line for line in lines if line.strip() != '']\n",
    "    code = '\\n'.join(non_blank_lines)\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def preprocess(data: str) -> str:\n",
    "    return preprocess_solidity_code(data) if FILE_TYPE == \"source\" else preprocess_hex(data)"
   ],
   "id": "41e2ed4561254c43",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labels Management",
   "id": "7e084d9053915673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:52:59.556226Z",
     "start_time": "2024-07-16T08:52:59.549638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_inputs_and_gt(data: pd.DataFrame) -> Tuple:\n",
    "    \"\"\"\n",
    "    Initialize inputs, labels, and groundtruth (gt) from the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :return: A tuple containing the list of inputs, labels dictionary, and gt dictionary.\n",
    "    \"\"\"\n",
    "    inputs, labels, gt = {}, {}, {}\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Initializing inputs and groundtruth data\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Initialize the documents\n",
    "            inputs[item_id] = preprocess(open(path_to_file, 'r', encoding=\"utf8\").read())\n",
    "\n",
    "            # Initialize the label\n",
    "            labels[item_id] = [0] * NUM_LABELS\n",
    "\n",
    "            # Initialize the groundtruth\n",
    "            prop = row[\"property\"].lower()\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "\n",
    "    return list(inputs.values()), labels, gt\n",
    "\n",
    "\n",
    "def set_labels(data: pd.DataFrame, labels: Dict, gt: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Set up the labels based on the groundtruth (gt) for the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :param labels: A dictionary where keys are item IDs and values are lists representing labels.\n",
    "    :param gt: A dictionary where keys are properties and values are their corresponding indices.\n",
    "    :return: A list of labels values.\n",
    "    \"\"\"\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Setting up the labels\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Set label   \n",
    "            prop = row[\"property\"].lower()\n",
    "            if row['property_holds'] == 't':\n",
    "                labels[item_id][gt[prop]] = 1\n",
    "\n",
    "    return list(labels.values())\n"
   ],
   "id": "1511840b0951d215",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization of the dataset",
   "id": "bac1ca9644eca1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.033010Z",
     "start_time": "2024-07-16T08:52:59.559467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the dataset from CSV\n",
    "dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "\n",
    "# Count the frequency of each item in the column\n",
    "frequency = dataset['dataset'].value_counts()\n",
    "\n",
    "# Find the item with the maximum occurrence\n",
    "most_frequent_item = frequency.idxmax()\n",
    "most_frequent_count = frequency.max()\n",
    "\n",
    "print(f\"The most frequent item in the column is '{most_frequent_item}' and it appears {most_frequent_count} times.\")\n",
    "\n",
    "# Exclude outliers from the dataset\n",
    "dataset = dataset[dataset[\"dataset\"] == most_frequent_item]\n",
    "\n",
    "# Initialize the documents and the groundtruth\n",
    "INPUTS, LABELS, gt = init_inputs_and_gt(dataset)\n",
    "\n",
    "# Set the labels for the multilabel classification problem\n",
    "LABELS = set_labels(dataset, LABELS, gt)\n",
    "\n",
    "VECTORIZER = TfidfVectorizer(max_features=MAX_FEATURES)"
   ],
   "id": "fb254a69e43e824e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent item in the column is 'CodeSmells' and it appears 10395 times.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing inputs and groundtruth data: 10395it [00:14, 740.75it/s] \n",
      "Setting up the labels: 10395it [00:00, 30013.76it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross Validation",
   "id": "546491e26d424c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.038120Z",
     "start_time": "2024-07-16T08:53:14.034288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels: List[Any], pred_labels: List[Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the given true and predicted labels.\n",
    "\n",
    "    :param true_labels: The ground truth labels.\n",
    "    :param pred_labels: The predicted labels.\n",
    "    :return: A dictionary containing precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the results to a CSV file.\n",
    "\n",
    "    :param results: The results to save, typically a list of dictionaries.\n",
    "    :param filename: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(LOG_DIR, filename), index=False)\n",
    "    print(f\"All fold results saved to '{LOG_DIR}'/'{filename}'\")\n"
   ],
   "id": "795c276e2fb6e8b1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.047096Z",
     "start_time": "2024-07-16T08:53:14.039161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for handling the training and evaluation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model, loss criterion, and optimizer.\n",
    "\n",
    "        :param model: The neural network model to be trained.\n",
    "        \"\"\"\n",
    "        self._model = model.to(DEVICE)\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "        self._optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch elements to the appropriate device (CPU/GPU)\n",
    "        batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "        # Prepare the inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(labels.cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def run_epoch(self, dataloader: DataLoader, train_mode: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single epoch of training or evaluation.\n",
    "\n",
    "        :param dataloader: DataLoader providing the data for the epoch.\n",
    "        :param train_mode: Boolean flag indicating whether to train or evaluate.\n",
    "        :return: A tuple containing the average loss and a dictionary of average metrics.\n",
    "        \"\"\"\n",
    "        # Set the mode for the epoch (Training or Testing)\n",
    "        phase = 'Training' if train_mode else 'Testing'\n",
    "        self._model.train() if train_mode else self._model.eval()\n",
    "\n",
    "        losses, metrics_list = []\n",
    "\n",
    "        # Iterate over the data loader\n",
    "        for batch in tqdm(dataloader, desc=phase):\n",
    "            # Move batch elements to the appropriate device\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            loss, batch_metrics = self._train_batch(batch) if train_mode else self._evaluate_batch(batch)\n",
    "\n",
    "            # Accumulate the loss and metrics\n",
    "            losses.append(loss)\n",
    "            metrics_list.append(batch_metrics)\n",
    "\n",
    "        # Compute average loss and metrics for the epoch\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "\n",
    "        return avg_loss, avg_metrics\n"
   ],
   "id": "7753f6da9628de81",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.056860Z",
     "start_time": "2024-07-16T08:53:14.048385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    CrossValidator class for handling k-fold cross-validation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer: Trainer, train_data: TensorDataset, test_data: TensorDataset):\n",
    "        \"\"\"\n",
    "        Initialize the CrossValidator with trainer, training data, and test data.\n",
    "\n",
    "        :param trainer: An instance of the Trainer class.\n",
    "        :param train_data: The training dataset.\n",
    "        :param test_data: The test dataset.\n",
    "        \"\"\"\n",
    "        self.__trainer = trainer\n",
    "        self.__train_data = train_data\n",
    "        self.__test_data = test_data\n",
    "\n",
    "    def __train_and_evaluate(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a specified number of epochs.\n",
    "\n",
    "        :param train_dataloader: DataLoader for the training data.\n",
    "        :param test_dataloader: DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n --- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "            # Train the model and print training metrics\n",
    "            avg_train_loss, avg_train_metrics = self.__trainer.run_epoch(train_dataloader, train_mode=True)\n",
    "            print(f\"\\n TRAIN | Loss: {avg_train_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_train_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_train_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_train_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "            # Evaluate the model on the validation set and print validation metrics\n",
    "            avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "            print(f\" VALID | Loss: {avg_test_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "    def __evaluate_on_test_set(self, test_dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "\n",
    "        :param test_dataloader: DataLoader for the test data.\n",
    "        :return: A dictionary of test set metrics.\n",
    "        \"\"\"\n",
    "        avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "\n",
    "        # Print test set metrics\n",
    "        print(f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} |\"\n",
    "              f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "              f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "              f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "        return avg_test_metrics\n",
    "\n",
    "    def k_fold_cv(self, log_id: str = \"bert\") -> None:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        :param log_id: Identifier for logging purposes, typically the model name.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Iterate over each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.__train_data)):\n",
    "            # Create data loaders for training and validation sets\n",
    "            train_subsampler = Subset(self.__train_data, train_idx)\n",
    "            val_subsampler = Subset(self.__train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_subsampler,\n",
    "                sampler=RandomSampler(train_subsampler),\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subsampler,\n",
    "                batch_size=BATCH_SIZE  # No need for shuffling\n",
    "            )\n",
    "\n",
    "            print(f\"Starting Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "            # Train and evaluate the model for the current fold\n",
    "            self.__train_and_evaluate(train_loader, val_loader)\n",
    "\n",
    "            # Evaluate on the test set after each fold\n",
    "            metrics = self.__evaluate_on_test_set(DataLoader(self.__test_data, batch_size=BATCH_SIZE, shuffle=False))\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "        # Calculate average and standard deviation of each metric across all folds\n",
    "        metric_keys = fold_metrics[0].keys()  # Assuming all metrics dictionaries have the same structure\n",
    "        average_metrics = {key: np.mean([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "        std_dev_metrics = {key: np.std([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "\n",
    "        # Print average metrics and their standard deviations\n",
    "        print(\"Average Metrics Over All Folds:\")\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f\"{key}: {value:.4f} (±{std_dev_metrics[key]:.4f})\")\n",
    "\n",
    "        # Save metrics to CSV file\n",
    "        save_results(fold_metrics, filename=f\"{log_id}.csv\")\n"
   ],
   "id": "543bd399383e5b7e",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "9ae8b68dbe64b7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT",
   "id": "f2934d41ac3e7b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:14.064229Z",
     "start_time": "2024-07-16T08:53:14.057957Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERTModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BERTModelTrainer class for handling the training and evaluation of a BERT-based model.\n",
    "    Inherits from the Trainer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BERTModelTrainer with model, optimizer, and loss function.\n",
    "\n",
    "        :param model: The BERT model to be trained.\n",
    "        \"\"\"\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Initialize the optimizer with model parameters and a learning rate\n",
    "        self._optimizer = AdamW(self._model.parameters(), lr=LR)\n",
    "\n",
    "        # Define the loss function for binary classification with logits\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare the inputs for the model\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(**inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs.logits).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(batch[2].detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n"
   ],
   "id": "c2c756331f70649e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:44.296480Z",
     "start_time": "2024-07-16T08:53:14.065140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(BERT_MODEL_TYPE, num_labels=20, ignore_mismatched_sizes=True)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "model.to(DEVICE)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_TYPE, ignore_mismatched_sizes=True)\n",
    "\n",
    "x, y = tokenizer(\n",
    "    INPUTS,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "), LABELS\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x['input_ids'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Split attention masks for training and test sets\n",
    "train_masks, test_masks, _, _ = train_test_split(x['attention_mask'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_data = TensorDataset(x_train, train_masks, torch.tensor(y_train).float())\n",
    "test_data = TensorDataset(x_test, test_masks, torch.tensor(y_test).float())\n",
    "CrossValidator(BERTModelTrainer(model), train_data, test_data).k_fold_cv(log_id=\"bert\")"
   ],
   "id": "cfb9656b31564534",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.7066 | Precision: 0.1548, Recall: 0.5000, F1: 0.2361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.4882 | Precision: 1.0000, Recall: 0.6750, F1: 0.8036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.5146 | Precision: 0.8333, Recall: 0.5500, F1: 0.6607\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.5039 | Precision: 1.0000, Recall: 0.6750, F1: 0.8036\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.3668 | Precision: 0.6667, Recall: 0.5833, F1: 0.6190\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.3557 | Precision: 0.8333, Recall: 0.5500, F1: 0.6607\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.8333 (±0.0000)\n",
      "recall: 0.5500 (±0.0000)\n",
      "f1: 0.6607 (±0.0000)\n",
      "All fold results saved to 'bert.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feed Forward Neural Network",
   "id": "523fb217e9d16400"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:44.312058Z",
     "start_time": "2024-07-16T08:53:44.301400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Neural Network with three fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network layers.\n",
    "        \"\"\"\n",
    "        super(FFNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, NUM_LABELS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :return: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x\n"
   ],
   "id": "c35b14720c73a4bc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:46.024303Z",
     "start_time": "2024-07-16T08:53:44.315238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FFNNClassifier()\n",
    "\n",
    "x = torch.FloatTensor(VECTORIZER.fit_transform(INPUTS).toarray())\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"ffnn\")"
   ],
   "id": "785da51a7681dd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8886 | Precision: 0.1750, Recall: 1.0000, F1: 0.2971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8883 | Precision: 0.1750, Recall: 1.0000, F1: 0.2971\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8629 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8999 | Precision: 0.1500, Recall: 1.0000, F1: 0.2609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8615 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8616 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.2250 (±0.0000)\n",
      "recall: 1.0000 (±0.0000)\n",
      "f1: 0.3667 (±0.0000)\n",
      "All fold results saved to 'ffnn.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "212af9b00ac25811"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:46.036512Z",
     "start_time": "2024-07-16T08:53:46.026655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier for text classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pretrained_embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM Classifier.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of the embedding vectors.\n",
    "        :param hidden_dim: Dimension of the hidden layer.\n",
    "        :param pretrained_embeddings: Pretrained embeddings to initialize the embedding layer.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Optionally freeze the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, NUM_LABELS)\n",
    "\n",
    "    def forward(self, text: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM Classifier.\n",
    "\n",
    "        :param text: Input tensor containing text data.\n",
    "        :return: Output tensor after passing through the LSTM and fully connected layers.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(text.long())\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.fc(hidden)\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file.\n",
    "\n",
    "    :param glove_file: Path to the GloVe embeddings file.\n",
    "    :return: Dictionary mapping words to their corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Loading GloVe Embeddings\"):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n"
   ],
   "id": "f90bc4ba8d73a74f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:53:57.285148Z",
     "start_time": "2024-07-16T08:53:46.043090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_embeddings = load_glove_embeddings(os.path.join(\"..\", \"asset\", \"glove.6B.100d.txt\"))\n",
    "\n",
    "# Tokenization and vocabulary creation\n",
    "word_count = Counter(word for sentence in INPUTS for word in sentence.lower().split())\n",
    "vocabulary = {word: i + 1 for i, word in enumerate(word_count)}  # start indexing from 1\n",
    "vocabulary['<PAD>'] = 0  # Padding value\n",
    "\n",
    "# Embedding matrix creation\n",
    "embedding_dim = 100  # Dimensionality of GloVe embeddings used\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "for word, i in tqdm(vocabulary.items(), desc='Creating Embedding Matrix'):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "sequences = [[vocabulary[word] for word in text.lower().split()] for text in INPUTS]\n",
    "\n",
    "# Finding the longest sequence\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "seq_padded = [seq + [vocabulary['<PAD>']] * (max_seq_len - len(seq)) for seq in sequences]"
   ],
   "id": "ed53b7c6c9f480c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embeddings: 400000it [00:08, 48500.56it/s]\n",
      "Creating Embedding Matrix: 100%|██████████| 257/257 [00:00<00:00, 115398.37it/s]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:56:39.140581Z",
     "start_time": "2024-07-16T08:56:34.829175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LSTMClassifier(len(vocabulary), embedding_dim, 64, embedding_matrix)\n",
    "\n",
    "x_tensor = torch.FloatTensor(seq_padded)\n",
    "y_tensor = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"lstm\")"
   ],
   "id": "6086b0ad8cb21eb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8965 | Precision: 0.1500, Recall: 1.0000, F1: 0.2609\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8707 | Precision: 0.2000, Recall: 1.0000, F1: 0.3304\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8594 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Starting Fold 2/2\n",
      "\n",
      " --- Epoch 1/1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TRAIN | Loss: 0.8707 | Precision: 0.2000, Recall: 1.0000, F1: 0.3333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/110 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " VALID | Loss: 0.8706 | Precision: 0.2000, Recall: 1.0000, F1: 0.3333\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/56 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Evaluation | Loss: 0.8587 | Precision: 0.2250, Recall: 1.0000, F1: 0.3667\n",
      "\n",
      "Average Metrics Over All Folds:\n",
      "precision: 0.2250 (±0.0000)\n",
      "recall: 1.0000 (±0.0000)\n",
      "f1: 0.3667 (±0.0000)\n",
      "All fold results saved to 'lstm.csv'\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SVM, Random Forest, Gradient Boosting",
   "id": "ccd9856e5cf86718"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:56:45.903285Z",
     "start_time": "2024-07-16T08:56:45.890548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassifiersPoolEvaluator:\n",
    "    \"\"\"\n",
    "    ClassifiersPoolEvaluator class for evaluating a pool of classifiers using TF-IDF features and k-fold cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifiersPoolEvaluator with TF-IDF vectorizer and a dictionary of classifiers.\n",
    "        \"\"\"\n",
    "        # Create a TF-IDF vectorizer with a maximum number of features defined by MAX_FEATURES\n",
    "        self.vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "\n",
    "        # Define a dictionary of classifiers to evaluate\n",
    "        self.classifiers = {\n",
    "            \"svm\": OneVsRestClassifier(SVC(kernel='linear', probability=True)),\n",
    "            \"random_forest\": OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)),\n",
    "            \"gradient_boosting\": OneVsRestClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3))\n",
    "        }\n",
    "\n",
    "        # Transform the documents into TF-IDF features\n",
    "        self.X = self.vectorizer.fit_transform(INPUTS)\n",
    "\n",
    "        # Transform the labels using MultiLabelBinarizer\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.y = self.mlb.fit_transform(LABELS)\n",
    "\n",
    "    def __evaluate_fold(self, classifier: OneVsRestClassifier, train_index: List[int], test_index: List[int]) -> Dict[\n",
    "        str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a classifier on a single fold of cross-validation.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :param train_index: Indices for the training data.\n",
    "        :param test_index: Indices for the test data.\n",
    "        :return: A dictionary of computed metrics.\n",
    "        \"\"\"\n",
    "        X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "        y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "        # Make predictions on the test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "\n",
    "        # Compute metrics using the provided utility function\n",
    "        return compute_metrics(y_test, predictions)\n",
    "\n",
    "    def __k_fold_cv(self, classifier: OneVsRestClassifier) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation on a given classifier.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :return: A DataFrame containing the results of each fold.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "        # Evaluate the classifier on each fold and collect the results\n",
    "        results = [self.__evaluate_fold(classifier, train_index, test_index) for train_index, test_index in\n",
    "                   kf.split(self.X)]\n",
    "        # Return the results as a DataFrame\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def pool_evaluation(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the evaluation for each classifier defined in self.classifiers.\n",
    "        \"\"\"\n",
    "        # Run the evaluation for each classifier defined in self.classifiers\n",
    "        for classifier_name, classifier in self.classifiers.items():\n",
    "            print(f\"\\nTesting classifier: {classifier_name}\")\n",
    "            # Evaluate the classifier and get the metrics DataFrame\n",
    "            metrics_df = self.__k_fold_cv(classifier)\n",
    "            # Save the results using the provided utility function\n",
    "            save_results(metrics_df, f\"{classifier_name}.csv\")\n",
    "            # Print the results\n",
    "            print(f\"Results for {classifier_name}:\\n{metrics_df}\\n\")\n"
   ],
   "id": "cd228b01fa44096f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-16T08:56:50.840437Z",
     "start_time": "2024-07-16T08:56:46.755861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluator = ClassifiersPoolEvaluator()\n",
    "evaluator.pool_evaluation()"
   ],
   "id": "4f3360bdb68dd76d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing classifier: svm\n",
      "All fold results saved to 'svm.csv'\n",
      "Results for svm:\n",
      "   precision  recall        f1\n",
      "0   0.998188     1.0  0.998792\n",
      "1   1.000000     1.0  1.000000\n",
      "\n",
      "\n",
      "Testing classifier: random_forest\n",
      "All fold results saved to 'random_forest.csv'\n",
      "Results for random_forest:\n",
      "   precision  recall        f1\n",
      "0   0.998188     1.0  0.998792\n",
      "1   1.000000     1.0  1.000000\n",
      "\n",
      "\n",
      "Testing classifier: gradient_boosting\n",
      "All fold results saved to 'gradient_boosting.csv'\n",
      "Results for gradient_boosting:\n",
      "   precision  recall        f1\n",
      "0   0.998188     1.0  0.998792\n",
      "1   1.000000     1.0  1.000000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
