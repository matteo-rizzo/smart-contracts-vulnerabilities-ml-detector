{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "172880abc3d187d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:12.252075Z",
     "start_time": "2024-07-26T14:34:12.218076Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Tuple, List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import Subset, RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc1ec8b396d0a08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:12.325276Z",
     "start_time": "2024-07-26T14:34:12.293653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DATASET     ASSESSMENTS\n",
    "CodeSmells        10395\n",
    "Zeus               7315\n",
    "eThor               702\n",
    "ContractFuzzer      367\n",
    "SolidiFI            343\n",
    "EverEvolvingG       292\n",
    "Doublade            276\n",
    "NPChecker           212\n",
    "JiuZhou             165\n",
    "SBcurated           129\n",
    "SWCregistry         116\n",
    "EthRacer            109\n",
    "NotSoSmartC          34\n",
    "\n",
    "Subset dataset to consider within CGT\n",
    "\"\"\"\n",
    "\n",
    "SUBSET = \"Zeus\"\n",
    "print(f\"Subset of CGT: {SUBSET}\")\n",
    "\n",
    "# Setting the device for torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device {DEVICE}\")\n",
    "\n",
    "# Setting random seeds for reproducibility\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "print(f\"Random seeds set to {RANDOM_SEED}\")\n",
    "\n",
    "# Path to dataset\n",
    "PATH_TO_DATASET = os.path.join(\"..\", \"dataset\", \"cgt\")\n",
    "print(f\"Path to dataset: {PATH_TO_DATASET}\")\n",
    "\n",
    "# Model and training configurations\n",
    "BERT_MODEL_TYPE = 'microsoft/codebert-base'\n",
    "print(f\"BERT model type: {BERT_MODEL_TYPE}\")\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "print(f\"Max features: {MAX_FEATURES}\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "NUM_FOLDS = 10\n",
    "print(f\"Number of folds: {NUM_FOLDS}\")\n",
    "\n",
    "NUM_EPOCHS = 25\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "NUM_LABELS = 20  # 7 for Zeus, 20 for CodeSmell, 160 overall\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "\n",
    "LR = 0.001\n",
    "print(f\"Learning rate: {LR}\")\n",
    "\n",
    "TEST_SIZE = 0.1\n",
    "print(f\"Test size: {TEST_SIZE}\")\n",
    "\n",
    "# File configurations\n",
    "FILE_TYPE = \"source\"  # Can be 'source', 'runtime', 'bytecode'\n",
    "FILE_EXT = None\n",
    "if FILE_TYPE == \"source\":\n",
    "    FILE_EXT = \".sol\"\n",
    "elif FILE_TYPE == \"runtime\":\n",
    "    FILE_EXT = \".rt.hex\"\n",
    "elif FILE_TYPE == \"bytecode\":\n",
    "    FILE_EXT = \".hex\"\n",
    "FILE_ID = \"sol\"  # Can be 'sol ,'sol2' for 'source', otherwise 'runtime', 'bytecode'\n",
    "\n",
    "print(f\"File type: {FILE_TYPE}\")\n",
    "print(f\"File extension: {FILE_EXT}\")\n",
    "print(f\"File ID: {FILE_ID}\")\n",
    "\n",
    "# Creating the log directory if it doesn't exist\n",
    "LOG_DIR = os.path.join(\"log\", FILE_TYPE)\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "    print(f\"Log directory created at {LOG_DIR}\")\n",
    "else:\n",
    "    print(f\"Log directory already exists at {LOG_DIR}\")"
   ],
   "id": "1374f63db7c6cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of CGT: Zeus\n",
      "Running on device cpu\n",
      "Random seeds set to 0\n",
      "Path to dataset: ../dataset/cgt\n",
      "BERT model type: microsoft/codebert-base\n",
      "Max features: 500\n",
      "Batch size: 1\n",
      "Number of folds: 10\n",
      "Number of epochs: 25\n",
      "Number of labels: 20\n",
      "Learning rate: 0.001\n",
      "Test size: 0.1\n",
      "File type: source\n",
      "File extension: .sol\n",
      "File ID: sol\n",
      "Log directory already exists at log/source\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset",
   "id": "a424b60aff524ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "24ae2bd4455509d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def preprocess_hex(hex_data: str) -> str:\n",
    "    # Reads a hex file and converts it to a byte string\n",
    "    byte_data = bytes.fromhex(hex_data.strip())\n",
    "\n",
    "    # Convert byte data to a readable ASCII string, ignoring non-ASCII characters\n",
    "    return ' '.join(f'{byte:02x}' for byte in byte_data)\n",
    "\n",
    "\n",
    "def preprocess_solidity_code(code: str) -> str:\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove blank lines (lines only containing whitespace)\n",
    "    lines = code.split('\\n')\n",
    "    non_blank_lines = [line for line in lines if line.strip() != '']\n",
    "    code = '\\n'.join(non_blank_lines)\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "def preprocess(data: str) -> str:\n",
    "    return preprocess_solidity_code(data) if FILE_TYPE == \"source\" else preprocess_hex(data)"
   ],
   "id": "41e2ed4561254c43",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labels Management",
   "id": "7e084d9053915673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:12.494019Z",
     "start_time": "2024-07-26T14:34:12.486983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_inputs_and_gt(data: pd.DataFrame) -> Tuple:\n",
    "    \"\"\"\n",
    "    Initialize inputs, labels, and groundtruth (gt) from the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :return: A tuple containing the list of inputs, labels dictionary, and gt dictionary.\n",
    "    \"\"\"\n",
    "    inputs, labels, gt = {}, {}, {}\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Initializing inputs and groundtruth data\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Initialize the documents\n",
    "            inputs[item_id] = preprocess(open(path_to_file, 'r', encoding=\"utf8\").read())\n",
    "\n",
    "            # Initialize the label\n",
    "            labels[item_id] = [0] * NUM_LABELS\n",
    "\n",
    "            # Initialize the groundtruth\n",
    "            prop = row[\"property\"].lower()\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "\n",
    "    return list(inputs.values()), labels, gt\n",
    "\n",
    "\n",
    "def set_labels(data: pd.DataFrame, labels: Dict, gt: Dict) -> List:\n",
    "    \"\"\"\n",
    "    Set up the labels based on the groundtruth (gt) for the given data.\n",
    "\n",
    "    :param data: A pandas DataFrame containing the data to process.\n",
    "    :param labels: A dictionary where keys are item IDs and values are lists representing labels.\n",
    "    :param gt: A dictionary where keys are properties and values are their corresponding indices.\n",
    "    :return: A list of labels values.\n",
    "    \"\"\"\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Setting up the labels\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Set label   \n",
    "            prop = row[\"property\"].lower()\n",
    "            if row['property_holds'] == 't':\n",
    "                labels[item_id][gt[prop]] = 1\n",
    "\n",
    "    return list(labels.values())\n"
   ],
   "id": "1511840b0951d215",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization of the dataset",
   "id": "bac1ca9644eca1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:13.413411Z",
     "start_time": "2024-07-26T14:34:12.549157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the dataset from CSV\n",
    "dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "\n",
    "# Exclude outliers from the dataset\n",
    "dataset = dataset[dataset[\"dataset\"] == SUBSET]\n",
    "\n",
    "# Initialize the documents and the groundtruth\n",
    "print(\"\\nInitializing the documents and the ground truth...\")\n",
    "INPUTS, LABELS, gt = init_inputs_and_gt(dataset)\n",
    "\n",
    "# Set the labels for the multilabel classification problem\n",
    "print(\"Setting the labels for the multilabel classification problem...\")\n",
    "LABELS = set_labels(dataset, LABELS, gt)\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "print(\"Initializing the TF-IDF vectorizer...\")\n",
    "VECTORIZER = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "print(\"Initialization complete!\")"
   ],
   "id": "fb254a69e43e824e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing the documents and the ground truth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing inputs and groundtruth data: 7315it [00:00, 14252.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the labels for the multilabel classification problem...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up the labels: 7315it [00:00, 44192.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the TF-IDF vectorizer...\n",
      "Initialization complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cross Validation",
   "id": "546491e26d424c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:13.420517Z",
     "start_time": "2024-07-26T14:34:13.415859Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels: List[Any], pred_labels: List[Any]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute evaluation metrics for the given true and predicted labels.\n",
    "\n",
    "    :param true_labels: The ground truth labels.\n",
    "    :param pred_labels: The predicted labels.\n",
    "    :return: A dictionary containing precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"recall\": recall_score(true_labels, pred_labels, average='samples', zero_division=0),\n",
    "        \"f1\": f1_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    }\n",
    "\n",
    "\n",
    "def save_results(results: List[Dict[str, Any]], filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the results to a CSV file.\n",
    "\n",
    "    :param results: The results to save, typically a list of dictionaries.\n",
    "    :param filename: The name of the file to save the results to.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(os.path.join(LOG_DIR, filename), index=False)\n",
    "    print(f\"All fold results saved to '{os.path.join(LOG_DIR, filename)}'\")\n"
   ],
   "id": "795c276e2fb6e8b1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:13.429681Z",
     "start_time": "2024-07-26T14:34:13.421494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Trainer class for handling the training and evaluation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the trainer with model, loss criterion, and optimizer.\n",
    "\n",
    "        :param model: The neural network model to be trained.\n",
    "        \"\"\"\n",
    "        self.__untrained_model = model\n",
    "        self._model = model.to(DEVICE)\n",
    "        self._optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "        self._loss_fn = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "\n",
    "    def reset_model(self):\n",
    "        self._model = self.__untrained_model\n",
    "        self._optimizer = optim.Adam(self.__untrained_model.parameters(), lr=LR)\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Move batch elements to the appropriate device (CPU/GPU)\n",
    "        batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "        # Prepare the inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(labels.cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input data and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs, labels = batch\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(labels.detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def run_epoch(self, dataloader: DataLoader, train_mode: bool = True) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Run a single epoch of training or evaluation.\n",
    "\n",
    "        :param dataloader: DataLoader providing the data for the epoch.\n",
    "        :param train_mode: Boolean flag indicating whether to train or evaluate.\n",
    "        :return: A tuple containing the average loss and a dictionary of average metrics.\n",
    "        \"\"\"\n",
    "        # Set the mode for the epoch (Training or Testing)\n",
    "        phase = 'Training' if train_mode else 'Testing'\n",
    "        self._model.train() if train_mode else self._model.eval()\n",
    "\n",
    "        losses, metrics_list = [], []\n",
    "\n",
    "        # Iterate over the data loader\n",
    "        for batch in tqdm(dataloader, desc=phase):\n",
    "            # Move batch elements to the appropriate device\n",
    "            batch = tuple(b.to(DEVICE) for b in batch)\n",
    "\n",
    "            loss, batch_metrics = self._train_batch(batch) if train_mode else self._evaluate_batch(batch)\n",
    "\n",
    "            # Accumulate the loss and metrics\n",
    "            losses.append(loss)\n",
    "            metrics_list.append(batch_metrics)\n",
    "\n",
    "        # Compute average loss and metrics for the epoch\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_metrics = {metric: np.mean([m[metric] for m in metrics_list]) for metric in metrics_list[0]}\n",
    "\n",
    "        return avg_loss, avg_metrics\n"
   ],
   "id": "7753f6da9628de81",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:13.447587Z",
     "start_time": "2024-07-26T14:34:13.432369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CrossValidator:\n",
    "    \"\"\"\n",
    "    CrossValidator class for handling k-fold cross-validation of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer: Trainer, train_data: TensorDataset, test_data: TensorDataset):\n",
    "        \"\"\"\n",
    "        Initialize the CrossValidator with trainer, training data, and test data.\n",
    "\n",
    "        :param trainer: An instance of the Trainer class.\n",
    "        :param train_data: The training dataset.\n",
    "        :param test_data: The test dataset.\n",
    "        \"\"\"\n",
    "        self.__trainer = trainer\n",
    "        self.__train_data = train_data\n",
    "        self.__test_data = test_data\n",
    "\n",
    "    def __train_and_evaluate(self, train_dataloader: DataLoader, test_dataloader: DataLoader) -> None:\n",
    "        \"\"\"\n",
    "        Train and evaluate the model for a specified number of epochs.\n",
    "\n",
    "        :param train_dataloader: DataLoader for the training data.\n",
    "        :param test_dataloader: DataLoader for the validation data.\n",
    "        \"\"\"\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            print(f\"\\n --- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "            # Train the model and print training metrics\n",
    "            avg_train_loss, avg_train_metrics = self.__trainer.run_epoch(train_dataloader, train_mode=True)\n",
    "            print(f\"\\n TRAIN | Loss: {avg_train_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_train_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_train_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_train_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "            # Evaluate the model on the validation set and print validation metrics\n",
    "            avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "            print(f\" VALID | Loss: {avg_test_loss:.4f} |\"\n",
    "                  f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "                  f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "                  f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "    def __evaluate_on_test_set(self, test_dataloader: DataLoader) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate the model on the test set.\n",
    "\n",
    "        :param test_dataloader: DataLoader for the test data.\n",
    "        :return: A dictionary of test set metrics.\n",
    "        \"\"\"\n",
    "        avg_test_loss, avg_test_metrics = self.__trainer.run_epoch(test_dataloader, train_mode=False)\n",
    "\n",
    "        # Print test set metrics\n",
    "        print(f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} |\"\n",
    "              f\" Precision: {avg_test_metrics['precision']:.4f},\"\n",
    "              f\" Recall: {avg_test_metrics['recall']:.4f},\"\n",
    "              f\" F1: {avg_test_metrics['f1']:.4f}\\n\")\n",
    "\n",
    "        return avg_test_metrics\n",
    "\n",
    "    def k_fold_cv(self, log_id: str = \"bert\") -> None:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation.\n",
    "\n",
    "        :param log_id: Identifier for logging purposes, typically the model name.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True)\n",
    "        fold_metrics = []\n",
    "\n",
    "        # Iterate over each fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(self.__train_data)):\n",
    "            # Create data loaders for training and validation sets\n",
    "            train_subsampler = Subset(self.__train_data, train_idx)\n",
    "            val_subsampler = Subset(self.__train_data, val_idx)\n",
    "\n",
    "            train_loader = DataLoader(\n",
    "                train_subsampler,\n",
    "                sampler=RandomSampler(train_subsampler),\n",
    "                batch_size=BATCH_SIZE\n",
    "            )\n",
    "            val_loader = DataLoader(\n",
    "                val_subsampler,\n",
    "                batch_size=BATCH_SIZE  # No need for shuffling\n",
    "            )\n",
    "\n",
    "            print(f\"Starting Fold {fold + 1}/{NUM_FOLDS}\")\n",
    "\n",
    "            # Train and evaluate the model for the current fold\n",
    "            self.__train_and_evaluate(train_loader, val_loader)\n",
    "\n",
    "            # Evaluate on the test set after each fold\n",
    "            metrics = self.__evaluate_on_test_set(DataLoader(self.__test_data, batch_size=BATCH_SIZE, shuffle=False))\n",
    "            fold_metrics.append(metrics)\n",
    "\n",
    "            # Reset the model to untrained\n",
    "            self.__trainer.reset_model()\n",
    "\n",
    "        # Calculate average and standard deviation of each metric across all folds\n",
    "        metric_keys = fold_metrics[0].keys()  # Assuming all metrics dictionaries have the same structure\n",
    "        average_metrics = {key: np.mean([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "        std_dev_metrics = {key: np.std([metric[key] for metric in fold_metrics]) for key in metric_keys}\n",
    "\n",
    "        # Print average metrics and their standard deviations\n",
    "        print(\"Average Metrics Over All Folds:\")\n",
    "        for key, value in average_metrics.items():\n",
    "            print(f\"{key}: {value:.4f} (±{std_dev_metrics[key]:.4f})\")\n",
    "\n",
    "        # Save metrics to CSV file\n",
    "        save_results(fold_metrics, filename=f\"{log_id}.csv\")\n"
   ],
   "id": "543bd399383e5b7e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "9ae8b68dbe64b7ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERT",
   "id": "f2934d41ac3e7b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:13.459436Z",
     "start_time": "2024-07-26T14:34:13.450104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERTModelTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    BERTModelTrainer class for handling the training and evaluation of a BERT-based model.\n",
    "    Inherits from the Trainer class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize the BERTModelTrainer with model, optimizer, and loss function.\n",
    "\n",
    "        :param model: The BERT model to be trained.\n",
    "        \"\"\"\n",
    "        super().__init__(model)\n",
    "\n",
    "        # Initialize the optimizer with model parameters and a learning rate\n",
    "        self._optimizer = AdamW(self._model.parameters(), lr=LR)\n",
    "\n",
    "    def _evaluate_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Evaluate a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare the inputs for the model\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        # Disable gradient computation for evaluation\n",
    "        with torch.no_grad():\n",
    "            outputs = self._model(**inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "            # Make predictions and compute batch metrics\n",
    "            predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "\n",
    "        # Return the loss and metrics\n",
    "        return loss.item(), batch_metrics\n",
    "\n",
    "    def _train_batch(self, batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Train a single batch of data.\n",
    "\n",
    "        :param batch: A tuple containing input_ids, attention_mask, and labels.\n",
    "        :return: A tuple containing the loss and a dictionary of metrics.\n",
    "        \"\"\"\n",
    "        # Prepare inputs for the model\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        self._model.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self._model(**inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = self._loss_fn(outputs.logits, inputs['labels'])\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "\n",
    "        # Make predictions and compute metrics\n",
    "        predictions = torch.sigmoid(outputs.logits).round().detach().cpu().numpy()\n",
    "        batch_metrics = compute_metrics(batch[2].detach().cpu().numpy(), predictions)\n",
    "\n",
    "        return loss.item(), batch_metrics\n"
   ],
   "id": "c2c756331f70649e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:42.777958Z",
     "start_time": "2024-07-26T14:34:13.461721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(BERT_MODEL_TYPE, num_labels=NUM_LABELS,\n",
    "                                                         ignore_mismatched_sizes=True)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "model.to(DEVICE)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_TYPE, ignore_mismatched_sizes=True)\n",
    "\n",
    "x, y = tokenizer(\n",
    "    INPUTS,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    "), LABELS\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x['input_ids'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Split attention masks for training and test sets\n",
    "train_masks, test_masks, _, _ = train_test_split(x['attention_mask'], y, test_size=TEST_SIZE)\n",
    "\n",
    "# Create datasets for training and testing\n",
    "train_data = TensorDataset(x_train, train_masks, torch.tensor(y_train).float())\n",
    "test_data = TensorDataset(x_test, test_masks, torch.tensor(y_test).float())\n",
    "CrossValidator(BERTModelTrainer(model), train_data, test_data).k_fold_cv(log_id=\"bert\")"
   ],
   "id": "cfb9656b31564534",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1/10\n",
      "\n",
      " --- Epoch 1/25 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 11/761 [00:22<25:10,  2.01s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m train_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_train, train_masks, torch\u001B[38;5;241m.\u001B[39mtensor(y_train)\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m     27\u001B[0m test_data \u001B[38;5;241m=\u001B[39m TensorDataset(x_test, test_masks, torch\u001B[38;5;241m.\u001B[39mtensor(y_test)\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[0;32m---> 28\u001B[0m \u001B[43mCrossValidator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mBERTModelTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_data\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mk_fold_cv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlog_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbert\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[18], line 87\u001B[0m, in \u001B[0;36mCrossValidator.k_fold_cv\u001B[0;34m(self, log_id)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_FOLDS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# Train and evaluate the model for the current fold\u001B[39;00m\n\u001B[0;32m---> 87\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__train_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# Evaluate on the test set after each fold\u001B[39;00m\n\u001B[1;32m     90\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__evaluate_on_test_set(DataLoader(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__test_data, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m))\n",
      "Cell \u001B[0;32mIn[18], line 29\u001B[0m, in \u001B[0;36mCrossValidator.__train_and_evaluate\u001B[0;34m(self, train_dataloader, test_dataloader)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m --- Epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mNUM_EPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Train the model and print training metrics\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m avg_train_loss, avg_train_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__trainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m TRAIN | Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m |\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     31\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Precision: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprecision\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     32\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Recall: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrecall\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     33\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mavg_train_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Evaluate the model on the validation set and print validation metrics\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[17], line 96\u001B[0m, in \u001B[0;36mTrainer.run_epoch\u001B[0;34m(self, dataloader, train_mode)\u001B[0m\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m tqdm(dataloader, desc\u001B[38;5;241m=\u001B[39mphase):\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;66;03m# Move batch elements to the appropriate device\u001B[39;00m\n\u001B[1;32m     94\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(b\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch)\n\u001B[0;32m---> 96\u001B[0m     loss, batch_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m train_mode \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_evaluate_batch(batch)\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;66;03m# Accumulate the loss and metrics\u001B[39;00m\n\u001B[1;32m     99\u001B[0m     losses\u001B[38;5;241m.\u001B[39mappend(loss)\n",
      "Cell \u001B[0;32mIn[19], line 56\u001B[0m, in \u001B[0;36mBERTModelTrainer._train_batch\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 56\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# Compute the loss\u001B[39;00m\n\u001B[1;32m     59\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss_fn(outputs\u001B[38;5;241m.\u001B[39mlogits, inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1195\u001B[0m, in \u001B[0;36mRobertaForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m   1187\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1188\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m   1189\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1193\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m-> 1195\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1196\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1206\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1207\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:832\u001B[0m, in \u001B[0;36mRobertaModel.forward\u001B[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    823\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m    825\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[1;32m    826\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[1;32m    827\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    830\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[1;32m    831\u001B[0m )\n\u001B[0;32m--> 832\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    834\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    835\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    836\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    839\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    840\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    841\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    842\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    844\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    845\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:521\u001B[0m, in \u001B[0;36mRobertaEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    510\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    511\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    512\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    518\u001B[0m         output_attentions,\n\u001B[1;32m    519\u001B[0m     )\n\u001B[1;32m    520\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 521\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    529\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:410\u001B[0m, in \u001B[0;36mRobertaLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    399\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    400\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    407\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    408\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[1;32m    409\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 410\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    419\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:346\u001B[0m, in \u001B[0;36mRobertaAttention.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[1;32m    327\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    328\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    329\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    335\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    336\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m    337\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself(\n\u001B[1;32m    338\u001B[0m         hidden_states,\n\u001B[1;32m    339\u001B[0m         attention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    344\u001B[0m         output_attentions,\n\u001B[1;32m    345\u001B[0m     )\n\u001B[0;32m--> 346\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mself_outputs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:289\u001B[0m, in \u001B[0;36mRobertaSelfOutput.forward\u001B[0;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor, input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    288\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[0;32m--> 289\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states \u001B[38;5;241m+\u001B[39m input_tensor)\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001B[0m, in \u001B[0;36mDropout.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/venv/lib/python3.10/site-packages/torch/nn/functional.py:1295\u001B[0m, in \u001B[0;36mdropout\u001B[0;34m(input, p, training, inplace)\u001B[0m\n\u001B[1;32m   1293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m p \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0.0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m p \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1.0\u001B[39m:\n\u001B[1;32m   1294\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdropout probability has to be between 0 and 1, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mp\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _VF\u001B[38;5;241m.\u001B[39mdropout_(\u001B[38;5;28minput\u001B[39m, p, training) \u001B[38;5;28;01mif\u001B[39;00m inplace \u001B[38;5;28;01melse\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FFNN",
   "id": "523fb217e9d16400"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T14:34:42.780154Z",
     "start_time": "2024-07-26T14:34:42.780100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFNNClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Neural Network with three fully connected layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the network layers.\n",
    "        \"\"\"\n",
    "        super(FFNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(MAX_FEATURES, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, NUM_LABELS)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "\n",
    "        :param x: Input tensor\n",
    "        :return: Output tensor\n",
    "        \"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "id": "c35b14720c73a4bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = FFNNClassifier()\n",
    "\n",
    "x = torch.FloatTensor(VECTORIZER.fit_transform(INPUTS).toarray())\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"ffnn\")"
   ],
   "id": "785da51a7681dd6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LSTM",
   "id": "212af9b00ac25811"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Classifier for text classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, pretrained_embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM Classifier.\n",
    "\n",
    "        :param vocab_size: Size of the vocabulary.\n",
    "        :param embedding_dim: Dimension of the embedding vectors.\n",
    "        :param hidden_dim: Dimension of the hidden layer.\n",
    "        :param pretrained_embeddings: Pretrained embeddings to initialize the embedding layer.\n",
    "        \"\"\"\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Optionally freeze the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, NUM_LABELS)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the LSTM Classifier.\n",
    "\n",
    "        :param x: Input tensor.\n",
    "        :return: Output tensor after passing through the LSTM and fully connected layers.\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x.long())\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden[-1]  # Get the last layer's hidden state\n",
    "\n",
    "        output = self.fc(hidden)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def load_glove_embeddings(glove_file: str) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file.\n",
    "\n",
    "    :param glove_file: Path to the GloVe embeddings file.\n",
    "    :return: Dictionary mapping words to their corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Loading GloVe Embeddings\"):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ],
   "id": "f90bc4ba8d73a74f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tokenization and vocabulary creation\n",
    "word_count = Counter(word for sentence in INPUTS for word in sentence.lower().split())\n",
    "vocabulary = {word: i + 1 for i, word in enumerate(word_count)}  # start indexing from 1\n",
    "vocabulary['<PAD>'] = 0  # Padding value\n",
    "\n",
    "# Load embeddings\n",
    "glove_embeddings = load_glove_embeddings(os.path.join(\"..\", \"asset\", \"glove.6B.100d.txt\"))\n",
    "\n",
    "# Embedding matrix creation\n",
    "embedding_dim = 100  # Dimensionality of GloVe embeddings used\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "for word, i in tqdm(vocabulary.items(), desc='Creating Embedding Matrix'):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "sequences = [[vocabulary.get(word, vocabulary['<PAD>']) for word in text.lower().split()] for text in INPUTS]\n",
    "\n",
    "# Finding the longest sequence\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "seq_padded = [seq + [vocabulary['<PAD>']] * (max_seq_len - len(seq)) for seq in sequences]\n"
   ],
   "id": "ed53b7c6c9f480c9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = LSTMClassifier(len(vocabulary), embedding_dim, 8, embedding_matrix)\n",
    "\n",
    "x = torch.FloatTensor(seq_padded)\n",
    "y = torch.FloatTensor(LABELS)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "CrossValidator(Trainer(model), train_data, test_data).k_fold_cv(log_id=\"lstm\")"
   ],
   "id": "6086b0ad8cb21eb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVM, Random Forest, Gradient Boosting",
   "id": "ccd9856e5cf86718"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class ClassifiersPoolEvaluator:\n",
    "    \"\"\"\n",
    "    ClassifiersPoolEvaluator class for evaluating a pool of classifiers using TF-IDF features and k-fold cross-validation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifiersPoolEvaluator with TF-IDF vectorizer and a dictionary of classifiers.\n",
    "        \"\"\"\n",
    "        # Define a dictionary of classifiers to evaluate\n",
    "        self.__classifiers = {\n",
    "            \"svm\": OneVsRestClassifier(SVC(kernel='linear', probability=True)),\n",
    "            \"random_forest\": OneVsRestClassifier(RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED)),\n",
    "            \"gradient_boosting\": OneVsRestClassifier(\n",
    "                GradientBoostingClassifier(n_estimators=100, learning_rate=LR, max_depth=3)),\n",
    "            \"logistic_regression\": OneVsRestClassifier(LogisticRegression(random_state=RANDOM_SEED)),\n",
    "            \"knn\": OneVsRestClassifier(KNeighborsClassifier(n_neighbors=5)),\n",
    "            \"xgboost\": OneVsRestClassifier(\n",
    "                XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_SEED))\n",
    "        }\n",
    "\n",
    "        # Transform the documents into TF-IDF features\n",
    "        self.X = VECTORIZER.fit_transform(INPUTS)\n",
    "\n",
    "        # Transform the labels into a numpy array\n",
    "        self.y = np.array(LABELS)\n",
    "\n",
    "    def __evaluate_fold(self, classifier: OneVsRestClassifier, train_index: List[int], test_index: List[int],\n",
    "                        fold_num: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate a classifier on a single fold of cross-validation.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :param train_index: Indices for the training data.\n",
    "        :param test_index: Indices for the test data.\n",
    "        :param fold_num: The fold number.\n",
    "        :return: A dictionary of computed metrics.\n",
    "        \"\"\"\n",
    "        X_train, X_test = self.X[train_index], self.X[test_index]\n",
    "        y_train, y_test = self.y[train_index], self.y[test_index]\n",
    "\n",
    "        # Train the classifier on the training data\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        predictions = classifier.predict(X_test)\n",
    "\n",
    "        # Compute metrics using the provided utility function\n",
    "        metrics = compute_metrics(y_test, predictions)\n",
    "        print(f\"Results for fold {fold_num} | \"\n",
    "              f\"Precision: {metrics['precision']:.4f}, \"\n",
    "              f\"Recall: {metrics['recall']:.4f}, \"\n",
    "              f\"F1: {metrics['f1']:.4f}\")\n",
    "        return metrics\n",
    "\n",
    "    def __k_fold_cv(self, classifier: OneVsRestClassifier) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Perform k-fold cross-validation on a given classifier.\n",
    "\n",
    "        :param classifier: The classifier to be evaluated.\n",
    "        :return: A DataFrame containing the results of each fold.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "        # Evaluate the classifier on each fold and collect the results\n",
    "        results = []\n",
    "        for fold_num, (train_index, test_index) in enumerate(kf.split(self.X), 1):\n",
    "            metrics = self.__evaluate_fold(classifier, train_index, test_index, fold_num)\n",
    "            results.append(metrics)\n",
    "        # Return the results as a DataFrame\n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "    def pool_evaluation(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the evaluation for each classifier defined in self.__classifiers.\n",
    "        \"\"\"\n",
    "        # Run the evaluation for each classifier defined in self.__classifiers\n",
    "        for classifier_name, classifier in self.__classifiers.items():\n",
    "            print(f\"\\nTesting classifier: {classifier_name}\\n\")\n",
    "            # Evaluate the classifier and get the metrics DataFrame\n",
    "            metrics_df = self.__k_fold_cv(classifier)\n",
    "            # Save the results using the provided utility function\n",
    "            save_results(metrics_df, f\"{classifier_name}.csv\")\n"
   ],
   "id": "cd228b01fa44096f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "evaluator = ClassifiersPoolEvaluator()\n",
    "evaluator.pool_evaluation()"
   ],
   "id": "4f3360bdb68dd76d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ea9456f0a6f32bd2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
