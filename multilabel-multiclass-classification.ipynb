{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "172880abc3d187d8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.582307Z",
     "start_time": "2024-04-25T15:47:24.564840Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torch.utils.data import RandomSampler\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.warn = warn"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Configuration",
   "id": "fc1ec8b396d0a08e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.667334Z",
     "start_time": "2024-04-25T15:47:24.657942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device {DEVICE}\")\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "PATH_TO_DATASET = os.path.join(\"dataset\")\n",
    "BERT_MODEL_TYPE = 'microsoft/codebert-base'\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 30\n",
    "NUM_LABELS = 20\n",
    "LR = 0.001\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "FILE_TYPE = \"source\"\n",
    "FILE_EXT = \".sol\"\n",
    "FILE_ID = \"sol\""
   ],
   "id": "1374f63db7c6cea9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device cpu\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset\n",
    "\n",
    "Create PyTorch dataset feeding either source code, bytecode or runtime to the models."
   ],
   "id": "a424b60aff524ded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "24ae2bd4455509d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.671358Z",
     "start_time": "2024-04-25T15:47:24.668434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_hex(hex_data: str) -> str:\n",
    "    # Reads a hex file and converts it to a byte string\n",
    "    byte_data = bytes.fromhex(hex_data.strip())\n",
    "\n",
    "    # Convert byte data to a readable ASCII string, ignoring non-ASCII characters\n",
    "    return ' '.join(f'{byte:02x}' for byte in byte_data)"
   ],
   "id": "41e2ed4561254c43",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.674844Z",
     "start_time": "2024-04-25T15:47:24.672115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_solidity_code(code: str) -> str:\n",
    "    # Remove single-line comments\n",
    "    code = re.sub(r'//.*', '', code)\n",
    "\n",
    "    # Remove multi-line comments\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "    # Remove blank lines (lines only containing whitespace)\n",
    "    lines = code.split('\\n')\n",
    "    non_blank_lines = [line for line in lines if line.strip() != '']\n",
    "    code = '\\n'.join(non_blank_lines)\n",
    "\n",
    "    return code"
   ],
   "id": "f75a48b51037e77c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.685966Z",
     "start_time": "2024-04-25T15:47:24.684081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess(data: str):\n",
    "    return preprocess_solidity_code(data) if FILE_TYPE == \"source\" else preprocess_hex(data)"
   ],
   "id": "15fa061be57e6304",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Labels Management",
   "id": "7e084d9053915673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.736709Z",
     "start_time": "2024-04-25T15:47:24.733443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_docs_and_gt(data: pd.DataFrame) -> Tuple:\n",
    "    docs, labels, gt = {}, {}, {}\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Initializing documents and groundtruth data\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Initialize the documents\n",
    "            docs[item_id] = preprocess(open(path_to_file, 'r', encoding=\"utf8\").read())\n",
    "\n",
    "            # Initialize the label\n",
    "            labels[item_id] = [0] * NUM_LABELS\n",
    "\n",
    "            # Initialize the groundtruth\n",
    "            prop = row[\"property\"].lower()\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "\n",
    "    return list(docs.values()), labels, gt"
   ],
   "id": "1511840b0951d215",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:24.751041Z",
     "start_time": "2024-04-25T15:47:24.748190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_labels(data: pd.DataFrame, labels: Dict, gt: Dict) -> List:\n",
    "    for _, row in tqdm(data.iterrows(), desc=\"Setting up the labels\"):\n",
    "        item_id, file_id = row[\"id\"], row[\"fp_\" + FILE_ID]\n",
    "\n",
    "        # Check if file exists\n",
    "        path_to_file = os.path.join(PATH_TO_DATASET, FILE_TYPE, str(file_id) + FILE_EXT)\n",
    "        if os.path.exists(path_to_file):\n",
    "\n",
    "            # Set label   \n",
    "            prop = row[\"property\"].lower()\n",
    "            if row['property_holds'] == 't':\n",
    "                labels[item_id][gt[prop]] = 1\n",
    "\n",
    "    return list(labels.values())"
   ],
   "id": "89cc44af1e668270",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialization of the dataset",
   "id": "bac1ca9644eca1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:26.472885Z",
     "start_time": "2024-04-25T15:47:24.752870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read the dataset from CSV\n",
    "dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "\n",
    "# Count the frequency of each item in the column\n",
    "frequency = dataset['dataset'].value_counts()\n",
    "\n",
    "# Find the item with the maximum occurrence\n",
    "most_frequent_item = frequency.idxmax()\n",
    "most_frequent_count = frequency.max()\n",
    "\n",
    "print(f\"The most frequent item in the column is '{most_frequent_item}' and it appears {most_frequent_count} times.\")\n",
    "\n",
    "# Exclude outliers from the dataset\n",
    "dataset = dataset[dataset[\"dataset\"] == most_frequent_item]\n",
    "\n",
    "# Initialize the documents and the groundtruth\n",
    "documents, labels, gt = init_docs_and_gt(dataset)\n",
    "\n",
    "# Set the labels for the multilabel classification problem\n",
    "labels = set_labels(dataset, labels, gt)"
   ],
   "id": "fb254a69e43e824e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent item in the column is 'CodeSmells' and it appears 10395 times.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing documents and groundtruth data: 10395it [00:01, 7451.97it/s]\n",
      "Setting up the labels: 10395it [00:00, 49058.04it/s]\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BERT-like Models",
   "id": "f2934d41ac3e7b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:29.798501Z",
     "start_time": "2024-04-25T15:47:26.473998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(BERT_MODEL_TYPE, num_labels=20, ignore_mismatched_sizes=True)\n",
    "model.config.problem_type = \"multi_label_classification\"\n",
    "model.to(DEVICE)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_MODEL_TYPE, ignore_mismatched_sizes=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ],
   "id": "5acdc46ff12870a3",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:29.803285Z",
     "start_time": "2024-04-25T15:47:29.799568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(true_labels, pred_labels):\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    recall = recall_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    f1 = f1_score(true_labels, pred_labels, average='samples', zero_division=0)\n",
    "    hamming = hamming_loss(true_labels, pred_labels)\n",
    "    return accuracy, precision, recall, f1, hamming"
   ],
   "id": "7440a306f6c6dda9",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:29.812868Z",
     "start_time": "2024-04-25T15:47:29.805486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_evaluate(model, train_dataloader, test_dataloader):\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_losses, train_metrics = [], []\n",
    "\n",
    "        # Training loop\n",
    "        for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "            batch = tuple(b.to(model.device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "            model.zero_grad()\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute metrics for the batch\n",
    "            with torch.no_grad():\n",
    "                predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "                batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "                train_metrics.append(batch_metrics)\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_train_metrics = np.mean(train_metrics, axis=0)\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS} | Train loss: {avg_train_loss:.4f} | Train Metrics: Precision: {avg_train_metrics[1]:.4f}, Recall: {avg_train_metrics[2]:.4f}, F1: {avg_train_metrics[3]:.4f}, Hamming Loss: {avg_train_metrics[4]:.4f}\\n\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_losses, test_metrics = [], []\n",
    "\n",
    "        for batch in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            batch = tuple(b.to(model.device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "                test_losses.append(loss.item())\n",
    "                predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "                batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "                test_metrics.append(batch_metrics)\n",
    "\n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "        avg_test_metrics = np.mean(test_metrics, axis=0)\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS} | Test loss: {avg_test_loss:.4f} | Test Metrics: Precision: {avg_test_metrics[1]:.4f}, Recall: {avg_test_metrics[2]:.4f}, F1: {avg_test_metrics[3]:.4f}, Hamming Loss: {avg_test_metrics[4]:.4f}\\n\")\n"
   ],
   "id": "c80667693427c66a",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:29.817898Z",
     "start_time": "2024-04-25T15:47:29.814094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_on_test_set(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_losses, test_metrics = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader, desc=\"Evaluating on Test Set\"):\n",
    "            batch = tuple(b.to(model.device) for b in batch)\n",
    "            inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, inputs['labels'])\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            predictions = torch.sigmoid(outputs.logits).round().cpu().numpy()\n",
    "            batch_metrics = compute_metrics(batch[2].cpu().numpy(), predictions)\n",
    "            test_metrics.append(batch_metrics)\n",
    "\n",
    "    avg_test_loss = np.mean(test_losses)\n",
    "    avg_test_metrics = np.mean(test_metrics, axis=0)\n",
    "    print(\n",
    "        f\"\\nTest Set Evaluation | Loss: {avg_test_loss:.4f} | Precision: {avg_test_metrics[1]:.4f}, Recall: {avg_test_metrics[2]:.4f}, F1: {avg_test_metrics[3]:.4f}, Hamming Loss: {avg_test_metrics[4]:.4f}\\n\")\n"
   ],
   "id": "c2c756331f70649e",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:43.400152Z",
     "start_time": "2024-04-25T15:47:29.818661Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoding = tokenizer(documents, add_special_tokens=True, max_length=512,\n",
    "                     return_token_type_ids=False, padding=\"max_length\",\n",
    "                     truncation=True, return_attention_mask=True,\n",
    "                     return_tensors='pt')\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(encoding['input_ids'], labels, test_size=TEST_SIZE)\n",
    "train_masks, test_masks, _, _ = train_test_split(encoding['attention_mask'], labels, test_size=TEST_SIZE)\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset = TensorDataset(x_train, train_masks, torch.tensor(y_train).float())\n",
    "test_dataset = TensorDataset(x_test, test_masks, torch.tensor(y_test).float())\n",
    "\n",
    "# K-Fold Configuration\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Applying K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_dataset)):\n",
    "    train_subsampler = Subset(train_dataset, train_idx)\n",
    "    val_subsampler = Subset(train_dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subsampler, sampler=RandomSampler(train_subsampler), batch_size=BATCH_SIZE)\n",
    "    val_loader = DataLoader(val_subsampler, batch_size=BATCH_SIZE)  # No need for shuffling\n",
    "\n",
    "    print(f\"Starting Fold {fold + 1}\")\n",
    "    train_and_evaluate(model, train_loader, val_loader)\n",
    "\n",
    "# Create test dataloader (assuming test_dataset is already created)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Evaluate on the test set\n",
    "evaluate_on_test_set(model, test_dataloader)"
   ],
   "id": "cfb9656b31564534",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 2/176 [00:09<13:15,  4.57s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m     val_loader \u001B[38;5;241m=\u001B[39m DataLoader(val_subsampler, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE)  \u001B[38;5;66;03m# No need for shuffling\u001B[39;00m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStarting Fold \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m     \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# Create test dataloader (assuming test_dataset is already created)\u001B[39;00m\n\u001B[1;32m     30\u001B[0m test_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(test_dataset, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[51], line 14\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(model, train_dataloader, test_dataloader)\u001B[0m\n\u001B[1;32m     12\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs\u001B[38;5;241m.\u001B[39mlogits, inputs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m     13\u001B[0m train_losses\u001B[38;5;241m.\u001B[39mappend(loss\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m---> 14\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Compute metrics for the batch\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SVM, Random Forest, Gradient Boosting",
   "id": "ccd9856e5cf86718"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classifiers = {\n",
    "    \"svm\": SVC(kernel='linear', probability=True),\n",
    "    \"random_forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_SEED),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "}"
   ],
   "id": "21eb9c99d919bdd1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:43.402440Z",
     "start_time": "2024-04-25T15:47:43.402275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing and Feature Extraction\n",
    "vectorizer = TfidfVectorizer(max_features=MAX_FEATURES)\n",
    "x = vectorizer.fit_transform(documents)\n",
    "y = labels\n",
    "\n",
    "# Splitting Data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    print(f\"\\nTESTING CLASSIFIER: {classifier_name}\")\n",
    "\n",
    "    # Train the SVM model with One-vs-Rest strategy\n",
    "    model = OneVsRestClassifier(classifier)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Evaluate the model\n",
    "    predictions = model.predict(x_test)\n",
    "    print(\"\\n-> Accuracy: .............. :\", accuracy_score(y_test, predictions))\n",
    "    print(\"\\n-> Classification Report ... :\", classification_report(y_test, predictions))\n",
    "\n",
    "    # Define a pipeline combining a text vectorizer, and a classifier\n",
    "    pipeline = Pipeline([('tfidf', vectorizer), ('clf', model)])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    scores = cross_val_score(pipeline, documents, y, cv=5, scoring='f1_samples')\n",
    "    print(\"\\n-> Mean CV F1 score ....... :\", scores.mean())"
   ],
   "id": "cd228b01fa44096f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Simple Neural Network",
   "id": "c489beb8f1bec7f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:47:49.364204Z",
     "start_time": "2024-04-25T15:47:49.359144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(MAX_FEATURES, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, y.shape[1])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ],
   "id": "efa02ce8766e7c38",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:48:20.410673Z",
     "start_time": "2024-04-25T15:47:50.452296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing and Feature Extraction\n",
    "x = TfidfVectorizer(max_features=MAX_FEATURES).fit_transform(documents).toarray()\n",
    "x = torch.FloatTensor(x)\n",
    "y = torch.FloatTensor(labels)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# K-Fold Configuration\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Prepare the test DataLoader\n",
    "test_data = DataLoader(TensorDataset(x_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "    print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Creating data subsets for the current fold\n",
    "    x_train_fold, x_val_fold = x_train[train_idx], x_train[val_idx]\n",
    "    y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "    train_data = DataLoader(TensorDataset(x_train_fold, y_train_fold), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_data = DataLoader(TensorDataset(x_val_fold, y_val_fold), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Initialize the network\n",
    "    model = SimpleNN()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "    # Training and validation loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in tqdm(train_data, desc=\"Training\"):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_predictions, all_targets = [], []\n",
    "            for inputs, targets in tqdm(val_data, desc=\"Validation\"):\n",
    "                outputs = model(inputs)\n",
    "                all_predictions.append(outputs)\n",
    "                all_targets.append(targets)\n",
    "\n",
    "            all_predictions = torch.cat(all_predictions).cpu()\n",
    "            all_targets = torch.cat(all_targets).cpu()\n",
    "            predicted_labels = (all_predictions > 0.5).type(torch.float)\n",
    "            acc = accuracy_score(all_targets.numpy(), predicted_labels.numpy())\n",
    "            print(f'Validation - Fold {fold + 1}, Epoch {epoch + 1}: Accuracy: {acc:.4f}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions, all_targets = [], []\n",
    "    for inputs, targets in tqdm(test_data, desc=\"Testing\"):\n",
    "        outputs = model(inputs)\n",
    "        all_predictions.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions).cpu()\n",
    "    all_targets = torch.cat(all_targets).cpu()\n",
    "    predicted_labels = (all_predictions > 0.5).type(torch.float)\n",
    "    acc = accuracy_score(all_targets.numpy(), predicted_labels.numpy())\n",
    "    precision = precision_score(all_targets.numpy(), predicted_labels.numpy(), average='samples', zero_division=0)\n",
    "    recall = recall_score(all_targets.numpy(), predicted_labels.numpy(), average='samples', zero_division=0)\n",
    "    f1 = f1_score(all_targets.numpy(), predicted_labels.numpy(), average='samples', zero_division=0)\n",
    "    print(f'Test Set Evaluation - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1:{f1:.4f}')\n"
   ],
   "id": "a863a02c32516439",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 661.67it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6772.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 1: Accuracy: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 763.51it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3518.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 2: Accuracy: 0.1477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 763.89it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6543.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 3: Accuracy: 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 782.52it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 2824.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 4: Accuracy: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 772.98it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9134.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 5: Accuracy: 0.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 751.84it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5482.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 6: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 792.98it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6604.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 7: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 753.18it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4765.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 8: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 640.23it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4370.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 9: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 634.05it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6720.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 10: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 605.03it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6677.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 11: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 523.84it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5034.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 12: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 670.12it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8994.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 13: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 705.45it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8523.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 14: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 674.59it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9319.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 15: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 795.11it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6674.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 16: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 672.49it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6299.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 17: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 718.68it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5108.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 18: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 724.32it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4496.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 19: Accuracy: 0.1477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 774.91it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6712.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 20: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 776.70it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7315.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 21: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 769.79it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9195.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 22: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 641.95it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5632.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 23: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 711.30it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7962.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 24: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 664.78it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9642.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 25: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 678.07it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5425.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 26: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 551.10it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6544.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 27: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 482.10it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9016.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 28: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 623.04it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6003.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 29: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 634.93it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5876.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 1, Epoch 30: Accuracy: 0.1932\n",
      "Fold 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 607.61it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6586.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 1: Accuracy: 0.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 590.36it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3830.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 2: Accuracy: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 579.79it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8906.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 3: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 561.97it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4200.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 4: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 542.44it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3975.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 5: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 611.13it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6075.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 6: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 593.71it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4134.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 7: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 605.92it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5883.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 8: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 580.47it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4953.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 9: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 493.68it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3191.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 10: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 447.94it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4032.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 11: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 564.01it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4950.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 12: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 570.25it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3810.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 13: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 589.93it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5880.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 14: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 593.01it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5638.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 15: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 596.24it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9251.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 16: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 619.97it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6309.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 17: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 608.80it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3781.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 18: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 418.61it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3771.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 19: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 461.96it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9056.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 20: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 643.48it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9621.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 21: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 592.04it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4013.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 22: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 645.84it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4754.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 23: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 640.41it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5990.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 24: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 635.28it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3977.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 25: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 605.17it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7189.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 26: Accuracy: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 566.95it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3332.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 27: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 611.47it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 3730.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 28: Accuracy: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 557.07it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4067.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 29: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 365.79it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7805.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 2, Epoch 30: Accuracy: 0.3068\n",
      "Fold 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 593.84it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5069.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 1: Accuracy: 0.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 578.25it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9674.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 2: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 639.51it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8461.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 3: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 635.96it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9098.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 4: Accuracy: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 637.37it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5964.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 5: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 698.63it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6894.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 6: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 501.97it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7433.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 7: Accuracy: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 451.05it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9542.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 8: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 619.95it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9544.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 9: Accuracy: 0.3523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 608.30it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8379.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 10: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 637.26it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9308.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 11: Accuracy: 0.3409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 636.94it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8741.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 12: Accuracy: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 607.62it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8079.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 13: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 592.90it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9345.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 14: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 623.21it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6997.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 15: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 577.39it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4163.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 16: Accuracy: 0.2841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 459.72it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8812.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 17: Accuracy: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 712.23it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5459.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 18: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 648.11it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6013.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 19: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 702.07it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8758.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 20: Accuracy: 0.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 680.13it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6858.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 21: Accuracy: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 621.91it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5048.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 22: Accuracy: 0.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 609.41it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4350.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 23: Accuracy: 0.3409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 583.72it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9031.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 24: Accuracy: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 646.17it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7458.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 25: Accuracy: 0.3068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 625.88it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9302.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 26: Accuracy: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 679.23it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 9230.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 27: Accuracy: 0.3409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 642.36it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5915.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 28: Accuracy: 0.3295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 597.93it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6882.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 29: Accuracy: 0.3636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 454.14it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7554.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 3, Epoch 30: Accuracy: 0.3295\n",
      "Fold 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 535.51it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5145.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 1: Accuracy: 0.1591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 726.63it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 7118.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 2: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 693.81it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6649.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 3: Accuracy: 0.2273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 775.32it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 4447.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 4: Accuracy: 0.2614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 561.75it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 6634.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 5: Accuracy: 0.2727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 644.28it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 5888.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 6: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 638.40it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 8956.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 7: Accuracy: 0.2386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 674.97it/s]\n",
      "Validation: 100%|██████████| 44/44 [00:00<00:00, 1347.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Fold 4, Epoch 8: Accuracy: 0.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 176/176 [00:00<00:00, 636.51it/s]\n",
      "Validation:  55%|█████▍    | 24/44 [00:00<00:00, 2277.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[55], line 51\u001B[0m\n\u001B[1;32m     49\u001B[0m all_predictions, all_targets \u001B[38;5;241m=\u001B[39m [], []\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, targets \u001B[38;5;129;01min\u001B[39;00m tqdm(val_data, desc\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m---> 51\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     52\u001B[0m     all_predictions\u001B[38;5;241m.\u001B[39mappend(outputs)\n\u001B[1;32m     53\u001B[0m     all_targets\u001B[38;5;241m.\u001B[39mappend(targets)\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[54], line 11\u001B[0m, in \u001B[0;36mSimpleNN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     10\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x))\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     12\u001B[0m     x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x))\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LSTM",
   "id": "990ea60a5cad8c3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:48:24.536363Z",
     "start_time": "2024-04-25T15:48:24.531355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pretrained_embeddings):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(pretrained_embeddings, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = True  # Optionally freeze the embeddings\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        packed_output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        output = self.fc(hidden)\n",
    "        return torch.sigmoid(output)"
   ],
   "id": "e77799d5ed7f928",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:48:25.422457Z",
     "start_time": "2024-04-25T15:48:25.418818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as file:\n",
    "        for line in tqdm(file, desc=\"Loading GloVe Embeddings\"):\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ],
   "id": "424061fefe78513e",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:48:30.307785Z",
     "start_time": "2024-04-25T15:48:26.153628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_embeddings = load_glove_embeddings('glove.6B.100d.txt')  # Update path as necessary\n",
    "\n",
    "# Tokenization and vocabulary creation\n",
    "word_count = Counter(word for sentence in documents for word in sentence.lower().split())\n",
    "vocabulary = {word: i + 1 for i, word in enumerate(word_count)}  # start indexing from 1\n",
    "vocabulary['<PAD>'] = 0  # Padding value\n",
    "\n",
    "# Embedding matrix creation\n",
    "embedding_dim = 100  # Dimensionality of GloVe embeddings used\n",
    "embedding_matrix = np.zeros((len(vocabulary), embedding_dim))\n",
    "for word, i in tqdm(vocabulary.items(), desc='Creating Embedding Matrix'):\n",
    "    embedding_vector = glove_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Convert text to sequence of integers\n",
    "sequences = [[vocabulary[word] for word in text.lower().split()] for text in documents]\n",
    "\n",
    "# Finding the longest sequence\n",
    "max_seq_len = max(len(seq) for seq in sequences)\n",
    "\n",
    "# Pad sequences\n",
    "seq_padded = [seq + [vocabulary['<PAD>']] * (max_seq_len - len(seq)) for seq in sequences]"
   ],
   "id": "a3609dd521dbe50e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading GloVe Embeddings: 400000it [00:03, 100997.07it/s]\n",
      "Creating Embedding Matrix: 100%|██████████| 45901/45901 [00:00<00:00, 1349143.29it/s]\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T15:48:42.875765Z",
     "start_time": "2024-04-25T15:48:31.746875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_tensor = torch.tensor(seq_padded, dtype=torch.long)\n",
    "y_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "# Split dataset for final evaluation\n",
    "x_train_val, x_test, y_train_val, y_test = train_test_split(x_tensor, y_tensor, test_size=TEST_SIZE,\n",
    "                                                            random_state=RANDOM_SEED)\n",
    "test_data = DataLoader(TensorDataset(x_test, y_test), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# K-Fold Configuration\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# Perform K-fold Cross Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(x_train_val)):\n",
    "    print(f\"Training Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "    # Create training and validation data loaders\n",
    "    train_data = DataLoader(TensorDataset(x_train_val[train_idx], y_train_val[train_idx]), batch_size=BATCH_SIZE,\n",
    "                            shuffle=True)\n",
    "    val_data = DataLoader(TensorDataset(x_train_val[val_idx], y_train_val[val_idx]), batch_size=BATCH_SIZE,\n",
    "                          shuffle=False)\n",
    "\n",
    "    # Model, loss, and optimizer setup\n",
    "    model = LSTMClassifier(len(vocabulary), embedding_dim, hidden_dim=100, output_dim=y_train_val.shape[1],\n",
    "                           pretrained_embeddings=embedding_matrix)\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.BCELoss().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in tqdm(train_data, desc=f\"Training Epoch {epoch + 1}, Fold {fold + 1}\"):\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_predictions, all_targets = [], []\n",
    "            for inputs, targets in tqdm(val_data, desc=\"Validating\"):\n",
    "                inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                predicted_labels = (outputs > 0.5).float()\n",
    "                all_predictions.append(predicted_labels)\n",
    "                all_targets.append(targets)\n",
    "\n",
    "            all_predictions = torch.cat(all_predictions).cpu()\n",
    "            all_targets = torch.cat(all_targets).cpu()\n",
    "            acc = accuracy_score(all_targets.numpy(), all_predictions.numpy())\n",
    "            precision = precision_score(all_targets.numpy(), all_predictions.numpy(), average='samples',\n",
    "                                        zero_division=0)\n",
    "            recall = recall_score(all_targets.numpy(), all_predictions.numpy(), average='samples', zero_division=0)\n",
    "            f1 = f1_score(all_targets.numpy(), all_predictions.numpy(), average='samples', zero_division=0)\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Fold {fold + 1} - Loss: {total_loss / len(train_data):.4f}, Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Evaluate on the separate test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    all_predictions, all_targets = [], []\n",
    "    for inputs, targets in tqdm(test_data, desc=\"Evaluating on Test Set\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        predicted_labels = (outputs > 0.5).float()\n",
    "        all_predictions.append(predicted_labels)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "    all_predictions = torch.cat(all_predictions).cpu()\n",
    "    all_targets = torch.cat(all_targets).cpu()\n",
    "    acc = accuracy_score(all_targets.numpy(), all_predictions.numpy())\n",
    "    precision = precision_score(all_targets.numpy(), all_predictions.numpy(), average='samples', zero_division=0)\n",
    "    recall = recall_score(all_targets.numpy(), all_predictions.numpy(), average='samples', zero_division=0)\n",
    "    f1 = f1_score(all_targets.numpy(), all_predictions.numpy(), average='samples', zero_division=0)\n",
    "\n",
    "    print(f\"Test Set Evaluation - Acc: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"
   ],
   "id": "c00f0ccc3317a45e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1, Fold 1:   6%|▋         | 11/176 [00:10<02:42,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 39\u001B[0m\n\u001B[1;32m     37\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[1;32m     38\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, targets)\n\u001B[0;32m---> 39\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     41\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
