{
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install pandas~=2.2.1 numpy~=1.26.4 torch~=2.2.2 scikit-learn~=1.4.1.post1 transformers~=4.39.3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NqG_kpUg46R",
    "outputId": "ebee0aa0-4fe3-40ed-fbda-62704325ae29",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.850785Z",
     "start_time": "2024-04-09T14:05:27.712477Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas~=2.2.1 in ./.venv/lib/python3.10/site-packages (2.2.1)\r\n",
      "Requirement already satisfied: numpy~=1.26.4 in ./.venv/lib/python3.10/site-packages (1.26.4)\r\n",
      "Requirement already satisfied: torch~=2.2.2 in ./.venv/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: scikit-learn~=1.4.1.post1 in ./.venv/lib/python3.10/site-packages (1.4.1.post1)\r\n",
      "Requirement already satisfied: transformers~=4.39.3 in ./.venv/lib/python3.10/site-packages (4.39.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas~=2.2.1) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas~=2.2.1) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas~=2.2.1) (2024.1)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (3.13.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (1.12)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch~=2.2.2) (2024.3.1)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn~=1.4.1.post1) (1.13.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn~=1.4.1.post1) (1.4.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn~=1.4.1.post1) (3.4.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (0.22.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (0.15.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (0.4.2)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers~=4.39.3) (4.66.2)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.1) (1.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch~=2.2.2) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers~=4.39.3) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers~=4.39.3) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers~=4.39.3) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers~=4.39.3) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch~=2.2.2) (1.3.0)\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.881652Z",
     "start_time": "2024-04-09T14:05:29.868483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import os\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "from time import time\n",
    "from typing import Tuple, Union, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from torch import Tensor, optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YonJYhSksT14",
    "outputId": "ac5970e3-db6e-4d32-9251-50207eb4dfd7",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.890019Z",
     "start_time": "2024-04-09T14:05:29.884997Z"
    }
   },
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "PATH_TO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Datasets/\"\n",
    "PATH_TO_DATASET = \"dataset\"\n",
    "\n",
    "# drive.mount('/content/drive/', force_remount=True)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odBQREVBcpA2",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.899310Z",
     "start_time": "2024-04-09T14:05:29.893530Z"
    }
   },
   "source": [
    "# zip_ref = zipfile.ZipFile(os.path.join(PATH_TO_DRIVE, \"sc.zip\"), 'r')\n",
    "# zip_ref.extractall(PATH_TO_DRIVE)\n",
    "# zip_ref.close()"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVsE0ENLUgrp"
   },
   "source": [
    "# Utils and Device Settings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AeiMbH2CVuqJ",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.908184Z",
     "start_time": "2024-04-09T14:05:29.901206Z"
    }
   },
   "source": [
    "SEPARATOR = {\"stars\": \"\".join([\"*\"] * 100), \"dashes\": \"\".join([\"-\"] * 100), \"dots\": \"\".join([\".\"] * 100)}\n",
    "\n",
    "\n",
    "def get_device(device_type: str) -> torch.device:\n",
    "    if device_type == \"cpu\":\n",
    "        print(\"\\n Running on device 'cpu' \\n\")\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "    if re.match(r\"\\bcuda:\\b\\d+\", device_type):\n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"\\n WARNING: running on cpu since device {} is not available \\n\".format(device_type))\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"\\n Running on device '{}' \\n\".format(device_type))\n",
    "        return torch.device(device_type)\n",
    "\n",
    "    raise ValueError(\"ERROR: {} is not a valid device! Supported device are 'cpu' and 'cuda:n'\".format(device_type))\n",
    "\n",
    "\n",
    "def make_deterministic(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUX7mSc8XsZd",
    "outputId": "4718526d-e667-4d12-e94c-3234e898a4ac",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.916028Z",
     "start_time": "2024-04-09T14:05:29.910440Z"
    }
   },
   "source": [
    "DEVICE_TYPE = \"cuda:0\"\n",
    "DEVICE = get_device(DEVICE_TYPE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " WARNING: running on cpu since device cuda:0 is not available \n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qZPMD_4BYmKB",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.927082Z",
     "start_time": "2024-04-09T14:05:29.918589Z"
    }
   },
   "source": [
    "class LossTracker:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricsTracker:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._predictions, self._true_labels = [], []\n",
    "        self.__monitored_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "        self._metrics, self._best_metrics = {}, {m: 0 for m in self.__monitored_metrics}\n",
    "\n",
    "    def compute_metrics(self) -> Dict:\n",
    "        predictions_flat = np.argmax(np.concatenate(self._predictions, axis=0), axis=1)\n",
    "        true_labels_flat = np.concatenate(self._true_labels, axis=0)\n",
    "        print(predictions_flat, true_labels_flat)\n",
    "        self._metrics = {\n",
    "            \"accuracy\": accuracy_score(true_labels_flat, predictions_flat),\n",
    "            \"precision\": precision_score(true_labels_flat, predictions_flat, average='macro'),\n",
    "            \"recall\": recall_score(true_labels_flat, predictions_flat, average='macro'),\n",
    "            \"f1\": f1_score(true_labels_flat, predictions_flat, average='macro')\n",
    "        }\n",
    "        return self._metrics\n",
    "\n",
    "    def update_best_metrics(self) -> Dict:\n",
    "        self._best_metrics[\"mean\"] = self._metrics[\"mean\"]\n",
    "        self._best_metrics[\"median\"] = self._metrics[\"median\"]\n",
    "        self._best_metrics[\"trimean\"] = self._metrics[\"trimean\"]\n",
    "        self._best_metrics[\"bst25\"] = self._metrics[\"bst25\"]\n",
    "        self._best_metrics[\"wst25\"] = self._metrics[\"wst25\"]\n",
    "        self._best_metrics[\"wst5\"] = self._metrics[\"wst5\"]\n",
    "        return self._best_metrics\n",
    "\n",
    "    def update(self, predictions: np.ndarray, true_labels: np.ndarray):\n",
    "        self._predictions.append(predictions)\n",
    "        self._true_labels.append(true_labels)\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions, self._true_labels = [], []\n",
    "\n",
    "    def get_best_metrics(self) -> Dict:\n",
    "        return self._best_metrics"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJ2bw9yVVuA9"
   },
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6tujde6iSz6x",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.941423Z",
     "start_time": "2024-04-09T14:05:29.929825Z"
    }
   },
   "source": [
    "class CGTDataset(Dataset):\n",
    "\n",
    "    def __init__(self, train: bool = True, fold_num: int = 0, data_type: str = \"all\"):\n",
    "        self._train = train\n",
    "        self.__data_type = data_type\n",
    "        self._items = self.__build_dataset()\n",
    "        ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
    "        train_idx, test_idx = [(i_train, i_test) for i_train, i_test in ss.split(range(len(self._items)))][fold_num]\n",
    "        self.__fold_idx = train_idx if train else test_idx\n",
    "\n",
    "    @staticmethod\n",
    "    def __encode(text: str):\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    def __make_item(self, dataset: pd.DataFrame):\n",
    "        item = {}\n",
    "        path2bytecode = os.path.join(PATH_TO_DATASET, \"bytecode\", dataset['fp_bytecode'].values[0] + \".hex\")\n",
    "        if (self.__data_type == \"all\" or self.__data_type == \"bytecode\") and os.path.exists(path2bytecode):\n",
    "            item['path2bytecode'] = path2bytecode\n",
    "        path2runtime = os.path.join(PATH_TO_DATASET, \"runtime\", dataset['fp_runtime'].values[0] + \".rt.hex\")\n",
    "        if (self.__data_type == \"all\" or self.__data_type == \"runtime\") and os.path.exists(path2runtime):\n",
    "            item['path2runtime'] = path2runtime\n",
    "        path2source = os.path.join(PATH_TO_DATASET, \"source\", dataset['fp_sol'].values[0] + \".sol\")\n",
    "        if (self.__data_type == \"all\" or self.__data_type == \"source\") and os.path.exists(path2source):\n",
    "            item['path2source'] = path2source\n",
    "        return item\n",
    "\n",
    "    def __build_dataset(self):\n",
    "        dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
    "        gt, items = {\"none\": 0}, []\n",
    "        for _, row in dataset.iterrows():\n",
    "            prop = row[\"property\"].lower() if row['property_holds'] == 't' else 'none'\n",
    "            if prop not in gt.keys():\n",
    "                gt[prop] = len(gt.values())\n",
    "            item = self.__make_item(dataset)\n",
    "            item[\"gt\"] = gt[prop]\n",
    "            items.append(item)\n",
    "        return items\n",
    "\n",
    "    def _load_f(self, path_to_item: str) -> Tensor:\n",
    "        with open(path_to_item, 'r') as fp:\n",
    "            return self.__encode(fp.read())\n",
    "\n",
    "    def _load_input(self, index: int) -> Dict:\n",
    "        item = self._items[index]\n",
    "        return {k.split(\"path2\")[-1]: self._load_f(v) for k, v in item.items() if \"path2\" in k}\n",
    "\n",
    "    def _load_label(self, index: int) -> Tensor:\n",
    "        return self._items[index]['gt']\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Dict, Tensor]:\n",
    "        index = self.__fold_idx[index]\n",
    "        x, y = self._load_input(index), self._load_label(index)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__fold_idx)\n",
    "\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self):\n",
    "        self._dataset = CGTDataset\n",
    "\n",
    "    def train_test_loaders(self, fold_num: int) -> Tuple:\n",
    "        training_loader = self.get_loader(train=True, fold_num=fold_num)\n",
    "        test_loader = self.get_loader(train=False, fold_num=fold_num)\n",
    "        return training_loader, test_loader\n",
    "\n",
    "    def get_loader(self, train: bool, fold_num: int) -> DataLoader:\n",
    "        dataset = self._dataset(train, fold_num)\n",
    "        # return DataLoader(dataset, batch_size=1, shuffle=train, num_workers=cpu_count(), drop_last=True)\n",
    "        return DataLoader(dataset, batch_size=1, shuffle=train, drop_last=True)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2qy7Cv7Xeqw"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fZ65rUz9X3yI",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.955484Z",
     "start_time": "2024-04-09T14:05:29.943133Z"
    }
   },
   "source": [
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.__bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=159)\n",
    "\n",
    "    def forward(self, input_id: Tensor, attention_mask: Tensor, label: Tensor) -> Tensor:\n",
    "        input_id = input_id.squeeze(1)\n",
    "        attention_mask = attention_mask.squeeze(1)\n",
    "        return self.__bert(input_ids=input_id, attention_mask=attention_mask, labels=label)\n",
    "\n",
    "\n",
    "class ModelBERT:\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._device = DEVICE\n",
    "        self._optimizer = None\n",
    "        self._strategy = BERT().to(self._device)\n",
    "\n",
    "    def predict(self, x: Dict, y: Tensor, *args, **kwargs) -> Union[Tensor, any]:\n",
    "        x = x[\"source\"]\n",
    "        input_id, attention_mask = x[\"input_ids\"], x[\"attention_mask\"]\n",
    "        return self._strategy(input_id, attention_mask, y)\n",
    "\n",
    "    def optimize(self, x: Dict, y: Tensor) -> float:\n",
    "        self._optimizer.zero_grad()\n",
    "        pred = self.predict(x, y)\n",
    "        loss = self.get_loss(pred)\n",
    "        loss.backward()\n",
    "        self._optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_loss(pred: Tensor) -> Tensor:\n",
    "        return pred.loss\n",
    "\n",
    "    def log_strategy(self, path_to_log: str):\n",
    "        open(os.path.join(path_to_log, \"strategy.txt\"), 'a+').write(str(self._strategy))\n",
    "\n",
    "    def train_mode(self):\n",
    "        self._strategy = self._strategy.train()\n",
    "\n",
    "    def eval_mode(self):\n",
    "        self._strategy = self._strategy.eval()\n",
    "\n",
    "    def save(self, path_to_save: str):\n",
    "        path_to_pth = os.path.join(path_to_save, \"model.pth\")\n",
    "        print(\"\\nSaving model at {}...\".format(path_to_pth))\n",
    "        torch.save(self._strategy.state_dict(), path_to_pth)\n",
    "        print(\"... Model saved successfully!\\n\")\n",
    "\n",
    "    def set_optimizer(self, learning_rate: float, optimizer_type: str = \"adamw\"):\n",
    "        optimizers_map = {\"adam\": optim.Adam, \"adamw\": optim.AdamW, \"rmsprop\": optim.RMSprop, \"sgd\": optim.SGD}\n",
    "        self._optimizer = optimizers_map[optimizer_type](self._strategy.parameters(), lr=learning_rate)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB9ERsPwYufr"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6s3Oa0LjY_HP",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:29.979357Z",
     "start_time": "2024-04-09T14:05:29.960499Z"
    }
   },
   "source": [
    "class TrainerBERT:\n",
    "    def __init__(self, path_to_log: str, val_frequency: int = 5):\n",
    "        self._device = DEVICE\n",
    "        self._val_frequency = val_frequency\n",
    "        self._path_to_log = path_to_log\n",
    "        self._metrics_tracker = MetricsTracker()\n",
    "        self._train_loss, self._val_loss = LossTracker(), LossTracker()\n",
    "        self._best_val_loss, self._best_metrics = 100.0, self._metrics_tracker.get_best_metrics()\n",
    "        self._path_to_metrics = str(os.path.join(path_to_log, \"metrics.csv\"))\n",
    "\n",
    "    def __to_device(self, x: Dict, y: Tensor):\n",
    "        return {k1: {k2: v2.to(self._device) for k2, v2 in v1.items()} for k1, v1 in x.items()}, y.to(self._device)\n",
    "\n",
    "    def _train_epoch(self, model: ModelBERT, data: DataLoader, epoch: int, *args, **kwargs):\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            if not \"source\" in x.keys():\n",
    "                continue\n",
    "            x, y = self.__to_device(x, y)\n",
    "            tl = model.optimize(x, y)\n",
    "            self._train_loss.update(tl)\n",
    "            if i % 5 == 0:\n",
    "                print(\"[ Epoch: {} - Batch: {} ] | Loss: {:.4f} \".format(epoch + 1, i, tl))\n",
    "\n",
    "    def _eval_epoch(self, model: ModelBERT, data: DataLoader):\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            x, y = self.__to_device(x, y)\n",
    "            pred = model.predict(x, y)\n",
    "            self._metrics_tracker.update(pred.logits, y.detach().numpy())\n",
    "            vl = model.get_loss(pred).item()\n",
    "            if i % 5 == 0:\n",
    "                print(\"[ Batch: {} ] | Loss: {:.4f} ]\".format(i, vl))\n",
    "\n",
    "    def _check_metrics(self):\n",
    "        \"\"\" Computes, prints and logs the current metrics using the metrics tracker \"\"\"\n",
    "        epoch_metrics = self._metrics_tracker.compute_metrics()\n",
    "        self._print_metrics(epoch_metrics)\n",
    "        self._log_metrics(epoch_metrics)\n",
    "\n",
    "    def _log_metrics(self, metrics: Dict):\n",
    "        log_data = pd.DataFrame({\"train_loss\": [self._train_loss.avg], \"val_loss\": [self._val_loss.avg],\n",
    "                                 **{\"best_\" + k: [v] for k, v in self._best_metrics.items()},\n",
    "                                 **{k: [v] for k, v in metrics.items()}})\n",
    "        header = log_data.keys() if not os.path.exists(self._path_to_metrics) else False\n",
    "        log_data.to_csv(self._path_to_metrics, mode='a', header=header, index=False)\n",
    "\n",
    "    def _print_metrics(self, metrics: Dict):\n",
    "        for mn, mv in metrics.items():\n",
    "            print((\" {} \" + \"\".join([\".\"] * (15 - len(mn))) + \" : {:.4f} (Best: {:.4f})\")\n",
    "                  .format(mn.capitalize(), mv, self._best_metrics[mn]))\n",
    "\n",
    "    def train(self, model: ModelBERT, training_set: DataLoader, test_set: DataLoader, lr: float, epochs: int):\n",
    "        \"\"\"\n",
    "        Trains the given model (a PyTorch nn.Module) for \"epochs\" epochs\n",
    "        :param model: the model to be trained (a PyTorch nn.Module)\n",
    "        :param training_set: the data loader containing the training data\n",
    "        :param test_set: the data loader containing the validation/test data\n",
    "        :param lr: a learning rate as base value for the optimizer\n",
    "        :param epochs: the number of epochs the model should be trained for\n",
    "        \"\"\"\n",
    "        model.log_strategy(self._path_to_log)\n",
    "        model.set_optimizer(lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            model.train_mode()\n",
    "            self._train_loss.reset()\n",
    "            self.print_heading(\"training\", epoch, epochs)\n",
    "\n",
    "            start = time()\n",
    "            self._train_epoch(model, training_set, epoch)\n",
    "            self.print_train_performance(train_time=time() - start)\n",
    "\n",
    "            if epoch % self._val_frequency == 0:\n",
    "                model.eval_mode()\n",
    "                self._val_loss.reset()\n",
    "                self._reset_metrics_tracker()\n",
    "                self.print_heading(\"validating\", epoch, epochs)\n",
    "\n",
    "                start = time()\n",
    "                with torch.no_grad():\n",
    "                    self._eval_epoch(model, test_set)\n",
    "                self.print_val_performance(val_time=time() - start)\n",
    "\n",
    "                self._check_metrics()\n",
    "                self._check_if_best_model(model)\n",
    "\n",
    "    def _reset_metrics_tracker(self):\n",
    "        \"\"\" Reset the metrics_tracker(s) zeroing out the running values \"\"\"\n",
    "        self._metrics_tracker.reset()\n",
    "\n",
    "    def _check_if_best_model(self, model: ModelBERT):\n",
    "        \"\"\"\n",
    "        Checks whether the provides model is the new best model based on the values of the validation loss.\n",
    "        If yes, updates the best metrics and validation loss (as side effect) and saves the model to file\n",
    "        :param model: the model to be possibly saved as new best model\n",
    "        \"\"\"\n",
    "        if 0 < self._val_loss.avg < self._best_val_loss:\n",
    "            self._best_val_loss = self._val_loss.avg\n",
    "            self._best_metrics = self._metrics_tracker.update_best_metrics()\n",
    "            print(\"\\n -> Saving new best model...\")\n",
    "            model.save(self._path_to_log)\n",
    "\n",
    "    def print_train_performance(self, train_time: float):\n",
    "        \"\"\"\n",
    "        Prints the training time/loss for the most recent epoch\n",
    "        :param train_time: the training time for the most recent epoch\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + SEPARATOR[\"stars\"])\n",
    "        print(\" Train Time ... : {:.4f}\".format(train_time))\n",
    "        print(\" Train Loss ... : {:.4f}\".format(self._train_loss.avg))\n",
    "        print(SEPARATOR[\"stars\"])\n",
    "\n",
    "    def print_val_performance(self, val_time: float):\n",
    "        \"\"\"\n",
    "        Prints the validation time/loss for the most recent epoch\n",
    "        :param val_time: the validation time for the most recent epoch\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + SEPARATOR[\"stars\"])\n",
    "        print(\" Val Time ... : {:.4f}\".format(val_time))\n",
    "        print(\" Val Loss ... : {:.4f}\".format(self._val_loss.avg))\n",
    "        print(SEPARATOR[\"stars\"] + \"\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def print_heading(mode: str, epoch: int, epochs: int):\n",
    "        print(\"\\n\" + SEPARATOR[\"dashes\"])\n",
    "        print(\"\\t\\t {} epoch {}/{}\".format(mode.upper(), epoch + 1, epochs))\n",
    "        print(SEPARATOR[\"dashes\"] + \"\\n\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev6vE71OaGHJ"
   },
   "source": [
    "# Perform CV"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ocEP4benaD8n",
    "outputId": "77f5a3e9-01e2-4ab5-ff75-d09c11ee0282",
    "ExecuteTime": {
     "end_time": "2024-04-09T14:05:57.659865Z",
     "start_time": "2024-04-09T14:05:29.981188Z"
    }
   },
   "source": [
    "make_deterministic(0)\n",
    "lr, epochs, log_frequency = 0.0001, 5, 1\n",
    "\n",
    "log_dir = \"{}\".format(time())\n",
    "path_to_log = os.path.join(\"logs\", log_dir)\n",
    "os.makedirs(path_to_log)\n",
    "\n",
    "for fold_num in range(5):\n",
    "    print(\"\\n Loading data for fold '{}'\".format(fold_num + 1))\n",
    "    train_loader, test_loader = DataHandler().train_test_loaders(fold_num)\n",
    "    model = ModelBERT()\n",
    "\n",
    "    print(\"\\n\" + SEPARATOR[\"dashes\"])\n",
    "    print(\"\\t\\t Training fold {}\".format(fold_num + 1))\n",
    "    print(SEPARATOR[\"dashes\"] + \"\\n\")\n",
    "\n",
    "    trainer = TrainerBERT(path_to_log, log_frequency)\n",
    "    trainer.train(model, train_loader, test_loader, lr, epochs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data for fold '1'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t Training fold 1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\t\t TRAINING epoch 1/5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[ Epoch: 1 - Batch: 0 ] | Loss: 5.3991 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(SEPARATOR[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdashes\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m TrainerBERT(path_to_log, log_frequency)\n\u001B[0;32m---> 18\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[21], line 70\u001B[0m, in \u001B[0;36mTrainerBERT.train\u001B[0;34m(self, model, training_set, test_set, lr, epochs)\u001B[0m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_heading(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining\u001B[39m\u001B[38;5;124m\"\u001B[39m, epoch, epochs)\n\u001B[1;32m     69\u001B[0m start \u001B[38;5;241m=\u001B[39m time()\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_train_performance(train_time\u001B[38;5;241m=\u001B[39mtime() \u001B[38;5;241m-\u001B[39m start)\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_val_frequency \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[21], line 19\u001B[0m, in \u001B[0;36mTrainerBERT._train_epoch\u001B[0;34m(self, model, data, epoch, *args, **kwargs)\u001B[0m\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m     18\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__to_device(x, y)\n\u001B[0;32m---> 19\u001B[0m tl \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_loss\u001B[38;5;241m.\u001B[39mupdate(tl)\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[0;32mIn[20], line 30\u001B[0m, in \u001B[0;36mModelBERT.optimize\u001B[0;34m(self, x, y)\u001B[0m\n\u001B[1;32m     28\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpredict(x, y)\n\u001B[1;32m     29\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_loss(pred)\n\u001B[0;32m---> 30\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/smart-contracts-vulnerabilities-ml-detector/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
