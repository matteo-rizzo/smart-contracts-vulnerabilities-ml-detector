{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install multiprocess pandas~=2.0.3 numpy~=1.26.4 torch~=2.2.1 scikit-learn~=1.4.1.post1 transformers~=4.39.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NqG_kpUg46R",
        "outputId": "ba32ee2e-7c71-4fd7-a8a3-9c644429bef4",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.850785Z",
          "start_time": "2024-04-09T14:05:27.712477Z"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (0.70.16)\n",
            "Requirement already satisfied: pandas~=2.0.3 in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch~=2.2.1 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn~=1.4.1.post1 in /usr/local/lib/python3.10/dist-packages (1.4.1.post1)\n",
            "Requirement already satisfied: transformers~=4.39.3 in /usr/local/lib/python3.10/dist-packages (4.39.3)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from multiprocess) (0.3.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=2.0.3) (2024.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.2.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.2.1) (12.4.127)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.4.1.post1) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.4.1.post1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn~=1.4.1.post1) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers~=4.39.3) (4.66.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas~=2.0.3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.2.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers~=4.39.3) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.2.1) (1.3.0)\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.881652Z",
          "start_time": "2024-04-09T14:05:29.868483Z"
        },
        "id": "4h6392kMMkRy"
      },
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import re\n",
        "from time import time\n",
        "from typing import Tuple, Union, Dict\n",
        "from google.colab import drive\n",
        "from multiprocess import cpu_count\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from torch import Tensor, optim, nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertForSequenceClassification, BertTokenizer"
      ],
      "outputs": [],
      "execution_count": 32
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_DRIVE = \"/content/drive/MyDrive/Colab Notebooks/Datasets/\"\n",
        "PATH_TO_DATASET = os.path.join(PATH_TO_DRIVE, \"dataset\")"
      ],
      "metadata": {
        "id": "MF1E31kFNsg-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YonJYhSksT14",
        "outputId": "79b57a12-821f-4410-b377-e7294bac2984",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.890019Z",
          "start_time": "2024-04-09T14:05:29.884997Z"
        }
      },
      "source": [
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odBQREVBcpA2",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.899310Z",
          "start_time": "2024-04-09T14:05:29.893530Z"
        }
      },
      "source": [
        "# zip_ref = zipfile.ZipFile(os.path.join(PATH_TO_DRIVE, \"sc.zip\"), 'r')\n",
        "# zip_ref.extractall(PATH_TO_DRIVE)\n",
        "# zip_ref.close()"
      ],
      "outputs": [],
      "execution_count": 35
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVsE0ENLUgrp"
      },
      "source": [
        "# Utils and Device Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeiMbH2CVuqJ",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.908184Z",
          "start_time": "2024-04-09T14:05:29.901206Z"
        }
      },
      "source": [
        "SEPARATOR = {\"stars\": \"\".join([\"*\"] * 100), \"dashes\": \"\".join([\"-\"] * 100), \"dots\": \"\".join([\".\"] * 100)}\n",
        "\n",
        "\n",
        "def get_device(device_type: str) -> torch.device:\n",
        "    if device_type == \"cpu\":\n",
        "        print(\"\\n Running on device 'cpu' \\n\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "    if re.match(r\"\\bcuda:\\b\\d+\", device_type):\n",
        "        if not torch.cuda.is_available():\n",
        "            print(\"\\n WARNING: running on cpu since device {} is not available \\n\".format(device_type))\n",
        "            return torch.device(\"cpu\")\n",
        "\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"\\n Running on device '{}' \\n\".format(device_type))\n",
        "        return torch.device(device_type)\n",
        "\n",
        "    raise ValueError(\"ERROR: {} is not a valid device! Supported device are 'cpu' and 'cuda:n'\".format(device_type))\n",
        "\n",
        "\n",
        "def make_deterministic(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "outputs": [],
      "execution_count": 36
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUX7mSc8XsZd",
        "outputId": "42ea6aaa-370f-40ee-9ee4-d3c727affc2b",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.916028Z",
          "start_time": "2024-04-09T14:05:29.910440Z"
        }
      },
      "source": [
        "DEVICE_TYPE = \"cuda:0\"\n",
        "DEVICE = get_device(DEVICE_TYPE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " WARNING: running on cpu since device cuda:0 is not available \n",
            "\n"
          ]
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZPMD_4BYmKB",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.927082Z",
          "start_time": "2024-04-09T14:05:29.918589Z"
        }
      },
      "source": [
        "class LossTracker:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
        "\n",
        "    def update(self, val: float, n: int = 1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "class MetricsTracker:\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._predictions, self._true_labels = [], []\n",
        "        self.__monitored_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
        "        self._metrics, self._best_metrics = {}, {m: 0 for m in self.__monitored_metrics}\n",
        "\n",
        "    def compute_metrics(self) -> Dict:\n",
        "        predictions_flat = np.argmax(np.concatenate(self._predictions, axis=0), axis=1)\n",
        "        true_labels_flat = np.concatenate(self._true_labels, axis=0)\n",
        "        print(predictions_flat, true_labels_flat)\n",
        "        self._metrics = {\n",
        "            \"accuracy\": accuracy_score(true_labels_flat, predictions_flat),\n",
        "            \"precision\": precision_score(true_labels_flat, predictions_flat, average='macro'),\n",
        "            \"recall\": recall_score(true_labels_flat, predictions_flat, average='macro'),\n",
        "            \"f1\": f1_score(true_labels_flat, predictions_flat, average='macro')\n",
        "        }\n",
        "        return self._metrics\n",
        "\n",
        "    def update_best_metrics(self) -> Dict:\n",
        "        self._best_metrics[\"mean\"] = self._metrics[\"mean\"]\n",
        "        self._best_metrics[\"median\"] = self._metrics[\"median\"]\n",
        "        self._best_metrics[\"trimean\"] = self._metrics[\"trimean\"]\n",
        "        self._best_metrics[\"bst25\"] = self._metrics[\"bst25\"]\n",
        "        self._best_metrics[\"wst25\"] = self._metrics[\"wst25\"]\n",
        "        self._best_metrics[\"wst5\"] = self._metrics[\"wst5\"]\n",
        "        return self._best_metrics\n",
        "\n",
        "    def update(self, predictions: np.ndarray, true_labels: np.ndarray):\n",
        "        self._predictions.append(predictions)\n",
        "        self._true_labels.append(true_labels)\n",
        "\n",
        "    def reset(self):\n",
        "        self._predictions, self._true_labels = [], []\n",
        "\n",
        "    def get_best_metrics(self) -> Dict:\n",
        "        return self._best_metrics"
      ],
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ2bw9yVVuA9"
      },
      "source": [
        "# Data Handling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tujde6iSz6x",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.941423Z",
          "start_time": "2024-04-09T14:05:29.929825Z"
        }
      },
      "source": [
        "class CGTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, train: bool = True, fold_num: int = 0, data_type: str = \"all\"):\n",
        "        self._train = train\n",
        "        self.__data_type = data_type\n",
        "        self._items = self.__build_dataset()\n",
        "        ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\n",
        "        train_idx, test_idx = [(i_train, i_test) for i_train, i_test in ss.split(range(len(self._items)))][fold_num]\n",
        "        self.__fold_idx = train_idx if train else test_idx\n",
        "\n",
        "    @staticmethod\n",
        "    def __encode(text: str):\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        return tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    def __make_item(self, dataset: pd.DataFrame):\n",
        "        item = {}\n",
        "        path2bytecode = os.path.join(PATH_TO_DATASET, \"bytecode\", dataset['fp_bytecode'].values[0] + \".hex\")\n",
        "        if (self.__data_type == \"all\" or self.__data_type == \"bytecode\") and os.path.exists(path2bytecode):\n",
        "            item['path2bytecode'] = path2bytecode\n",
        "        path2runtime = os.path.join(PATH_TO_DATASET, \"runtime\", dataset['fp_runtime'].values[0] + \".rt.hex\")\n",
        "        if (self.__data_type == \"all\" or self.__data_type == \"runtime\") and os.path.exists(path2runtime):\n",
        "            item['path2runtime'] = path2runtime\n",
        "        path2source = os.path.join(PATH_TO_DATASET, \"source\", dataset['fp_sol'].values[0] + \".sol\")\n",
        "        if (self.__data_type == \"all\" or self.__data_type == \"source\") and os.path.exists(path2source):\n",
        "            item['path2source'] = path2source\n",
        "        return item\n",
        "\n",
        "    def __build_dataset(self):\n",
        "        dataset = pd.read_csv(os.path.join(PATH_TO_DATASET, \"consolidated.csv\"), sep=\";\")\n",
        "        gt, items = {\"none\": 0}, []\n",
        "        for _, row in dataset.iterrows():\n",
        "            prop = row[\"property\"].lower() if row['property_holds'] == 't' else 'none'\n",
        "            if prop not in gt.keys():\n",
        "                gt[prop] = len(gt.values())\n",
        "            item = self.__make_item(dataset)\n",
        "            item[\"gt\"] = gt[prop]\n",
        "            items.append(item)\n",
        "        return items\n",
        "\n",
        "    def _load_f(self, path_to_item: str) -> Tensor:\n",
        "        with open(path_to_item, 'r') as fp:\n",
        "            return self.__encode(fp.read())\n",
        "\n",
        "    def _load_input(self, index: int) -> Dict:\n",
        "        item = self._items[index]\n",
        "        return {k.split(\"path2\")[-1]: self._load_f(v) for k, v in item.items() if \"path2\" in k}\n",
        "\n",
        "    def _load_label(self, index: int) -> Tensor:\n",
        "        return self._items[index]['gt']\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Dict, Tensor]:\n",
        "        index = self.__fold_idx[index]\n",
        "        x, y = self._load_input(index), self._load_label(index)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.__fold_idx)\n",
        "\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self):\n",
        "        self._dataset = CGTDataset\n",
        "\n",
        "    def train_test_loaders(self, fold_num: int) -> Tuple:\n",
        "        training_loader = self.get_loader(train=True, fold_num=fold_num)\n",
        "        test_loader = self.get_loader(train=False, fold_num=fold_num)\n",
        "        return training_loader, test_loader\n",
        "\n",
        "    def get_loader(self, train: bool, fold_num: int) -> DataLoader:\n",
        "        dataset = self._dataset(train, fold_num)\n",
        "        return DataLoader(dataset, batch_size=1, shuffle=train, drop_last=True)"
      ],
      "outputs": [],
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2qy7Cv7Xeqw"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ65rUz9X3yI",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.955484Z",
          "start_time": "2024-04-09T14:05:29.943133Z"
        }
      },
      "source": [
        "class BERT(nn.Module):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.__bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=159)\n",
        "\n",
        "    def forward(self, input_id: Tensor, attention_mask: Tensor, label: Tensor) -> Tensor:\n",
        "        input_id = input_id.squeeze(1)\n",
        "        attention_mask = attention_mask.squeeze(1)\n",
        "        return self.__bert(input_ids=input_id, attention_mask=attention_mask, labels=label)\n",
        "\n",
        "\n",
        "class ModelBERT:\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._device = DEVICE\n",
        "        self._optimizer = None\n",
        "        self._strategy = BERT().to(self._device)\n",
        "\n",
        "    def predict(self, x: Dict, y: Tensor, *args, **kwargs) -> Union[Tensor, any]:\n",
        "        x = x[\"source\"]\n",
        "        input_id, attention_mask = x[\"input_ids\"], x[\"attention_mask\"]\n",
        "        return self._strategy(input_id, attention_mask, y)\n",
        "\n",
        "    def optimize(self, x: Dict, y: Tensor) -> float:\n",
        "        self._optimizer.zero_grad()\n",
        "        pred = self.predict(x, y)\n",
        "        loss = self.get_loss(pred)\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    @staticmethod\n",
        "    def get_loss(pred: Tensor) -> Tensor:\n",
        "        return pred.loss\n",
        "\n",
        "    def log_strategy(self, path_to_log: str):\n",
        "        open(os.path.join(path_to_log, \"strategy.txt\"), 'a+').write(str(self._strategy))\n",
        "\n",
        "    def train_mode(self):\n",
        "        self._strategy = self._strategy.train()\n",
        "\n",
        "    def eval_mode(self):\n",
        "        self._strategy = self._strategy.eval()\n",
        "\n",
        "    def save(self, path_to_save: str):\n",
        "        path_to_pth = os.path.join(path_to_save, \"model.pth\")\n",
        "        print(\"\\nSaving model at {}...\".format(path_to_pth))\n",
        "        torch.save(self._strategy.state_dict(), path_to_pth)\n",
        "        print(\"... Model saved successfully!\\n\")\n",
        "\n",
        "    def set_optimizer(self, learning_rate: float, optimizer_type: str = \"adamw\"):\n",
        "        optimizers_map = {\"adam\": optim.Adam, \"adamw\": optim.AdamW, \"rmsprop\": optim.RMSprop, \"sgd\": optim.SGD}\n",
        "        self._optimizer = optimizers_map[optimizer_type](self._strategy.parameters(), lr=learning_rate)"
      ],
      "outputs": [],
      "execution_count": 40
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB9ERsPwYufr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s3Oa0LjY_HP",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:29.979357Z",
          "start_time": "2024-04-09T14:05:29.960499Z"
        }
      },
      "source": [
        "class TrainerBERT:\n",
        "    def __init__(self, path_to_log: str, val_frequency: int = 5):\n",
        "        self._device = DEVICE\n",
        "        self._val_frequency = val_frequency\n",
        "        self._path_to_log = path_to_log\n",
        "        self._metrics_tracker = MetricsTracker()\n",
        "        self._train_loss, self._val_loss = LossTracker(), LossTracker()\n",
        "        self._best_val_loss, self._best_metrics = 100.0, self._metrics_tracker.get_best_metrics()\n",
        "        self._path_to_metrics = str(os.path.join(path_to_log, \"metrics.csv\"))\n",
        "\n",
        "    def __to_device(self, x: Dict, y: Tensor):\n",
        "        return {k1: {k2: v2.to(self._device) for k2, v2 in v1.items()} for k1, v1 in x.items()}, y.to(self._device)\n",
        "\n",
        "    def _train_epoch(self, model: ModelBERT, data: DataLoader, epoch: int, *args, **kwargs):\n",
        "        for i, (x, y) in enumerate(data):\n",
        "            if not \"source\" in x.keys():\n",
        "                continue\n",
        "            x, y = self.__to_device(x, y)\n",
        "            tl = model.optimize(x, y)\n",
        "            self._train_loss.update(tl)\n",
        "            if i % 5 == 0:\n",
        "                print(\"[ Epoch: {} - Batch: {} ] | Loss: {:.4f} \".format(epoch + 1, i, tl))\n",
        "\n",
        "    def _eval_epoch(self, model: ModelBERT, data: DataLoader):\n",
        "        for i, (x, y) in enumerate(data):\n",
        "            x, y = self.__to_device(x, y)\n",
        "            pred = model.predict(x, y)\n",
        "            self._metrics_tracker.update(pred.logits, y.detach().numpy())\n",
        "            vl = model.get_loss(pred).item()\n",
        "            if i % 5 == 0:\n",
        "                print(\"[ Batch: {} ] | Loss: {:.4f} ]\".format(i, vl))\n",
        "\n",
        "    def _check_metrics(self):\n",
        "        \"\"\" Computes, prints and logs the current metrics using the metrics tracker \"\"\"\n",
        "        epoch_metrics = self._metrics_tracker.compute_metrics()\n",
        "        self._print_metrics(epoch_metrics)\n",
        "        self._log_metrics(epoch_metrics)\n",
        "\n",
        "    def _log_metrics(self, metrics: Dict):\n",
        "        log_data = pd.DataFrame({\"train_loss\": [self._train_loss.avg], \"val_loss\": [self._val_loss.avg],\n",
        "                                 **{\"best_\" + k: [v] for k, v in self._best_metrics.items()},\n",
        "                                 **{k: [v] for k, v in metrics.items()}})\n",
        "        header = log_data.keys() if not os.path.exists(self._path_to_metrics) else False\n",
        "        log_data.to_csv(self._path_to_metrics, mode='a', header=header, index=False)\n",
        "\n",
        "    def _print_metrics(self, metrics: Dict):\n",
        "        for mn, mv in metrics.items():\n",
        "            print((\" {} \" + \"\".join([\".\"] * (15 - len(mn))) + \" : {:.4f} (Best: {:.4f})\")\n",
        "                  .format(mn.capitalize(), mv, self._best_metrics[mn]))\n",
        "\n",
        "    def train(self, model: ModelBERT, training_set: DataLoader, test_set: DataLoader, lr: float, epochs: int):\n",
        "        \"\"\"\n",
        "        Trains the given model (a PyTorch nn.Module) for \"epochs\" epochs\n",
        "        :param model: the model to be trained (a PyTorch nn.Module)\n",
        "        :param training_set: the data loader containing the training data\n",
        "        :param test_set: the data loader containing the validation/test data\n",
        "        :param lr: a learning rate as base value for the optimizer\n",
        "        :param epochs: the number of epochs the model should be trained for\n",
        "        \"\"\"\n",
        "        model.log_strategy(self._path_to_log)\n",
        "        model.set_optimizer(lr)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            model.train_mode()\n",
        "            self._train_loss.reset()\n",
        "            self.print_heading(\"training\", epoch, epochs)\n",
        "\n",
        "            start = time()\n",
        "            self._train_epoch(model, training_set, epoch)\n",
        "            self.print_train_performance(train_time=time() - start)\n",
        "\n",
        "            if epoch % self._val_frequency == 0:\n",
        "                model.eval_mode()\n",
        "                self._val_loss.reset()\n",
        "                self._reset_metrics_tracker()\n",
        "                self.print_heading(\"validating\", epoch, epochs)\n",
        "\n",
        "                start = time()\n",
        "                with torch.no_grad():\n",
        "                    self._eval_epoch(model, test_set)\n",
        "                self.print_val_performance(val_time=time() - start)\n",
        "\n",
        "                self._check_metrics()\n",
        "                self._check_if_best_model(model)\n",
        "\n",
        "    def _reset_metrics_tracker(self):\n",
        "        \"\"\" Reset the metrics_tracker(s) zeroing out the running values \"\"\"\n",
        "        self._metrics_tracker.reset()\n",
        "\n",
        "    def _check_if_best_model(self, model: ModelBERT):\n",
        "        \"\"\"\n",
        "        Checks whether the provides model is the new best model based on the values of the validation loss.\n",
        "        If yes, updates the best metrics and validation loss (as side effect) and saves the model to file\n",
        "        :param model: the model to be possibly saved as new best model\n",
        "        \"\"\"\n",
        "        if 0 < self._val_loss.avg < self._best_val_loss:\n",
        "            self._best_val_loss = self._val_loss.avg\n",
        "            self._best_metrics = self._metrics_tracker.update_best_metrics()\n",
        "            print(\"\\n -> Saving new best model...\")\n",
        "            model.save(self._path_to_log)\n",
        "\n",
        "    def print_train_performance(self, train_time: float):\n",
        "        \"\"\"\n",
        "        Prints the training time/loss for the most recent epoch\n",
        "        :param train_time: the training time for the most recent epoch\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + SEPARATOR[\"stars\"])\n",
        "        print(\" Train Time ... : {:.4f}\".format(train_time))\n",
        "        print(\" Train Loss ... : {:.4f}\".format(self._train_loss.avg))\n",
        "        print(SEPARATOR[\"stars\"])\n",
        "\n",
        "    def print_val_performance(self, val_time: float):\n",
        "        \"\"\"\n",
        "        Prints the validation time/loss for the most recent epoch\n",
        "        :param val_time: the validation time for the most recent epoch\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + SEPARATOR[\"stars\"])\n",
        "        print(\" Val Time ... : {:.4f}\".format(val_time))\n",
        "        print(\" Val Loss ... : {:.4f}\".format(self._val_loss.avg))\n",
        "        print(SEPARATOR[\"stars\"] + \"\\n\")\n",
        "\n",
        "    @staticmethod\n",
        "    def print_heading(mode: str, epoch: int, epochs: int):\n",
        "        print(\"\\n\" + SEPARATOR[\"dashes\"])\n",
        "        print(\"\\t\\t {} epoch {}/{}\".format(mode.upper(), epoch + 1, epochs))\n",
        "        print(SEPARATOR[\"dashes\"] + \"\\n\")"
      ],
      "outputs": [],
      "execution_count": 41
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ev6vE71OaGHJ"
      },
      "source": [
        "# Perform CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "ocEP4benaD8n",
        "outputId": "ea364381-f231-438d-aabe-231e65946aea",
        "ExecuteTime": {
          "end_time": "2024-04-09T14:05:57.659865Z",
          "start_time": "2024-04-09T14:05:29.981188Z"
        }
      },
      "source": [
        "make_deterministic(0)\n",
        "lr, epochs, log_frequency = 0.0001, 5, 1\n",
        "\n",
        "log_dir = \"{}\".format(time())\n",
        "path_to_log = os.path.join(\"logs\", log_dir)\n",
        "os.makedirs(path_to_log)\n",
        "\n",
        "for fold_num in range(5):\n",
        "    print(\"\\n Loading data for fold '{}'\".format(fold_num + 1))\n",
        "    train_loader, test_loader = DataHandler().train_test_loaders(fold_num)\n",
        "    model = ModelBERT()\n",
        "\n",
        "    print(\"\\n\" + SEPARATOR[\"dashes\"])\n",
        "    print(\"\\t\\t Training fold {}\".format(fold_num + 1))\n",
        "    print(SEPARATOR[\"dashes\"] + \"\\n\")\n",
        "\n",
        "    trainer = TrainerBERT(path_to_log, log_frequency)\n",
        "    trainer.train(model, train_loader, test_loader, lr, epochs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Loading data for fold '1'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\t\t Training fold 1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\t\t TRAINING epoch 1/5\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9f6809cde6f1>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerBERT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_log\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_frequency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-b382c6a96921>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, training_set, test_set, lr, epochs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_train_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-b382c6a96921>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, model, data, epoch, *args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelBERT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m\"source\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e3ae31a71b07>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fold_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e3ae31a71b07>\u001b[0m in \u001b[0;36m_load_input\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"path2\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e3ae31a71b07>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"path2\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e3ae31a71b07>\u001b[0m in \u001b[0;36m_load_f\u001b[0;34m(self, path_to_item)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_item\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-e3ae31a71b07>\u001b[0m in \u001b[0;36m__encode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2084\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   2087\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         \u001b[0;31m# Instantiate the tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m             raise OSError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordpieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0mdo_lower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# 4. If some of the special tokens are not part of the vocab, we add them, at the end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# the order of addition is the same as self.SPECIAL_TOKENS_ATTRIBUTES following `tokenizers`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         self._add_tokens(\n\u001b[0m\u001b[1;32m    368\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens_extended\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_added_tokens_encoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_add_tokens\u001b[0;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0madded_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;31m# TODO this is fairly slow to improve!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mcurrent_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0mnew_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_vocab\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# only call this once, len gives the last index + 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mget_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 42
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}